<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://theallen1996.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://theallen1996.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-10T16:53:13+00:00</updated><id>https://theallen1996.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">2023年阅读小结</title><link href="https://theallen1996.github.io/blog/2024/2023-Reading-Summary/" rel="alternate" type="text/html" title="2023年阅读小结"/><published>2024-07-06T00:00:00+00:00</published><updated>2024-07-06T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2024/2023-Reading-Summary</id><content type="html" xml:base="https://theallen1996.github.io/blog/2024/2023-Reading-Summary/"><![CDATA[<p>本来这篇是想着过完年到了法国后慢慢写的，但命运叵测，各种原因一直搁置到现在。现在2024年都过去一半了，再不写恐怕就要和2024年总结撞一起了，到时难免不好看，索性还是早点整理完好了。</p> <p>今年一共完成了23本书，年初因为《三体》热映，所以又重新读了一遍，但无奈确实欣赏不来，最后连个人前十都没能进入。此外读了些关于社会、经济的书，毕竟大环境在变，我个人在社会的角色也在变，多些视角让我了解下游戏规则也是好的。不过更多时候是抱着一种好奇心去读，也没有说一定要解决问题或者找到答案，只是想满足心里的一些“为什么”。</p> <p>总体而言数量偏少，不过质量上自己倒也满意。只是后面不能再拖延了，希望自己能把这个年度总结的习惯好好完善下。毕竟时间过得是越来越快了，感觉一转眼，就要准备下一年的年度总结了。</p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_overview-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_overview-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_overview-1600.webp"/> <img src="/assets/img/books/2023_overview.jpg" class="img-fluid rounded z-depth-1" width="366" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 2023年阅读书单 </div> <p><b>10.《叫魂》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_10-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_10-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_10-1600.webp"/> <img src="/assets/img/books/2023_10.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《叫魂》 </div> <p>最早应该是在书单狗上知道这本书的，那会儿大概是太久没有读恐怖悬疑小说了，我就将这本猎奇向的书加入书单了。乍看书名时，给的定位是中国民间恐怖故事大合集，结果没有想到是个非常严肃深入的学术著作，尤其最后两章的讨论，又有种读《大明王朝1566》和《万历十五年》的感觉。孔老师一生就出了四本书，有时感觉自己用一顿饭不到的钱就换来了人家小半生的心血，实在感激不已。但也可能是过于严肃深刻了，有些地方自己并没能消化，我在2024年读《饥饿的盛世》时，感慨要是我先读《饥饿的盛世》再读这一本书，可能会理解更深，给他的排名也会更高。</p> <p><b>9.《基因组》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_9-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_9-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_9-1600.webp"/> <img src="/assets/img/books/2023_9.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《基因组》 </div> <p>疫情期间买的了，但一直拖到疫情结束才开始读，是个不错的科普向读物。读这类书其实就是想更好地了解自己，我觉得生物学角度的书，比心理学或社会学角度的更客观。但有时这类解读也有点过于宽泛，导致无法产生具体到个人层面的认知。</p> <p>这本书最大的点应该是：大脑会影响激素分泌进而影响基因的表达，从而影响人的身体状态，比如一般人面对压力时，身体状况都会变差，虽然这听起来有点唯心主义；此外就是基因其实已经把很多东西都决定了，这又有点宿命论的感觉了。</p> <p><b>8.《长安的荔枝》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_8-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_8-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_8-1600.webp"/> <img src="/assets/img/books/2023_8.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《长安的荔枝》 </div> <p>飞机读物，一天就能读完。一开始感觉马伯庸有点过于卖弄自己的“唐学”了，动不动一大堆司局部门、规矩流程名词，而台词人物却都简单直白，甚至有点低幼化，导致内容和形式有极大的落差，差点没坚持下去。但中后期节奏就好起来了，从主角个人思想的改变，到点出“上头一道命令，下面的人得忙活上半天”这个主题，层层递进，加上本身篇幅又不长，读完感觉还是可以推荐的。</p> <p><b>7.《小家，越住越大2》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_7-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_7-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_7-1600.webp"/> <img src="/assets/img/books/2023_7.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 小家，越住越大2 </div> <p>总体来说比第一部质量高一些，格局总体会大些了。而且大概是从这部开始吧，自己了解到“住商”（类似智商情商逆商等等）这个概念，这个还是挺有意思的。现在自己在日常居家时，会主动下意识地去思考当前这个房子有哪些不合理的地方，以及有哪些布置华而不实，大概率会积灰，甚至开始会花心思在一些品质的改善上。</p> <p><b>6.《遗忘，刑警》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_6-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_6-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_6-1600.webp"/> <img src="/assets/img/books/2023_6.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《遗忘，刑警》 </div> <p>这应该还是我第一次读陈浩基的书，这年头想找本好点的中文悬疑小说还是挺难的。这本书节奏不错，可读性很强，篇幅也很短，值得一读。整体的风格可能因为有“警察”这个元素在，会让人想到香港黄金时代的警匪片，有自己的特色。</p> <p><b>5.《饱食穷民》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_5-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_5-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_5-1600.webp"/> <img src="/assets/img/books/2023_5.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《饱食穷民》 </div> <p>主要讲了日本在经济快速发展时期一些疯狂的现象：如消费主义的盛行、人与人之间因疏远而产生的心理及社会压力。</p> <p>这本书里的案例故事看似身边也有，但却都没有什么代入感，或许我们还没到那一步吧。读的时候更多是有一种猎奇心上的满足感，会忍不住想这会不会就是十年后的我们。读的过程也发现，我们好像很难从日本过去的发展来分析理解我们当前的问题和走向。这些零零散散的书和文章看下来，他们的个人生活、工作、教育固然跟我们都有很多相似的地方，但有些非常核心且重要的点，我们却和他们截然不同。</p> <p><b>4.《13 67》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_4-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_4-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_4-1600.webp"/> <img src="/assets/img/books/2023_4.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《13 67》 </div> <p>读完《遗忘，刑警》后，觉得陈浩基的小说节奏和逻辑都不错，属于很高质量的快餐，就又继续读了他的另一本代表作。相比上一本，这本书内容更多，细节也相对更丰富，小说整体的布局和逻辑都非常不错。后面搜了下，陈浩基是港中文的计算机系毕业生，可能这就是为什么他的推理逻辑会让人觉得合理缜密吧。</p> <p><b>3.《置身事内》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_3-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_3-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_3-1600.webp"/> <img src="/assets/img/books/2023_3.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《置身事内》 </div> <p>这两年到处都在唱衰，自己进入社会也一年有余了，觉得自己不能再像学生时那样无忧无虑，多少也要学会了解大环境这个东西了。恰好那段时间很多人在强推这本书，就买来学习下。</p> <p>这几年相关的讨论到处都是，很多内容也已经不是第一次了解接触了，但读起来仍感觉干货满满。重点是帮助我大致理清了当年经济增长核心靠什么，以及浅浅点出了为什么现在会这么多唱衰。当然，经济的影响是方方面面的，我们不能说作者说的就一定对或是错，但能给我们提供一个更高更全面的视角，这一点就已经很值得一读了。</p> <p><b>2.《日本新中产阶级》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_2-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_2-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_2-1600.webp"/> <img src="/assets/img/books/2023_2.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《日本新中产阶级》 </div> <p>围绕工薪阶级讲述了日本家庭观念的转变，从中可以看到很多我们的影子，有非常强的代入感，很难想象是写于60年前，可见我们在某种程度上，确实在走别人的老路。但对比下来，我们面临的境遇更为险恶。而且书里最后的补充描述，所指示的似乎也不是一个光明的未来。</p> <p><b>1.《我在北京送快递》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2023_1-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2023_1-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2023_1-1600.webp"/> <img src="/assets/img/books/2023_1.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《我在北京送快递》 </div> <p>北京篇年度最佳，作者用直白的话语写出千万打工人的不易，引起社会共鸣，让人不禁感叹这就是文字的力量。要是格局再大一些，北京篇的篇幅再长一些，就更完美了。</p> <p>我发现我还挺喜欢这种纪实类的，直白叙述的“苦难”文学的，比如之前读《平凡的世界》我也非常上头。但有时想想，引起共鸣是一回事，能解决问题又是另一回事。就像余华说的，“永远不要相信苦难是值得的，苦难就是苦难，苦难不会带来成功，苦难不值得追求，磨练意志是因为苦难无法躲开。”如果不值得追求，那当我们在阅读别人的苦难时，我们在读什么？或许是在遇到困境时，知道你我并不孤独吧。</p>]]></content><author><name></name></author><category term="Reading"/><category term="Chinese"/><summary type="html"><![CDATA[本来这篇是想着过完年到了法国后慢慢写的，但命运叵测，各种原因一直搁置到现在。现在2024年都过去一半了，再不写恐怕就要和2024年总结撞一起了，到时难免不好看，索性还是早点整理完好了。]]></summary></entry><entry><title type="html">NeRF - From view synthesis to 3D assets</title><link href="https://theallen1996.github.io/blog/2023/NeRF-From-view-synthesis-to-3D-assets/" rel="alternate" type="text/html" title="NeRF - From view synthesis to 3D assets"/><published>2023-09-04T00:00:00+00:00</published><updated>2023-09-04T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2023/NeRF%E2%80%93From-view-synthesis-to-3D-assets</id><content type="html" xml:base="https://theallen1996.github.io/blog/2023/NeRF-From-view-synthesis-to-3D-assets/"><![CDATA[<p><a href="https://docs.google.com/presentation/d/1CRSZUUCWnTdeZR-NzzTaJuINbcMViILK/edit?usp=sharing&amp;ouid=115169074992480226929&amp;rtpof=true&amp;sd=true">NeRF From-view-synthesis-to-3D-assets</a></p> <p>In this talk, we discussed the fundamentals of NeRF (Neural Radiance Fields) and breifly mentioned recent advancements, particularly those related to 3D assets. The progress made in this field holds great promise, especially in the realms of gaming and the metaverse. Can’t wait to see how it will revolutionize the way we experience virtual reality.</p>]]></content><author><name></name></author><category term="Computer-Graphics"/><summary type="html"><![CDATA[Fundamentals of NeRF (Neural Radiance Fields) and recent advancements.]]></summary></entry><entry><title type="html">2022年阅读小结</title><link href="https://theallen1996.github.io/blog/2022/2022-Reading-Summary-copy/" rel="alternate" type="text/html" title="2022年阅读小结"/><published>2022-12-31T00:00:00+00:00</published><updated>2022-12-31T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2022/2022-Reading-Summary%20copy</id><content type="html" xml:base="https://theallen1996.github.io/blog/2022/2022-Reading-Summary-copy/"><![CDATA[<p>上一次写总结还是在<a href="http://theallen1996.github.io/blog/2019/2019-Reading-Summary/">2019年</a>，那会儿是一股脑把个人年度十佳列出来发表在微博上，但后来因为不玩微博了，也没找到其它更合适的平台，就没再写了，不过还是有零零散散地写些笔记。其实我一直觉得阅读是件很私人的事，也很少跟朋友聊我读了什么书。一来我性格总是偏玩，给人感觉不像是会去读书的人，二来现在很多人确实也不读书了，主动分享难免有炫耀之嫌，反而会让自己自讨没趣，所以也就罢了。</p> <p>然而过去一年，因为疫情、求职、毕业、等大小事，我和一些很久没联系的、原本并不那么熟的朋友有了契机互相分享彼此的经历和变化，不免一阵唏嘘感慨。想想自己的阅读笔记里，也多少包含着些自己的心路历程，或者说反应着自己认知的变化，就感觉一年一次的阅读总结还是值得一写的。</p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_overview-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_overview-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_overview-1600.webp"/> <img src="/assets/img/books/2022_overview.png" class="img-fluid rounded z-depth-1" width="366" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 2022年阅读书单 </div> <p>今年一共读了30本书，我读书选书很多时候看心情，可能会比较倾向于历史类科普类，但总体也不讲什么方法论，也不刻意追求什么收获，一本书如果读到5%时不能吸引我，我就换了。具体的总结还是按年度十佳的形式，也比较简单粗暴。</p> <p><b>10.《万历十五年》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_Wan_Li_Shi_Wu_Nian-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_Wan_Li_Shi_Wu_Nian-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_Wan_Li_Shi_Wu_Nian-1600.webp"/> <img src="/assets/img/books/2022_Wan_Li_Shi_Wu_Nian.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《万历十五年》 </div> <p>这本书是在读完《大明王朝1566》后读的，书的正文内容比想象中的短，而且感觉前三四章的内容都和章节标题对不上号，看标题给人感觉是单独的人物小传，但具体内容覆盖得又比较广。后面三章对上了，但正文也结束了，有点意犹未尽。这种意犹未尽多少也跟《大明王朝1566》有关。在我看来，1566负责讲故事，重要的是刻画人物，把事情有条理地讲出来。而本书则属于透过现象看本质，分析事情背后的原因并给出一定的总结和评价。学术层面的考究当然可以让人对历史的理解更深刻，但要想理解这些本质，前提是对历史的全貌足够熟悉，所以需要两者相辅相成。</p> <p><b>9.The GodFather</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_The_GodFather-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_The_GodFather-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_The_GodFather-1600.webp"/> <img src="/assets/img/books/2022_The_GodFather.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The GodFather </div> <p>之前读过中文版，因为疫情居家实在无聊，翻起了英文原版。可能因为中文语言里有江湖快意恩仇这些概念，所以总感觉中文版更有浪漫和传奇色彩，英文版尽管本身也强调家族这种概念，但还是显得平铺直叙了一点。中间一些类似个人小传的章节感觉完全可以压缩下或者当番外展开讲，有点拖节奏了。结尾老唐倒下的时候整个家族势力一落千丈，还是挺让人唏嘘的。</p> <p>另外就是再次感受到写读书笔记的意义，五年前这个小说真的给我留下了很深的印象，那时我会在笔记里聊阿波罗尼亚聊叛徒。但这次读的时候我很多地方真的跟第一次读一样，什么印象都没了。而当我读完后再去翻笔记，相当于是跟旧时空中的我沟通分享彼此的阅读心得，这种对话也能让自己意识到自己发生的变化。</p> <p><b>8.《漩涡》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_Xuan_Wo-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_Xuan_Wo-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_Xuan_Wo-1600.webp"/> <img src="/assets/img/books/2022_Xuan_Wo.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《漩涡》 </div> <p>这本书是今年读完的第三十本。总体而言体验非常好非常上头，差不多一天就看完了。读的过程就是惊讶于伊藤润二的想象力，好怪，好爱，读起来有种猎奇心被填补的满足感。而读到后面的“解说”时，虽有牵强附会之感，但也觉得自己还是太小看这个作品背后的深度了，有时间了会再翻翻看体会下背后反应的社会问题。但有一说一，从第一话开始“秀一”就劝女主离开小镇，结果即使亲眼目睹那么多奇葩事女主还是依然不采取行动，多少感觉有点俗套，给人一种恐怖片中看别人作死的折磨感。</p> <p><b>7.The Martian</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_The_Martian-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_The_Martian-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_The_Martian-1600.webp"/> <img src="/assets/img/books/2022_The_Martian.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Martian </div> <p>大概是科幻版的《鲁滨逊漂流记》吧。看的时候一直在想一个问题，为什么小说很难给人喜剧体验？《火星救援》电影版我是能笑出声的，甚至会考虑打上喜剧标签的，但原著读下来笑点虽然也有，但我的反应就很一般，更多时候是以一个更客观的视角去发现主角身上难能可贵的乐观精神。回头想想，自己读过的让我觉得幽默的小说好像也就《围城》和韩寒的两三部，期待以后能挖到新宝藏吧。</p> <p><b>6.Game Programming Pattern</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_Game_Programming_Patterns-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_Game_Programming_Patterns-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_Game_Programming_Patterns-1600.webp"/> <img src="/assets/img/books/2022_Game_Programming_Patterns.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Game Programming Pattern </div> <p>秋招时开的坑，其实多少是有点急功近利了，一边看一边面试，后来拿到offer后就又闲置下来，直到后面因为疫情才又捡起来。书里介绍了游戏开发一些常用的设计模式，但跟软件工程中的那类还不太同，因为有具体的游戏场景，专业性更强。很多章节都是用一个具体问题来开篇，这些问题本身不但质量高，还能加强读者的阅读体验和对相关模式的理解。此外还有很多示例代码，个人认为对初学者很有参考性，可以从中对游戏行业的一些“标准”有些了解。最好的阅读时机应该是做第三个游戏作品时？边看边实践收获可能会更大。</p> <p><b>5.《俗世奇人》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_Su_Shi_Qi_Ren-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_Su_Shi_Qi_Ren-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_Su_Shi_Qi_Ren-1600.webp"/> <img src="/assets/img/books/2022_Su_Shi_Qi_Ren.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《俗世奇人》 </div> <p>那段时间读了太久的英文书和外译本，实在想换换口味了，就翻起来了这本地道中文书，两天读完还有点意犹未尽。因为是半文半白，加上一些语气词，有时读起来就像是在听人说书，让人有回到小镇集市凑热闹看戏的惬意感。书里的故事质量都很高，印象最深的是《小杨月楼义结李金鏊》，是真的被感动到了。另外就是书里很多描写、言语和故事背景的细节都很值得玩味，读的虽然是奇人轶事，但也能从中看众生百态。</p> <p>读《泥人张》时有一句“在裤裆里捏吧！”，一边笑一边在想当初课本有这句话吗，这不会被和谐吗，要是被和谐了就好笑了。说真的，我觉得大家根本没必要捧着个烂翻译版的《一九八四》在那指桑骂槐，现实本身就很荒诞啊。</p> <p><b>4.Philosophy of Science</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_Philosophy_of_Science-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_Philosophy_of_Science-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_Philosophy_of_Science-1600.webp"/> <img src="/assets/img/books/2022_Philosophy_of_Science.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Philosophy of Science </div> <p>本书除了语言通俗，所介绍的例子也很精准贴切，相当有意思。就这本书的内容来说，里面讨论了很多颠覆科学“常识”的观点，这类观点我们可能不认同，却也无法用合理完整的逻辑去完全推翻。我认为很多人都该读读本书，在推荐算法的陷阱下，大家某种程度上都生活在信息茧房中，而学会思考则有助于打破这层壁垒。</p> <p>刚读完时我的笔记中写道“开始读牛津这个系列哲学相关的书了，本书作为第一站，体验很好，很愿意继续”。但后来读到亚里士多德篇实在没读下去，计划只得搁置了。</p> <p><b>3.《慢思考：大脑超载时代的思考学》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_Brainchains-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_Brainchains-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_Brainchains-1600.webp"/> <img src="/assets/img/books/2022_Brainchains.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《慢思考：大脑超载时代的思考学》 </div> <p>这本书排名这么靠前其实我自己都是有些吃惊的，因为我现在对这类鸡汤型、方法论、成功学、或者说工作效率类的书嗤之以鼻：无非都是些老生常谈自己想也能想明白的事，甚至可能会因被人发现自己读这类书而感到羞耻。不过看了下当初的笔记，这本书应该是有两点打动我：一是作者讲了三种脑的思考机制，在此之前这个概念对我还是比较陌生的；二是作者非常强调保持离线远离手机，分享的一些经验技巧与我当时的习惯可谓是不谋而合，某种意义上算是对我的一种肯定，心心相惜下我就打了高分吧。</p> <p>现在再让我读这本书，可能收获就很少了，所以这些读书体验到底还是有历史性的。</p> <p><b>2.《强风吹拂》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_Qiang_Feng_Chui_Fu-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_Qiang_Feng_Chui_Fu-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_Qiang_Feng_Chui_Fu-1600.webp"/> <img src="/assets/img/books/2022_Qiang_Feng_Chui_Fu.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《强风吹拂》 </div> <p>朋友闲聊时跟我提起的书，说是关于一群业余大学生参加箱根驿传的故事，又提到在日本这种年轻人的赛事非常受关注，这个赛事每年都还在办，三言两语听下来就感觉这个故事一定很有趣。因为那段时间刚找到工作比较清闲，所以我还甚至感慨：我苦苦找寻的打发时间的书，就是这本了。当晚就买下来开始读，也确实好久没有这么痛快的阅读体验了，真的是刷夜在读。正赛大概是在书大概中间的位置开始，当时还想的是，不可能整个后半段都只有这一场比赛吧，干脆一口气把比赛看完好了，结果看完上半场时发现已经是凌晨三点多了，只得做罢。后来看完比赛，发现全书也快完结了，是真的很意犹未尽。更难得的是，它所带来的快感并不肤浅，是基于对友谊、活力、梦想的真诚赞赏而带来的喜悦和感动，是一本非常积极、能改变生活的书，我后来跟一些朋友聊这本书，还会有人提到自己爱上跑步也是因为这本书。</p> <p><b>1.《大明王朝1566》</b></p> <div class="row justify-content-sm-center" align="center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/img/books/2022_Da_Ming_Wang_Chao_1566-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/books/2022_Da_Ming_Wang_Chao_1566-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/img/books/2022_Da_Ming_Wang_Chao_1566-1600.webp"/> <img src="/assets/img/books/2022_Da_Ming_Wang_Chao_1566.jpg" class="img-fluid rounded z-depth-1" width="150" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 《大明王朝1566》 </div> <p>非常精彩的一本小说，嘉靖本身行为非常人所能揣测，也就使得整个故事很自然得充满了悬念，高潮迭起。就我个人而言，最精彩的还是“血经”那段，荒诞与现实，强弱的对抗以及命运的转折，全部揉杂在了一起，让人越读越来劲了。嘉靖和海瑞间的故事也比我想象中的要更有张力，虽然作者可能多少有些夸张和主观描述内容，但也能看到作者有给出史料依据，还是很有说服力的，甚至可能历史比小说更夸张。</p> <p>我自己是疫情封控期间读的，不可避免会将故事里的一些官僚文化和当时上海的防疫政策相联系起来，也深深感觉我们的历史虽然源远流长，但同时也是一种负担。我们对自己民族的归属感、认同感，除了那些好的，这些坏的也功不可没。</p>]]></content><author><name></name></author><category term="Reading"/><category term="Chinese"/><summary type="html"><![CDATA[上一次写总结还是在2019年，那会儿是一股脑把个人年度十佳列出来发表在微博上，但后来因为不玩微博了，也没找到其它更合适的平台，就没再写了，不过还是有零零散散地写些笔记。其实我一直觉得阅读是件很私人的事，也很少跟朋友聊我读了什么书。一来我性格总是偏玩，给人感觉不像是会去读书的人，二来现在很多人确实也不读书了，主动分享难免有炫耀之嫌，反而会让自己自讨没趣，所以也就罢了。]]></summary></entry><entry><title type="html">Computer Graphics - Graphics Pipeline</title><link href="https://theallen1996.github.io/blog/2021/GraphicsPipeline/" rel="alternate" type="text/html" title="Computer Graphics - Graphics Pipeline"/><published>2021-09-08T00:00:00+00:00</published><updated>2021-09-08T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2021/GraphicsPipeline</id><content type="html" xml:base="https://theallen1996.github.io/blog/2021/GraphicsPipeline/"><![CDATA[<p>This is a brief summary about graphics pipeline based on the first four assignments of the course <a href="http://games-cn.org/intro-graphics/">Games101</a>. The course is educational and friendly to people who is very new to this field like me. Thanks to this, I now finally have a basic understanding of graphics pipeline from a much more practical perspective. This post will cover some concepts about <em>application</em>, <em>geometry</em>, and <em>rasterization</em> of a graphics pipeline (Fig 1) in an (trying to be) intuitive and straightforward way.</p> <div align="center"><img src="../../../../assets/images/Graphics_pipeline_2_en.svg.png" alt="Graphics Pipeline from Wiki" width="550" height="96"/> </div> <center> <p style="font-size:80%;"> Figure 1. Graphics Pipeline (Wikipedia) </p> </center> <ul id="markdown-toc"> <li><a href="#1-application" id="markdown-toc-1-application">1. Application</a></li> <li><a href="#2-geometry-processing" id="markdown-toc-2-geometry-processing">2. Geometry Processing</a> <ul> <li><a href="#21-mvp-transformation" id="markdown-toc-21-mvp-transformation">2.1 MVP Transformation</a></li> <li><a href="#22-viewport-transformation" id="markdown-toc-22-viewport-transformation">2.2 Viewport Transformation</a></li> </ul> </li> <li><a href="#3-rasterization" id="markdown-toc-3-rasterization">3. Rasterization</a> <ul> <li><a href="#31-triangle-traversal" id="markdown-toc-31-triangle-traversal">3.1 Triangle Traversal</a></li> <li><a href="#32-pixel-shading" id="markdown-toc-32-pixel-shading">3.2 Pixel Shading</a></li> </ul> </li> <li><a href="#4-conclusion" id="markdown-toc-4-conclusion">4. Conclusion</a></li> </ul> <h1 id="1-application">1. Application</h1> <p>During the application stage, there will be computation such as collision detection, animation, acceleration, and dealing with inputs, <em>etc</em>. The output data should contain the information like positions of vertices, normal information, and colour information an so on.</p> <p><strong>Example 1.1</strong>: In assignment 2, we are required to draw two triangles on the screen, and the data we need is the positions of the six vertices, the indices that determine the components of each triangle, and the colour of each vertices. Here we get the data directly without extra computation.</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Eigen</span><span class="o">::</span><span class="n">Vector3f</span><span class="o">&gt;</span> <span class="n">pos</span>		<span class="c1">// positions of vertices</span>
<span class="p">{</span>
    <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">},</span>
    <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">},</span>
    <span class="p">{</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">},</span>
    <span class="p">{</span><span class="mf">3.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">},</span>
    <span class="p">{</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">},</span>
    <span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">}</span>
<span class="p">};</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Eigen</span><span class="o">::</span><span class="n">Vector3i</span><span class="o">&gt;</span> <span class="n">ind</span>		<span class="c1">// vertices' indices of each triangle</span>
<span class="p">{</span>
    <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">},</span>
    <span class="p">{</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">}</span>
<span class="p">};</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Eigen</span><span class="o">::</span><span class="n">Vector3f</span><span class="o">&gt;</span> <span class="n">cols</span>		<span class="c1">// color of each vertices</span>
<span class="p">{</span>
    <span class="p">{</span><span class="mf">217.0</span><span class="p">,</span> <span class="mf">238.0</span><span class="p">,</span> <span class="mf">185.0</span><span class="p">},</span>
    <span class="p">{</span><span class="mf">217.0</span><span class="p">,</span> <span class="mf">238.0</span><span class="p">,</span> <span class="mf">185.0</span><span class="p">},</span>
    <span class="p">{</span><span class="mf">217.0</span><span class="p">,</span> <span class="mf">238.0</span><span class="p">,</span> <span class="mf">185.0</span><span class="p">},</span>
    <span class="p">{</span><span class="mf">185.0</span><span class="p">,</span> <span class="mf">217.0</span><span class="p">,</span> <span class="mf">238.0</span><span class="p">},</span>
    <span class="p">{</span><span class="mf">185.0</span><span class="p">,</span> <span class="mf">217.0</span><span class="p">,</span> <span class="mf">238.0</span><span class="p">},</span>
    <span class="p">{</span><span class="mf">185.0</span><span class="p">,</span> <span class="mf">217.0</span><span class="p">,</span> <span class="mf">238.0</span><span class="p">}</span>
<span class="p">};</span>

<span class="n">rasterizer</span><span class="p">.</span><span class="n">load_positions</span><span class="p">(</span><span class="n">pos</span><span class="p">);</span>
<span class="n">rasterizer</span><span class="p">.</span><span class="n">load_indices</span><span class="p">(</span><span class="n">ind</span><span class="p">);</span>
<span class="n">rasterizer</span><span class="p">.</span><span class="n">load_colors</span><span class="p">(</span><span class="n">cols</span><span class="p">);</span>
</code></pre></div></div> <h1 id="2-geometry-processing">2. Geometry Processing</h1> <p>The geometry processing step is responsible for most of the operations with triangles and their vertices. In the assignments, there are mainly two tasks during this stage.</p> <div align="center"><img src="../../../../assets/images/MVP.png" alt="unknown source"/> </div> <center> <p style="font-size:80%;"> Figure 2. MVP Transformation and Viewport Transformation </p> </center> <h2 id="21-mvp-transformation">2.1 MVP Transformation</h2> <p><strong>Model Transformation</strong>: Transforming the vertices of the objects we want to render. Most time this means applying scaling, translation, and rotation. This can be generally done by using three well-defined matrix in the homogeneous coordinates. One can also derive each matrix manually by solving simple equations.</p> <p>Scale matrix:</p> \[S(s_x, s_y, s_z)=\begin{bmatrix}s_x &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; s_y &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; s_z &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix};\] <p>Translation matrix:</p> \[T(t_x, t_y, t_z)=\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; t_x \\ 0 &amp; 1 &amp; 0 &amp; t_y \\ 0 &amp; 0 &amp; 1 &amp; t_z\\ 0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix};\] <p>Rotation matrix (rotate counter-clockwise by \(\alpha\) degree):</p> \[R_x(\alpha)=\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; \cos\alpha &amp; -\sin\alpha &amp; 0 \\ 0 &amp; \sin\alpha &amp; \cos\alpha &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix},\\ R_y(\alpha)=\begin{bmatrix} \cos\alpha &amp; 0 &amp; \sin\alpha &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ -\sin\alpha &amp; 0 &amp; \cos\alpha &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix},\\ R_z(\alpha)=\begin{bmatrix} \cos\alpha &amp; -\sin\alpha &amp; 0 &amp; 0 \\ \sin\alpha &amp; \cos\alpha &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}.\] <p><strong>Example 2.1</strong>: In assignment 1, we are required to render a triangle supporting keyboard input. When the key ‘a’ and ‘d’ are pressed, the triangle should be rotated counter-clockwise and clockwise by 10 degrees around \(z\)-axis, respectively. Therefore we have the model transformation matrix as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Matrix model = Identity();
Update model according to R_z(alpha) defined above;
</code></pre></div></div> <p><strong>Viewing Transformation</strong>: Transforming all the vertices of the objects in a space where the camera is located at the origin. More specifically, we want the position, gaze direction, and up direction of the camera represents the origin, \(-z\)-axis, \(y\)-axis of the new space, respectively. Obviously, it can be done by applying translation (for the origin) and rotation (for the axes). One can derive the transformation matrix by considering its inverse transformation, especially for its rotation matrix. By doing viewing transformation, we can greatly simplify the computation for projection.</p> <p><strong>Projection Transformation</strong>: Transforming all the vertices of the objects into a normalized device coordinates (NDC) which is \([-1, 1]^3\). By doing so, we can perform projection and clipping more efficiently. For the orthographic projection where all the objects to be projected are in a cuboid space (with top and bottom at \(t\) and \(b\), left and right at \(l\) and \(r\), near and far at \(n\) and \(f\)), the transformation matrix to get NDC is given as</p> \[M_{NDC}=\begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix} \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -\frac{r+l}{2} \\ 0 &amp; 1 &amp; 0 &amp; -\frac{t+b}{2} \\ 0 &amp; 0 &amp; 1 &amp; -\frac{n+f}{2}\\ 0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}.\] <p>This matrix can be easily derived manually. Then for orthographic projection, we can just drop the \(Z\) coordinate to get the projection (but with incorrect aspect ratio). For perspective projection where the parallel lines are not parallel in the image space and the projection space is actually a frustum (can be represented by field of view, aspect ratio, near and far planes), we can firstly squish the frustum into a cuboid and then do the orthographic projection. The squish matrix is given as</p> \[M_{persp-&gt;ortho} =\begin{bmatrix} n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; n + f &amp; -nf \\ 0 &amp; 0 &amp; 1 &amp; 0\end{bmatrix}.\] <p><strong>Example 2.2</strong>: In assignment 1, we are required to render a triangle with perspective projection. The projection matrix is implemented as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Matrix projection = Identity();

Matrix persp = Identity();
Set persp according to M_{persp-&gt;ortho} defined above;

Matrix ortho = Identity();
Set ortho according to M_{NDC} defined above;

projection = ortho * persp;
</code></pre></div></div> <p>The resulting \(Z\) coordinate will not be stored in the image, but are only used in depth buffer in the later stages.</p> <h2 id="22-viewport-transformation">2.2 Viewport Transformation</h2> <p>The projection we obtained from MVP transformation after dropping \(Z\) coordinate is in the space \([-1, 1]^2\) with wrong aspect ratio. Viewport transformation helps us to get a correct image on the screen with is \([0, width)\times [0, height)\).</p> <p><strong>Example 2.3</strong>: In almost all the four assignments, the viewport transformation is done by scaling the coordinates:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for each vertex in NDC:
    vertex.x = width * (vertex.x + 1) / 2;
    vertex.y = height * (vertex.y + 1) / 2;
    // z coordinate will be used in the later steps
    vertex.z = vertex.z * (far - near) / 2 + (far + near) / 2;
</code></pre></div></div> <h1 id="3-rasterization">3. Rasterization</h1> <p>Now it is time for rasterization. This is about writing colour values into the framebuffer for all the pixels. The complexity varies over how we implement texturing, blending, and anti-aliasing, <em>etc</em>. In the assignments, we mainly focus on two tasks (from a very high level): triangle traversal and pixel shading.</p> <figure> <div style="display:flex"> <figure> <img src="../../../../assets/images/Rasterization_point.jpg" width="380" alt="Games101"/> <figcaption><center style="font-size:80%;">(a) Triangle Traversal</center></figcaption> </figure> <figure> <img src="../../../../assets/images/Rasterization_pixel.jpg" width="380" alt="Games101"/> <figcaption><center style="font-size:80%;">(b) Pixel Shading</center></figcaption> </figure> </div> </figure> <center> <p style="font-size:80%;"> Figure 3. Rasterization </p> </center> <h2 id="31-triangle-traversal">3.1 Triangle Traversal</h2> <p>After viewport transformation, we got a projection of \([0, width)\times [0, height)\) to be rendered on the screen with \(width \times height\) pixels. For each triangle, we need to traverse all the pixels to determine whether they are in the triangle or not. Together with depth information, each pixel then be referred as a point in a single triangle, and will be shaded based on the information of the triangle. We can leverage cross product to determine whether a point is inside a triangle or not:</p> <p><strong>Example 3.1</strong>: In assignment 2, we are required to draw two triangles on the screen. The implementation for inside check can be done as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bool insideTriangle(Point p, Triangle t):
    // points of `t` are `t0`, `t1`, `t2` (counter-clockwise)
    Define vectors p0 = t0 to p, p1 = t1 to p, p2 = t2 to p;
    temp0 = p0.cross(t0 to t1);
    temp1 = p1.cross(t1 to t2);
    temp2 = p2.cross(t2 to t0);
    return temp0 * temp1 * temp2 &gt; 0;
</code></pre></div></div> <p><strong>Example 3.2</strong>: Still in assignment 2, the triangle traversal and part of the shading is done as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>void rasterize_triangle(Triangle t):
    Construct a bounding box of t to accelerate the computation;
    for each pixel (w, h) in the bounding box:
        if(insideTriangle(Point(w+0.5, h+0.5), t)):
            // Pixel Shading;
</code></pre></div></div> <h2 id="32-pixel-shading">3.2 Pixel Shading</h2> <p>Intuitively, pixel shading is responsible for the colour of each pixel. It can be very simple when the colour of each pixel is given by a predefined value. It can be also very tricky when computing those implicit values such as depth that determines the rendering of the overlap part.</p> <p>After the triangle traversal, we refer each pixel as a point inside a triangle. But very often, we only have the vertices information of a triangle, and we have no information about an arbitrary point inside the triangle. To get the pixel shaded correctly based on what we have, we need the help of interpolation.</p> <p><strong>Interpolation</strong>: The formal definition of interpolation can be found everywhere. Here I just want to explain it in an intuitive way: interpolation is getting new data based on known data on the same space. The key of (linear) interpolation is finding something like weights that describe the contribution of known data to the new data so that the transition from the constructed new data to known data can be smooth. In our case for triangles, we often use Barycentric coordinates to describe the contribution of each vertex to the new data.</p> <p><strong>Barycentric Coordinates</strong>: For every point \((x, y)\) inside a triangle ABC, we can find a corresponding coordinate \((\alpha, \beta, \gamma), \alpha,\beta,\gamma \ge 0\) and \(\alpha+\beta+\gamma=1\).</p> <div align="center"><img src="../../../../assets/images/Barycentric.jpg" width="750" alt="Games101"/> </div> <center> <p style="font-size:80%;"> Figure 4. Barycentric Coordinates </p> </center> <p>The Barycentric coordinates provides us a natural way to capture the weights of the contribution of each vertex of a triangle. Therefore we can interpolate values based on this.</p> <p><strong>Example 3.3</strong>: The triangle traversal in assignment 2 with the interpolation details would be like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>void rasterize_triangle(Triangle t):
    Construct a bounding box of t to accelerate the computation;
    for each pixel (w, h) in the bounding box:
        if(insideTriangle(Point(w+0.5, h+0.5), t)):
            alpha, beta, gamma = compute_Barycentric(Point(w+0.5, h+0.5), t);
            // if the projection is perspective, perspective-correct interpolation should be used
            Get the z value by interpolation;
            if(depth_buffer[w, h] &gt; z):
                // the closer one that should be rasterized
                depth_buffer[w, h] = z;
                frame_buffer[w, h] = color;
</code></pre></div></div> <p>As colour of each triangle in assignment 2 is specified, the rendering result on \(700\times 700\) pixels would be like:</p> <figure> <div style="display:flex"> <figure> <img src="../../../../assets/images/Rasterization.png" alt="Code" width="380"/> <figcaption><center style="font-size:80%;">(a) Rendering Result </center></figcaption> </figure> <figure> <img src="../../../../assets/images/Rasterization_300.jpg" alt="Code" width="380"/> <figcaption><center style="font-size:80%;">(b) Zoom In</center></figcaption> </figure> </div> </figure> <center> <p style="font-size:80%;"> Figure 5. Rasterization of Assignment 2 </p> </center> <p>Notice that there are many jaggies along the edge of the right triangle. That is because each pixel itself is a square and we shade it in a binary way: the colour of it can only be green/blue or black. To remove the jaggies, we should allow each pixel to be shaded in varying colours so that the transition from colourful pixels to black ones can be more smooth. Again, interpolation plays an important role here, but in a different way.</p> <p><strong>Multi-Sampling Anti-Aliasing (MSAA)</strong>: In the previous triangle traversal, we traverse each pixel and shade the pixel based on the middle point of the pixel as shown in Fig 6 (a). Therefore for those pixels whose majority part is outside the triangle, the colour will be biased. In MSAA, instead of setting the colour of each pixel based on the middle point, we set it based on \(N\times N\) points inside the pixel. A \(2\times 2\) example is shown in Fig 6 (b). We divide each pixel into four equal parts, and traverse each sub-pixel. The final colour of a pixel will be the average colour of its four sub-pixels.</p> <figure> <div style="display:flex"> <figure> <img src="../../../../assets/images/MSAA_eg.jpg" width="380" alt="Games 101"/> <figcaption><center style="font-size:80%;">(a) Triangle Traversal Per Pixel</center></figcaption> </figure> <figure> <img src="../../../../assets/images/MSAA_eg1.jpg" width="380" alt="Games 101"/> <figcaption><center style="font-size:80%;">(b) Triangle Traversal Per Sub-pixel</center></figcaption> </figure> </div> </figure> <center> <p style="font-size:80%;"> Figure 6. MSAA </p> </center> <p><strong>Example 3.3</strong>: The triangle traversal in assignment 2 with MSAA would be like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>void rasterize_triangle(Triangle t):
    Construct a bounding box of t to accelerate the computation;
    for each pixel (w, h) in the bounding box:
        int count = 0;
        for each sub-pixel (w_, h_) in the pixel (w, h):
            if(insideTriangle(Point(w_+0.25, h_+0.25), t)):
                Get the z value by Barycentric interpolation;
                if(depth_buffer[w_, h_] &gt; z):
                    // the closer one that should be rasterized
                    depth_buffer[w_, h_] = z;
                    count++;
        if(count):
            frame_buffer[w, h] += color * count / 4;
</code></pre></div></div> <p>The rendering result with MSAA is shown in Fig 7. In MSAA, we need to maintain a depth buffer for all the sub-pixels. Otherwise there will be black squares along the overlap edge as we shrink the colour values of the pixel by the averaging operation.</p> <figure> <div style="display:flex"> <figure> <img src="../../../../assets/images/MSAA.png" alt="Code" width="380"/> <figcaption><center style="font-size:80%;">(a) Rendering Result </center></figcaption> </figure> <figure> <img src="../../../../assets/images/MSAA_300.jpg" alt="Code" width="380"/> <figcaption><center style="font-size:80%;">(b) Zoom In</center></figcaption> </figure> </div> </figure> <center> <p style="font-size:80%;"> Fig 7. MSAA of Assignment 2 </p> </center> <p>All the examples we provide above do not require a specific shading inside a triangle, which means all the pixels of a triangle will be in the same colour. In most cases, the colour of a triangle varies from pixel to pixel. And if we have no information about the vertex colour inside a triangle, we can, again, get one by interpolation. We now discuss four different shaders.</p> <figure> <div style="display:flex"> <figure> <img src="../../../../assets/images/normal.png" alt="Code" width="380"/> <figcaption><center style="font-size:80%;">(a) Normal Shader</center></figcaption> </figure> <figure> <img src="../../../../assets/images/phong.png" alt="Code" width="380"/> <figcaption><center style="font-size:80%;">(b) Blinn-Phong Shader</center></figcaption> </figure> </div> </figure> <figure> <div style="display:flex"> <figure> <img src="../../../../assets/images/texture.png" alt="Code" width="380"/> <figcaption><center style="font-size:80%;"> (c) Texture Shader</center></figcaption> </figure> <figure> <img src="../../../../assets/images/texture_bilinear.png" alt="Code" width="380"/> <figcaption><center style="font-size:80%;">(d) Binlinear Texture Shader</center></figcaption> </figure> </div> </figure> <center> <p style="font-size:80%;"> Fig 8. Shaders </p> </center> <p><strong>Normal Shader</strong>: Consider the case where we want to shade each pixel based on the normal of the point to which it is corresponding. All the normal of the inside points are obtained by interpolation.</p> <p><strong>Example 3.4</strong>: In assignment 3, we are provided with a basic normal shader. It works as follows</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Vector3f normal_shader(Vector3f normal):
    Vector3f color = 255 * (normal + Vector3f(1.0f, 1.0f, 1.0f)) / 2.0f;
    return color;

void rasterize_triangle(Triangle t):
    Construct a bounding box of `t` to accelerate the computation;
    for each pixel (w, h) in the bounding box:
        if(insideTriangle(Point(w+0.5, h+0.5), t)):
            Get the z value by Barycentric interpolation;
            Get the normal of the point Point(w+0.5, h+0.5) by Barycentric interpolation;
            color = normal_shader(normal);
            if(depth_buffer[w, h] &gt; z):
                // the closer one that should be rasterized
                depth_buffer[w, h] = z;
                frame_buffer[w, h] = color;
</code></pre></div></div> <p>The rendering result is shown Fig 8 (a). The blue part indicates the triangles on that part are generally perpendicular to the view ray, and their normal is more like \({0, 0, 1}\), which hence makes them more blue. The normal information can also be obtained from normal map. It is very common to use normal map to trick human eyes that there are a great many surface details. (See bump shader and displacment shader.)</p> <p><strong>Blinn-Phong Shader</strong>: Blinn-Phong shader is based on Blinn-Phong reflectance model. It allows us to take lights into consideration without bringing in too much computation cost. In Blinn-Phong reflectance model, the reflection consists of Lambertian Diffuse, Blinn-Phong Specular, and Ambient. Intuitively, Lambertian diffuse assumes the shading is independent of the view direction, and the ray direction will only affect the intensity of the light. Blinn-Phong specular assumes the view direction will also affect the light intensity at the shading point, and the closer the view direction is to the mirror direction of the ray direction, the higher the intensity at the point. Ambient can be viewed as a constant to simulate the whole environmental reflectance.</p> <p><strong>Example 3.5</strong>: In assignment 3, we are required to implement a Phong shader. To simplify the computation, I implement a Blinn-Phong shader as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Vector3f blinn_phong_shader(Fragment_Shader_Payload payload):
    // pre-define the parameters of ambient, diffuse, and specular as `ka`, `kd`, `ks`
    // assume `normal` is the normal of the shading point
	
    Vector3f color = {0, 0, 0};
	
    // Ambient
    color += ka * ambient_light_intensity;
	
    for each light source:
        Vector3f light_vec = light.position - point;
        Vector3f view_vec = eye.position - point;
        Vector3f bisector = (light_vec + view_vec).normalized();
		
        // Lambertain diffuse
        float r2 = light_vec.dot(light_vec);	// the distance
        color += kd.crossProduct(light.intensity) / r2 * max(0, normal.dot(light_vec.normalized())); 
		
        // Specular
        color += ks.crossProduct(light.intensity) / r2 * max(0, normal.dor(bisector)^150);
		
    return color;
</code></pre></div></div> <p>The rendering result is shown Fig 8 (b). As we can see, the image now can show us the reflectance, which makes it more realistic.</p> <p><strong>Texture Shader</strong>: Now consider that we want to have colourful patterns rather than normal or reflectance. One way to specify the colour is by using texture map. Texture shader is usually the same as the Blinn-Phong shader. The only difference is that the parameter \(k_d\) for Lambertian diffuse is replaced by the one extracted from a texture map.</p> <p><strong>Example 3.6</strong>: In assignment 3, we are required to implement a Texture shader. It can be done by modifying Blinn-Phong shader as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Vector3f texture_shader(Fragment_Shader_Payload payload):
    // pre-define the parameters of ambient, diffuse, and specular as `ka`, `kd`, `ks`
    // assume `normal` is the normal of the shading point
	
    kd = payload.texture.getColor(u, v) / 255.f;
	
    Vector3f color = {0, 0, 0};
	
    // compute color based on Blinn-Phong reflectance model
		
    return color;
</code></pre></div></div> <p>The rendering result is shown in Fig 8 (c). However, if we zoom in the resulting image, we can tell that there are jaggies again in the result. Instead of using MSAA, we can use Bilinear interpolation here to remove jaggies. The idea of Bilinear interpolation is shown in Fig 9. It is trying to get an averaging colour so that the transition from one pixel to the other can be as smooth as possible. The rendering result of Bilinear interpolation is shown in Fig 8 (d).</p> <div align="center"><img src="../../../../assets/images/Bilinear.jpg" width="750" alt="Games101"/> </div> <center> <p style="font-size:80%;"> Figure 9. Bilinear Interpolation </p> </center> <h1 id="4-conclusion">4. Conclusion</h1> <p>This post is a hasty summary about graphics pipeline after my completing the first four assignments. Before this, though I had read many articles about graphics pipeline, some concepts were very abstract to me. After getting hands dirty, I finally have a practical perspective about how we draw things literally. Many thanks to this great course!</p>]]></content><author><name></name></author><category term="Computer-Graphics"/><summary type="html"><![CDATA[A brief summary about graphics pipeline from a practical perspective.]]></summary></entry><entry><title type="html">Paper - Reinforcement Learning with Deep Energy-Based Policies</title><link href="https://theallen1996.github.io/blog/2021/soft_q_learning/" rel="alternate" type="text/html" title="Paper - Reinforcement Learning with Deep Energy-Based Policies"/><published>2021-01-11T00:00:00+00:00</published><updated>2021-01-11T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2021/soft_q_learning</id><content type="html" xml:base="https://theallen1996.github.io/blog/2021/soft_q_learning/"><![CDATA[<p><em>This is a brief summary of paper <a href="https://arxiv.org/pdf/1702.08165.pdf">Reinforcement Learning with Deep Energy-Based Policies</a> for my personal interest.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-motivation" id="markdown-toc-1-motivation">1. Motivation</a></li> <li><a href="#2-soft-definition" id="markdown-toc-2-soft-definition">2. Soft Definition</a> <ul> <li><a href="#21-objective-function" id="markdown-toc-21-objective-function">2.1. Objective Function</a></li> <li><a href="#22-soft-function" id="markdown-toc-22-soft-function">2.2. Soft Function</a></li> </ul> </li> <li><a href="#3-theorem-analyses" id="markdown-toc-3-theorem-analyses">3. Theorem Analyses</a> <ul> <li><a href="#31-policy-improvement" id="markdown-toc-31-policy-improvement">3.1. Policy Improvement</a></li> <li><a href="#32-policy-iteration" id="markdown-toc-32-policy-iteration">3.2. Policy Iteration</a></li> <li><a href="#33-soft-bellman-equation" id="markdown-toc-33-soft-bellman-equation">3.3. Soft Bellman Equation</a></li> <li><a href="#34-soft-value-iteration" id="markdown-toc-34-soft-value-iteration">3.4. Soft Value Iteration</a></li> </ul> </li> <li><a href="#4-algorithm" id="markdown-toc-4-algorithm">4. Algorithm</a></li> <li><a href="#4-conclusion" id="markdown-toc-4-conclusion">4. Conclusion</a></li> <li><a href="#5-references" id="markdown-toc-5-references">5. References</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>After publishing the paper <a href="https://arxiv.org/pdf/1702.08165.pdf">soft Q learning</a> in Jul 2017, the author proposed the influential algorithm <a href="https://arxiv.org/pdf/1801.01290.pdf">SAC</a> in the next year. While SAC has received tremendous publicity, the discussion, if any, about this precedent work generally falls into the rut of <a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">the excellent blog</a>. Therefore I decided to summarize some of thoughts and problems I had during the reading. Though this post is mainly for my personal interest, any advice will be appreciated.</p> <h1 id="1-motivation">1. Motivation</h1> <p>In reinforcement learning (RL) problem, given an agent and an environment, the objective of the agent is to learn a policy that maximizes the rewards the agent can gain. However, faced with an unknown environment, there is a tradeoff between exploitation and exploration. Before maximum entropy RL, the exploration is generally ensured by external mechanisms, such as \(\varepsilon-\)greedy in DQN and adding exploratory noise to the actions in DDPG. Un like those heuristic and inefficient methods, maximum entropy encourages the agent to conduct exploration by itself based on both reward and entropy. Specifically, in maximum entropy RL, the optimal policy is redefined as</p> \[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\mathbb{E}_{\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{1}\] <p>which adds a regularization term to the standard definition. Many paper adds a parameter \(\alpha\) to entropy, which we will ignore in the following discussion as it does not affect the related conclusion. Based on this redefinition, the motivation of this paper can be summarized as follows:</p> <ul> <li> <p>Generalize it to continuous cases:</p> <p>Before this work, entropy maximization was mainly utilized in discrete cases. Theoretical analyses were needed for applying into continuous cases.</p> </li> <li> <p>Take trajectory-wise entropy into consideration:</p> <p>The term \(\mathcal{H}(\pi(\cdot\vert s_t))\) only considers the entropy of the policy at state \(s_t\). Traditional methods tend to act greedily based on the entropy at the next state. In this work, the author considers the long term entropy reward instead that of the next state.</p> </li> <li> <p>Policy formulation:</p> <p>Even though we have an objective function given the definition, it still needs a probabilistic definition from which we can make sampling. Instead of using conditional Gaussian distribution like many other works, the author borrows the idea of Boltzmann distribution.</p> </li> </ul> <h1 id="2-soft-definition">2. Soft Definition</h1> <p>We now discuss the related definition in this paper.</p> <h2 id="21-objective-function">2.1. Objective Function</h2> <p>The optimal policy in this paper is defined as</p> \[\pi^\ast=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{2}\] <p>which differs from the one defined in \((1)\) as it aims to reach states where they may have high entropy in the future. Specifically, a detailed version is given by</p> \[\pi^\ast=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\left[r(s_l,a_l)+\mathcal{H}(\pi(\cdot\vert s_l))\right]\bigg\vert s_t,a_t\right],\tag{3}\] <p>where the first expectation \(\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\) is over all the pairs \((s_t, a_t)\) at any time step, and the second expectation \(\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\) is over all the trajectories originating from \((s_t,a_t)\).</p> <blockquote> <p>In the original paper, the detailed version is actually given by</p> \[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right]\bigg\vert s_t,a_t\right].\] <p>Problem: The subscripts (shown in red below) in the second expectation does confuse me a lot. Why it is over \(t\) rather than \(l\)?</p> \[r(s_{\color{red}t},a_{\color{red}t})+\mathcal{H}(\pi(\cdot\vert s_{\color{red}t}))?\] </blockquote> <h2 id="22-soft-function">2.2. Soft Function</h2> <p>The corresponding \(Q\)-function in this paper is defined as</p> \[Q_\text{soft}^\pi(s_t,a_t)\triangleq r(s_t,a_t)+\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right].\tag{4}\] <p>Notice that the entropy at state \(s_t\) is omitted in the definition.</p> <p>The corresponding value function is given by</p> \[V_\text{soft}^\pi(s_t)\triangleq \log \int_{\mathcal{A}}\exp\left(Q_\text{soft}^\pi(s_t,a)\right)da,\tag{5}\] <p>which is actually in the form of log sum exponential that approximates maximum.</p> <p>Given the two definitions, the soft policy is then given by</p> \[\pi(a_t\vert s_t)=\exp\left(Q^\pi_\text{soft}(s_t,a_t)-V^\pi_\text{soft}(s_t)\right).\tag{6}\] <p>As \(V^\pi_\text{soft}\) only depends on \(Q^\pi_\text{soft}\), the soft policy is actually the Boltzmann distribution based on the value of \(Q^\pi_\text{soft}\). The comparison is shown in Figure 1. It can be found that Boltzmann distribution assigns a reasonable likelihood for all actions (rather than just the optimal one).</p> <figure> <div style="max-width: 500px;"> <figure> <img src="http://bair.berkeley.edu/static/blog/softq/figure_3a_unimodal-policy.png"/> <figcaption><center>(a)</center></figcaption> </figure> <figure> <img src="http://bair.berkeley.edu/static/blog/softq/figure_3b_multimodal_policy.png"/> <figcaption><center>(b)</center></figcaption> </figure> </div> </figure> <center> <p style="font-size:100%;"> Figure 1. Policies based on the value of Q function. (a) Unimodal policy. (b) Multimodal policy. </p> </center> <p>With those definitions, we have proposed the solutions to the problems mentioned in the motivation: continuous function for continuous states and actions space; the trajectory-wise optimization defined by the objective function \((3)\); and Boltzmann distribution to represent the optimal policy. To ensure things work, we need theoretical analyses and feasible update rules.</p> <h1 id="3-theorem-analyses">3. Theorem Analyses</h1> <p>We now discuss the related theoretical guarantee. The first theorem shows us the optimality:</p> <p><strong>Theorem 1</strong>: The optimal policy for equation (3) is given by</p> \[\pi^\ast(a_t\vert s_t)=\exp\left(Q^\ast_\text{soft}(s_t,a_t)-V^\ast_\text{soft}(s_t)\right).\] <p>The proof sketch follows two steps: policy improvement and policy iteration.</p> <h2 id="31-policy-improvement">3.1. Policy Improvement</h2> <p>We first show that given any policy, we can improve it by ‘softmizing’ it. Specifically,</p> \[\forall\pi,\text{let }\tilde\pi(\cdot\vert s)\propto\exp\left(Q^\pi_\text{soft}(s,\cdot)\right),\text{then }Q^\pi_\text{soft}(s_t,a_t)\le Q^{\tilde\pi}_\text{soft}(s_t,a_t).\] <p>To show that, we rewrite the second part of \(Q\) function (defined in (4)) as</p> \[\begin{aligned}&amp;\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[[\gamma\mathcal{H}(\pi(\cdot\vert s_{l}))+\gamma r(s_{t+1},a_{t+1})\\&amp;+\sum_{l=t+2}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))]]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[\gamma\mathcal{H}(\pi(\cdot\vert s_{l}))+\gamma (r(s_{t+1},a_{t+1})\\&amp;+ \sum_{l=t+2}^\infty \gamma^{l-t-1}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))])]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[\gamma\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\gamma Q^\pi_\text{soft}(s_{t+1},a_{t+1})].\end{aligned}\] <p>As the entropy term is independent of \(a_{t+1}\), we then have the following equation</p> \[Q^\pi_\text{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}\left[Q^\pi_\text{soft}(s_{t+1},a_{t+1})]\right].\tag{7}\] <p>We then provide an inequality:</p> \[\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\le \mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right].\tag{8}\] <p><em>Proof of (8):</em></p> <p>We rewrite the left hand side of the inequality</p> \[\begin{aligned}&amp;\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\\=&amp;\mathbb{E}_{a_t\sim \pi}\left[{\color{red}{-\log\pi(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \pi}\left[-\log\pi(a_t\vert s_t){\color{red}{+\log\tilde{\pi}(a_t\vert s_t)-\log\tilde{\pi}(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;-\mathbb{E}_{a_t\sim \pi}\left[\log\pi(a_t\vert s_t)-\log\tilde{\pi}(a_t\vert s_t)\right]+\mathbb{E}_{a_t\sim \pi}\left[Q^\pi_\text{soft}(s_t,a_t)-\log\tilde{\pi}(a_t\vert s_t)\right]\\=&amp;-D_\text{KL}(\pi\vert\vert\tilde{\pi})+\mathbb{E}_{a_t\sim\pi}\left[Q^\pi_\text{soft}(s_t,a_t)-Q^\pi_\text{soft}(s_t,a_t)+\log\int_\mathcal{A}\exp(Q^\pi_\text{soft}(s_t,a'))da'\right]\\=&amp;-D_\text{KL}(\pi\vert\vert\tilde{\pi})+\log\int_\mathcal{A}\exp(Q^\pi_\text{soft}(s_t,a'))da’.\end{aligned}\] <p>For the right hand side of the inequality, we have</p> \[\begin{aligned}&amp;\mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[{\color{red}{-\log\tilde\pi(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[-\log\frac{\exp\left(Q^\pi_\text{soft}(s_t,a_t)\right)}{\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da'}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[-Q^\pi_\text{soft}(s_t,a_t)+\log\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da'+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\log\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da’.\end{aligned}\] <p>Since \(D_\text{KL}\ge 0\), we have</p> <p>\(\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\le \mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right].\tag*{\)\blacksquare\(}\)</p> <p>With (7) and (8), we now ready to show policy improvement. The idea is simple: we use inequality (8) to contract the right hand side of equality (7) to complete the proof.</p> <p><em>Proof of policy improvement:</em></p> \[\begin{aligned}Q_\text{soft}^\pi(s_t,a_t)=&amp; r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}[Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\right]\\\le&amp; r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[Q^\pi_\text{soft}(s_{t+1},a_{t+1})]\right]\\=&amp;r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))\\&amp;+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+2}\sim\mathcal{P}}[\mathcal{H}(\pi(\cdot\vert s_{t+2}))+\mathbb{E}_{a_{t+2}\sim\pi}[Q_\text{soft}^\pi(s_{t+2},a_{t+2})]]]\\\le &amp;r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))\\&amp;+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+2}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+2}))+\mathbb{E}_{a_{t+2}\sim{\color{red}{\tilde{\pi}}}}[Q_\text{soft}^\pi(s_{t+2},a_{t+2})]]]\\ \vdots \\ \le &amp; r(s_t,a_t)+\sum_{l=t+1}^\infty \mathbb{E}_{(s_l,a_l)\sim\rho_{\tilde\pi}}\left[r(s_l,a_l)+\mathcal{H}(\tilde\pi(\cdot\vert s_l))\right]\\=&amp; Q^{\tilde\pi}_\text{soft}(s_t,a_t).\end{aligned}\] <p>Therefore we complete the proof. \(\tag*{\)\blacksquare\(}\)</p> <h2 id="32-policy-iteration">3.2. Policy Iteration</h2> <p>With <em>policy improvement</em> theorem, we can improve any arbitrary policy. Therefore the policy can be naturally updated by</p> \[\pi_{i+1}(\cdot \vert s_t)\propto \exp\left(Q^{\pi_i}_\text{soft}(s_t,\cdot)\right).\] <p>Since any policy can be improved in this way, the optimal policy must satisfy this form, and the proof of <em>Theorem 1</em> is completed. \(\tag*{\)\blacksquare\(}\)</p> <h2 id="33-soft-bellman-equation">3.3. Soft Bellman Equation</h2> <p>Though we have that the optimal policy can be obtained by policy iteration, it would be exhausting to conduct the iteration exactly in that way (just think about the integral we omit with the help of \(\propto\))! Therefore, a more feasible way is to find the optimal \(Q\) function (which is why they call the algorithm <em>soft Q learning</em>, I guess) as</p> \[\pi^\ast(a_t\vert s_t)\propto\exp\left(Q^\ast_\text{soft}(s_t,a_t)\right).\] <p>We now show the soft Bellman optimality equation which connects the two optimal function.</p> <p><strong>Theorem 2.</strong> The soft \(Q\) function defined in (4) satisfies the soft Bellman equation</p> \[Q^\ast_\text{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\ast_\text{soft}(s_{t+1})\right].\] <p><em>Proof of Theorem 2</em>:</p> <p>The proof is pretty straightforward. Notice that</p> \[\begin{aligned}&amp;\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}[Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\ =&amp;\mathbb{E}_{a_{t+1}\sim\pi}[-\log \pi(a_{t+1}\vert s_{t+1})+Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\ =&amp;\mathbb{E}_{a_{t+1}\sim\pi}[-\log \exp(Q_\text{soft}^\pi(s_{t+1},a_{t+1})-V_\text{soft}^\pi(s_{t+1}))+Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\ =&amp;\mathbb{E}_{a_{t+1}\sim\pi}[V_\text{soft}^\pi(s_{t+1})]\\ =&amp;V_\text{soft}^\pi(s_{t+1}).\end{aligned}\] <p>Therefore the soft \(Q\) function defined in (4) is equivalent to</p> \[\begin{aligned}Q_\text{soft}^\pi(s_t,a_t)&amp;= r(s_t,a_t)+\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right]\\&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\pi_\text{soft}(s_{t+1})\right].\end{aligned}\] <p>\(\tag*{\)\blacksquare\(}\)</p> <p>Theorem 2 actually sheds light on how we update our \(Q\) function, which will be introduced in next section.</p> <h2 id="34-soft-value-iteration">3.4. Soft Value Iteration</h2> <p>So far we have shown the optimality of soft policy (<em>Theorem 1</em>) and soft \(Q\) function <em>(Theorem 2)</em>. However, we still need a rule to learn the function. Specifically, we mainly focus on the update rule of \(Q\) function as the policy and value function both are defined by \(Q\) function. To this end, the author provides the following theorem.</p> <p><strong>Theorem 3.</strong> The iteration</p> \[Q^\pi_\text{soft}(s_t,a_t)\gets r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\pi_\text{soft}(s_{t+1})\right],\] \[V_\text{soft}^\pi(s_t)\gets \log \int_{\mathcal{A}}\exp\left(Q_\text{soft}^\pi(s_t,a)\right)da,\] <p>converges to \(Q^\ast_\text{soft}\) and \(V^\ast_\text{soft}\), respectively.</p> <p>The proof is quite similar to the general case in RL. For the detailed proof one can refer to the paper directly. Notice that the update of \(Q\) function does not involve the policy, therefore it is an off-policy RL.</p> <h1 id="4-algorithm">4. Algorithm</h1> <p>Given the above analyses, there are two key issues in designing a truly practical algorithm:</p> <ul> <li>The intractable integral for computing the value of \(V^\pi_\text{soft}\);</li> <li>The intractable sampling from Boltzmann distribution.</li> </ul> <p>To deal with the integral issue, this paper leverages <em>importance sampling</em>, which has been widely used in many previous works. For the second issue, generally speaking, the author uses a neural network to approximate the Boltzmann distribution of the policy (rather than the policy itself, and that differs from actor-critic, claimed by the author), and the loss function is defined as</p> \[J_\pi(\phi;s_t)=D_\text{KL}\left(\pi^\phi(a_t\vert s_t)\bigg\vert\bigg\vert\exp\left(Q^\theta_\text{soft}(s_t,a_t)-V^\theta_\text{soft}(s_t)\right)\right).\] <p>The gradient of the loss function is given by <em>Stein Variational Gradient Descent</em> (SVGD). In my view, the use of SVGD is mainly for the analysis of the resemblance between the proposed algorithm, soft Q learning (SQL), and actor-critic, as the succeeding works seem to use no SVGD anymore.</p> <p>The author provides the implementation in <a href="https://github.com/haarnoja/softqlearning">github-softqlearning</a>. However, the latest version is faced with dependencies issue. Luckily, the older version (committed on Oct 30, 2017) works well. Other feasible implementation can be hardly found. For the performance, I tested it on Multigoal environment and the results are consistent with that of the original paper. Besides, I conducted experiments with varying values of \(\alpha\), which is shown in Figure 2.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/images/sql_alpha0_500-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/images/sql_alpha0_500-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/images/sql_alpha0_500-1600.webp"/> <img src="/assets/images/sql_alpha0_500.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="$$\alpha=0$$" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/images/sql_alpha05_500-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/images/sql_alpha05_500-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/images/sql_alpha05_500-1600.webp"/> <img src="/assets/images/sql_alpha05_500.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="$$\alpha=0.5$$" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/images/sql_alpha1_500-680.webp"/> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/images/sql_alpha1_500-900.webp"/> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/images/sql_alpha1_500-1600.webp"/> <img src="/assets/images/sql_alpha1_500.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="$$\alpha=1$$" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. After 500 steps, the performance of SQL in Multigoal with differnt values of alpha=0, 0.5, 1, respectively. </div> <p>Generally speaking, when \(\alpha\ne 0\), SQL tends to have a high variance, which can also be viewed as the cost for exploration.</p> <h1 id="4-conclusion">4. Conclusion</h1> <p>It was proven that bringing in the entropy term does work for the end of better exploring. The author successfully extended the entropy RL framework to contiunous case in this paper. However, the high variance makes it challenging to be used for performing complicated tasks, and that may be one of the reasons why little (relatively) ink has been spilled on SQL. A wise choice could be to use SQL as an initializer rather than a trainer.</p> <h1 id="5-references">5. References</h1> <p><a href="https://arxiv.org/pdf/1702.08165.pdf">Reinforcement Learning with Deep Energy-Based Policies</a> - Tuomas Haarnoja et al.</p> <p><a href="https://arxiv.org/pdf/1801.01290.pdf">Soft Actor-Critic</a> - Tuomas Haarnoja et al.</p> <p><a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning</a> - BAIR</p> <p><a href="https://julien-vitay.net/deeprl/EntropyRL.html">Deep Reinforcement Learning</a> - Julien Vitay</p> <p><a href="https://www.slideshare.net/DongMinLee32/maximum-entropy-reinforcement-learning-stochastic-control">Maximum Entropy Reinforcement Learning (Stochastic Control)</a> - Dongmin Lee</p> <p><a href="https://arxiv.org/pdf/1608.04471.pdf">Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a> - Qiang Liu and Dilin Wang</p>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><summary type="html"><![CDATA[A personal summary of paper Reinforcement Learning with Deep Energy-Based Policies.]]></summary></entry><entry><title type="html">Machine Learning - 09 Exact Inference of Graphical Models</title><link href="https://theallen1996.github.io/blog/2020/inference-ml09/" rel="alternate" type="text/html" title="Machine Learning - 09 Exact Inference of Graphical Models"/><published>2020-12-05T00:00:00+00:00</published><updated>2020-12-05T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2020/inference-ml09</id><content type="html" xml:base="https://theallen1996.github.io/blog/2020/inference-ml09/"><![CDATA[<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>, <a href="https://ermongroup.github.io/cs228-notes/">CS228-notes</a> and <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">PRML</a>. For the fundamental of probability, one can refer to <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view">Introduction to Probability</a>. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-variable-elimination" id="markdown-toc-1-variable-elimination">1. Variable Elimination</a></li> <li><a href="#2-belief-propagation" id="markdown-toc-2-belief-propagation">2. Belief Propagation</a> <ul> <li><a href="#21-message" id="markdown-toc-21-message">2.1. Message</a></li> <li><a href="#22-sum-product" id="markdown-toc-22-sum-product">2.2. Sum-Product</a></li> <li><a href="#23-max-product" id="markdown-toc-23-max-product">2.3 Max-Product</a></li> </ul> </li> <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>In the last <a href="https://2ez4ai.github.io/2020/11/29/probabilistic_graphical_models-ml08/">post</a>, we introduce graphical models which is capable of representing random variables and the conditional independences among them. We now consider the problem of inference in graphical models. Particularly, we wish to compute posterior distributions of one or more nodes conditioned on some other known (observed) nodes, and the techniques we shall talk in this post are for <em>exact inference</em>.</p> <h1 id="1-variable-elimination">1. Variable Elimination</h1> <p>We first consider the marginal inference in a <em>chain</em> Bayesian network which has the following joint distribution,</p> \[P(x_1,x_2,\dots,x_n)=P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1}).\] <p>Suppose we want to infer the marginal distribution \(P(x_n)\). A naive way to do that is marginalizing \(x_1,x_2,\dots,x_{n-1}\):</p> \[P(x_n)=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1,x_2,\dots,x_{n-1}).\] <p>However, as there are \(n-1\) variables each with \(k\) states, the computation needs to sum the probability over \(k^{n-1}\) values and would scale exponentially with the length of the chain. To simplify the computation, we can leverage <em>variable elimination</em>.</p> <p>The key of <em>variable elimination</em> is in <em>rearranging the order</em> of the summations and the multiplications. Specifically, the marginal distribution follows that</p> \[\begin{aligned}P(x_n)&amp;=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1})\\&amp;=\sum_{x_{n-1}}P(x_n\vert x_{n-1})\cdot \sum_{x_{n-2}}P(x_{n-1}\vert x_{n-2})\cdot\dots\cdot \sum_{x_{1}}P(x_{2}\vert x_{1})P(x_1).\end{aligned}\] <p>Such a rearrangement works because multiplication is distributive over addition,</p> \[ab+ac=a(b+c),\] <p>where the number of arithmetic operations are reduced from three (the left-hand side) to two (the right-hand side). Before we move on, we generalize the new expression to a Markov network since every Bayesian network can be transformed into a Markov network. For the chain Bayesian network we considered, we can remove all the arrows of the graph to obtain a Markov network. The obtained Markov network would have maximum cliques \(\{x_1,x_2\}\), …, \(\{x_{n-2},x_{n-1}\}\) and \(\{x_{n-1},x_{n}\}\) and the corresponding potentials are</p> \[\begin{aligned}\psi_{1,2}(x_1,x_2)&amp;=P(x_2\vert x_1)P(x_1),\\&amp;\vdots\\\psi_{x_{n-2},x_{n-1}}(x_{n-2},x_{n-1})&amp;=P(x_{n-2}\vert x_{n-1}),\\\psi_{n-1,n}(x_{n-1},x_n)&amp;=P(x_{n-1}\vert x_n).\end{aligned}\] <p>The rearrangement for the Markov network then follows that</p> \[P(x_n)=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1}, x_{n})\cdot \sum_{x_{n-2}}\psi_{n-2,n-1}(x_{n-2}, x_{n-1})\cdot\dots\cdot \sum_{x_{1}}\psi_{1,2}(x_1,x_2).\] <p>Now the computation composes of \(n-1\) summations. More importantly, unlike the previous one sums over \(k^{n-1}\) values, this expression allows each term only need to sum over \(k\times k\) values. Specifically, the sum</p> \[\sum_{x_i}\psi_{x_i,x_{i+1}}(x_{i},x_{i+1})\] <p>only involves two variables and thus the summation is over \(k\times k\) values. Then the overall computation is of \(O(nk^2)\) complexity, which is much better than the naive \(O(k^n)\) method.</p> <p>However, the variable elimination (VE) method requires an ordering over the variables. In fact, the running time of VE on different orderings would vary greatly, while to find the best ordering is still an NP-hard problem. Moreover, VE method for \(P(x_n)\) can be hard to be generalized to other marginal distribution as it does not store the intermediate results.</p> <h1 id="2-belief-propagation">2. Belief Propagation</h1> <p>For convenience, we consider undirected graphs with tree structure, where the optimal variable elimination ordering for node \(x_i\) is the post-order iteration of the subtree rooted at \(x_i\). The relationship between any two directly connected nodes is decided by which node the tree is rooted at and how far the two nodes are away from the root: the close one is the parent of the farther one.</p> <h2 id="21-message">2.1. Message</h2> <p>For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal \(P(x_i)\), we need to eliminate all nodes that are in the subtree of \(x_i\). For node \(x_j\), the elimination involves computing \(\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\) where \(x_k\) is the parent of \(x_j\) in the tree. The term \(m_{j,k}\) can be thought of a <em>message</em> that \(x_j\) sends to \(x_k\) about the subtree rooted at \(x_j\). Similarly, the computing result can be viewed as</p> \[m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\] <p>that contains the information for \(x_l\), the parent of \(x_k\), about the subtree rooted at \(x_k\). By doing so, at the end of VE, \(x_i\) would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.</p> <p>Suppose that after computing \(P(x_i)\), we are interested in computing \(P(x_k)\) as well. If we use VE algorithm again, we can find that the computation also involves the messages \(m_{j,k}\) as node \(x_k\) is still the parent of node \(x_j\). Moreover, such a message is exactly the same as the one used in computing \(P(x_i)\) since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.</p> <h2 id="22-sum-product">2.2. Sum-Product</h2> <p>Belief propagation can be viewed as a combination of VE and <em>caching</em>. For each edge between \(x_i\) and \(x_j\), the messages passing on it are \(m_{i,j}\) and \(m_{j,i}\), which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.</p> <p>Belief propagation:</p> <ul> <li>Set a node, for example, node \(x_i\), as the root;</li> <li>For each \(x_j\) in \(N(x_i)\), <em>i.e.,</em> the neighborhood of \(x_i\), collect the messages sent to \(x_i\):</li> </ul> \[m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};\] <ul> <li>For each \(x_j\) in \(N(x_i)\), collect the messages sent from \(x_i\):</li> </ul> \[m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.\] <p>By doing so, we can obtain all the messages with \(2\vert E\vert\) steps, where \(E\) is the set of edges. Then for any marginal we have</p> \[P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.\] <h2 id="23-max-product">2.3 Max-Product</h2> <p>We now consider a problem of finding the set of values that have the largest probability so that</p> \[\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).\] <p>Notice that</p> \[\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).\] <p>By <em>sum-product</em>, we have</p> \[\begin{aligned}\max_{\text{x}} P(\text{x})&amp;=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&amp;=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\\&amp;\quad\ \dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned}\] <p>Such a method for maximizing <em>max-product</em> is known as <em>max-product</em> algorithm.</p> <h1 id="3-conclusion">3. Conclusion</h1> <p>In this post, we briefly introduced two algorithms for <em>exact inference</em> in graphical models. Given a proper order of nodes, <em>variable elimination</em> algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, <em>belief propagation</em> stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random variables with the largest probability, which is known as <em>max-product</em>.</p>]]></content><author><name></name></author><category term="Machine-Learning"/><summary type="html"><![CDATA[A very abstract post as it involves graphs and trees while providing no figures and examples.]]></summary></entry><entry><title type="html">Machine Learning - 08 Probabilistic Graphical Models</title><link href="https://theallen1996.github.io/blog/2020/probabilistic_graphical_models-ml08/" rel="alternate" type="text/html" title="Machine Learning - 08 Probabilistic Graphical Models"/><published>2020-11-29T00:00:00+00:00</published><updated>2020-11-29T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2020/probabilistic_graphical_models-ml08</id><content type="html" xml:base="https://theallen1996.github.io/blog/2020/probabilistic_graphical_models-ml08/"><![CDATA[<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a> and <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">PRML</a>. For the fundamental of probability, one can refer to <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view">Introduction to Probability</a>. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-bayesian-networks" id="markdown-toc-1-bayesian-networks">1. Bayesian Networks</a> <ul> <li><a href="#11-conditional-independence" id="markdown-toc-11-conditional-independence">1.1. Conditional Independence</a> <ul> <li><a href="#111-tail-to-tail" id="markdown-toc-111-tail-to-tail">1.1.1. Tail-to-tail</a></li> <li><a href="#112-head-to-tail" id="markdown-toc-112-head-to-tail">1.1.2. Head-to-tail</a></li> <li><a href="#113-head-to-head" id="markdown-toc-113-head-to-head">1.1.3. Head-to-head</a></li> <li><a href="#114-d-separation" id="markdown-toc-114-d-separation">1.1.4. D-separation</a></li> </ul> </li> <li><a href="#12-markov-blanket" id="markdown-toc-12-markov-blanket">1.2. Markov Blanket</a></li> </ul> </li> <li><a href="#2-markov-network" id="markdown-toc-2-markov-network">2. Markov Network</a> <ul> <li><a href="#21-conditional-independence" id="markdown-toc-21-conditional-independence">2.1. Conditional Independence</a></li> <li><a href="#22-maximum-clique" id="markdown-toc-22-maximum-clique">2.2. Maximum Clique</a></li> <li><a href="#23-moralization" id="markdown-toc-23-moralization">2.3. Moralization</a></li> </ul> </li> <li><a href="#3-factor-graph" id="markdown-toc-3-factor-graph">3. Factor Graph</a></li> <li><a href="#4-conclusion" id="markdown-toc-4-conclusion">4. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>For a vector featured with multiple random variables like \(X=[x_1,x_2,\dots,x_n]\), what we care most are <em>marginal probability</em> \(P(x_i)\), <em>joint distribution</em> \(P(X)\) and <em>conditional probability</em> \(P(x_i\vert x_j)\):</p> <ul> <li> <p>Marginal distribution:</p> \[P(x_i)=\sum_{x_n}\dots\sum_{x_{i+1}}\sum_{x_{i-1}}\dots\sum_{x_1}P(X);\] </li> <li> <p>Joint distribution:</p> \[P(X)=P(x_1)\cdot\prod_{i=2}^{n}P(x_i\vert x_1,\dots,x_{i-1});\] </li> <li> <p>Conditional probability (<em>Bayesian rule</em>):</p> \[P(x_i\vert x_j)=\frac{P(x_i,x_j)}{P(x_j)}.\] </li> </ul> <p>Obviously when \(n\) is large, all of the above three can be of high computation complexity. Now we consider simplifying the computation of joint distribution, and to achieve that most methods have been proposed by</p> <ul> <li> <p>assuming all the features are totally <em>independent</em>:</p> \[P(X)=\prod_{i=1}^nP(x_i);\] </li> <li> <p>or, assuming that the features are <em>conditional independent</em>, which is used in <strong>naive Bayes classifier</strong> as class conditional independence:</p> \[P(X\vert Y)=\prod_{i=1}^n P(x_i\vert Y)\implies P(X)=\int_y \prod_{i=1}^nP(x_i\vert y)P(y)\text{d}y;\] </li> <li> <p>or, based on <em>conditional independent</em>, assuming that the features process the <em>Markov Property</em>, which is used in <strong>hidden Markov models</strong>:</p> \[P(x_j\vert x_1,x_2,\dots,x_{j-1})=P(x_j\vert x_{j-1})\implies P(X)=P(x_1)\cdot\prod_{i=2}^nP(x_i\vert x_{i-1}).\] </li> </ul> <p>Among them, <em>graphical probabilistic models</em> (PGM) are generally based on the <em>conditional independence assumption</em>, ‘<em>capturing the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables</em>’. (<a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">PRML</a>)</p> <p>By leveraging the manipulations and properties of graph, many probabilistic insights can be obtained, motivating the design of new models.</p> <h1 id="1-bayesian-networks">1. Bayesian Networks</h1> <p>We now consider general graphical probabilistic models <em>Bayesian networks</em> which are defined by <em>directed acyclic graphs</em> (DAGs). Given a DAG, the <em>nodes</em> in a Bayesian network represent random variables and edges represent conditional dependencies among those variables.</p> <div align="center"><img src="../../../../assets/images/Figure8.2.png" alt="Figure8.2 in PRML" width="250"/> </div> <center> <p style="font-size:80%;"> Figure 1. Example of a directed acyclic graph (Figure 8.2 of PRML). </p> </center> <p>A Bayesian network actually represents a joint distribution over all the random variables represented by its nodes. As it is shown in Figure 1 (from PRML), there are 7 random variables. The edge going from a node \(x_i\) to a node \(x_j\) indicates that \(x_j\) is dependent to \(x_i\). The joint distribution of the model in Figure 1 is then given by</p> \[P(x_1,x_2,\dots,x_7)=P(x_1)P(x_2)P(x_3)P(x_4\vert x_1,x_2,x_3)P(x_5\vert x_1,x_3)P(x_6\vert x_4)P(x_7\vert x_4,x_5).\] <p>More generally, for a graph with \(K\) nodes, the corresponding joint distribution is given by</p> \[P(X)=\prod_{k=1}^KP(x_k\vert \mathbb{Pa}_k),\] <p>where \(\mathbb{Pa}_k\) denotes the set of parents of \(x_k\) and \(X=(x_1,x_2,\dots,x_k)\). Such a key equation expresses the <em>factorization</em> properties of the joint distribution for a directed graph. Compared with the joint distribution expression mentioned in Section 0, the expression of that of Figure 1 we show above definitely is simplified a lot. Such simplification is actually introduced by the <em>absence</em> of edges in the graph, which conveys the <em>conditional independence</em> information of those variables.</p> <h2 id="11-conditional-independence">1.1. Conditional Independence</h2> <p>Conditional independence sometimes can greatly simplify the computations needed to perform inference and learning under probabilistic models. By using graphical models, we can find the conditional independence properties directly without effort to any analytical manipulations. We start the discussion by considering three simple examples each involving graphs having just three nodes.</p> <h3 id="111-tail-to-tail">1.1.1. Tail-to-tail</h3> <div align="center"><img src="../../../../assets/images/Figure8.15.png" alt="Figure8.15 in PRML" width="250"/> </div> <center> <p style="font-size:80%;"> Figure 2. Example of tail-to-tail for conditional independence (Figure 8.15 of PRML). </p> </center> <p>Intuitively, we can consider the path from node <em>a</em> to node <em>b</em> via <em>c</em> in Figure 2. The node <em>c</em> is said to be <em>tail-to-tail</em> as the node is connected to the tails of the two arrows. Given such a path, we have \(a\) and \(b\) dependent. However, if we condition on node <em>c</em>, then the conditioned node <em>c</em> will <em>block</em> the path and cause \(a\) and \(b\) to become conditionally independent. A naive proof is as follows.</p> <p>By the definition given in section 1, the joint distribution of the graph in Figure 2 is</p> \[P(a,b,c)=P(c)P(a\vert c)P(b\vert c).\] <p>By the general definition of the marginal distribution, we have</p> \[P(a,b)=\sum_c P(c)P(a\vert c)P(b\vert c),\] <p>which generally dose not factorize into the product \(P(a)\cdot P(b)\), therefore \(a\) and \(b\) are dependent, denoted as</p> \[a\not\perp b\ \ \ \ \ \ \text{or, equivalenly,}\ \ \ \ \ \ a\not\!\perp\!\!\!\perp b\vert \emptyset.\] <p>By the general definition of the joint distribution, we have</p> \[P(a,b,c)=P(c)P(a\vert c)P(b\vert a,c).\] <p>Thus it follows that</p> \[\begin{alignat*}{3}&amp;&amp;P(c)P(a\vert c)P(b\vert a,c)&amp;=P(c)P(a\vert c)P(b\vert c)\\\implies&amp;&amp;P(b\vert a,c)&amp;=P(b\vert c)\\\implies&amp;&amp;\frac{P(a,b\vert c)}{P(a\vert c)}&amp;=P(b\vert c)\\\implies&amp;&amp;P(a,b\vert c)&amp;=P(a\vert c)P(b\vert c),\end{alignat*}\] <p>which means</p> \[a\perp\!\!\!\perp b\vert c.\] <h3 id="112-head-to-tail">1.1.2. Head-to-tail</h3> <div align="center"><img src="../../../../assets/images/Figure8.17.png" alt="Figure8.17 in PRML" width="250"/> </div> <center> <p style="font-size:80%;"> Figure 3. Example of head-to-tail for conditional independence (Figure 8.17 of PRML). </p> </center> <p>The example of head-to-tail is shown in Figure 3. Similarly, the node <em>c</em> is said to be <em>head-to-tail</em> with respect to the path from node <em>a</em> to node <em>b</em>. Given such a path, we have \(a\) and \(b\) dependent. When node <em>c</em> is conditioned, the case would be the same as <em>tail-to-tail</em>: the conditioned node <em>c</em> would <em>block</em> the path and render \(a\) and \(b\) conditional independent. The naive proof of this is as follows.</p> <p>The joint distribution of the model in Figure 3 is</p> \[P(a,b,c)=P(a)P(c\vert a)P(b\vert c).\] <p>Marginalizing the distribution over \(c\), we have</p> \[\begin{aligned}P(a,b)&amp;=\sum_c P(a)P(c\vert a)P(b\vert c)\\&amp;=P(a)\sum_c P(b,c\vert a)\\&amp;=P(a)P(b\vert a),\end{aligned}\] <p>which means \(a\not\perp b\).</p> <p>By rewriting the general joint distribution expression, we have</p> \[P(a,b,c)=P(a)P(b\vert a)P(c\vert a,b).\] <p>Then we have</p> \[\begin{alignat*}{3}&amp;&amp;P(a)P(b\vert a)P(c\vert a,b)&amp;=P(a)P(c\vert a)P(b\vert c)\\\implies&amp;&amp; P(b\vert a)\cdot\frac{P(a,b\vert c)P(c)}{P(b\vert a)P(a)}&amp;=\frac{P(a\vert c)P(c)}{P(a)}\cdot P(b\vert c)\\\implies&amp;&amp;P(a,b\vert c)&amp;=P(a\vert c)P(b\vert c)\\\implies&amp;&amp;a&amp;\perp\!\!\!\perp b\vert c.\end{alignat*}\] <h3 id="113-head-to-head">1.1.3. Head-to-head</h3> <div align="center"><img src="../../../../assets/images/Figure8.19.png" alt="Figure8.19 in PRML" width="250"/> </div> <center> <p style="font-size:80%;"> Figure 4. Example of head-to-head for conditional independence (Figure 8.19 of PRML). </p> </center> <p>The third example given in Figure 4 is opposite to the previous two cases. The node <em>c</em> in Figure 4 is <em>head-to-head</em> with respect to the path from <em>a</em> to <em>b</em> as it connects to the heads of the two arrows. In this case, node <em>c</em> would <em>block</em> the path while conditioning on node <em>c</em> would <em>unblock</em> the path and render \(a\) and \(b\) dependent. The proof is much similar to the previous cases.</p> <p>The joint distribution of the model in Figure 4 is</p> \[P(a,b,c)=P(a)P(b)P(c\vert a,b).\] <p>Marginalizing the distribution over \(c\), we have</p> \[\begin{aligned}P(a,b)&amp;=\sum_c P(a)P(b)P(c\vert a,b)\\&amp;=P(a)P(b)\sum_c P(c\vert a)\\&amp;=P(a)P(b)\end{aligned}\] <p>which means \(a\perp b\). In particular, the third equation is given by the fact ‘<em>conditional probabilities are probabilities</em>’ (<a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view">Introduction to Probability</a>).</p> <p>By rewriting the general joint distribution expression, we have</p> \[P(a,b,c)=P(a,b\vert c)P(c).\] <p>Then we have</p> \[\begin{alignat*}{3}&amp;&amp;P(a,b\vert c)P(c)&amp;=P(a)P(b)P(c\vert a,b)\\\implies&amp;&amp;P(a,b\vert c)&amp;=\frac{P(a)P(b)P(c\vert a,b)}{P(c)},\end{alignat*}\] <p>where the last term in general does not factorize into the product \(P(a\vert c)P(b\vert c)\), hence we have</p> \[a\not\!\perp\!\!\!\perp b\vert c.\] <h3 id="114-d-separation">1.1.4. D-separation</h3> <p>With the three examples, we now introduce <em>D-separation</em> which is used to determine the independence of those random variables in a graph. Specifically, given three arbitrary nonintersecting sets \(\mathcal{A}\), \(\mathcal{B}\) and \(\mathcal{C}\) of the nodes of the graph, by D-separation we can determine whether the statement \(\mathcal{A}\perp\!\!\!\perp\mathcal{B}\vert \mathcal{C}\) is true under the graph.</p> <p>The method proceeds as follows. (I am sure this great <a href="https://www.youtube.com/watch?v=yDs_q6jKHb0">video</a> can help you get into <em>D-separation</em> easily.)</p> <ul> <li>Find out all possible <em>undirected</em> paths between any node in \(\mathcal{A}\) and any node in \(\mathcal{B}\);</li> <li>Check whether those paths are blocked: for each path, <ul> <li>splitting it into continuous triples;</li> <li>for each triple, its structure (with directionality concerns) must belong to one of the three examples we mentioned before, and we just need to determine whether it is blocked when conditioning on \(\mathcal{C}\);</li> <li>if there is at least one triple blocked, the path is said to be blocked, otherwise, the path is unblocked;</li> </ul> </li> <li>If all the paths are blocked, the statement \(\mathcal{A}\perp\!\!\!\perp\mathcal{B}\vert \mathcal{C}\) is true and \(\mathcal{A}\) is said to be d-separated from \(\mathcal{B}\) by \(\mathcal{C}\).</li> </ul> <h2 id="12-markov-blanket">1.2. Markov Blanket</h2> <p>We now introduce the concept of a <em>Markov blanket</em> or <em>Markov boundary</em>. Consider a joint distribution \(P(x_1,x_2,\dots,x_N)\) represented by a directed graph having \(N\) nodes. In particular, we want to determine the conditional distribution</p> \[P(x_i\vert x_{\{j\ne i\}})=\frac{\prod_k P(x_k\vert \mathbb{Pa}_k)}{\int\prod_k P(x_k\vert \mathbb{Pa}_k)\text{d}x_i}.\] <p>Notice that if the term \(p(x_k\vert \mathbb{Pa}_k)\) does not involve \(x_i\), that is to say \(k\ne i\) and/or \(x_i\notin\mathbb{Pa}_k\), we then can remove the term from both numerator and denominator. The remaining terms in the conditional distribution then must be</p> \[P(x_k\vert \mathbb{Pa}_k), k\in\{i\}\cup\{k\vert x_i\in\mathbb{Pa}_k\},\] <p>and the conditional distribution \(P(x_i\vert x_{\{j\ne i\}})\) depends only on those terms. We now discuss which nodes those terms are related to. Obviously, when \(k\in\{i\}\), the term \(P(x_i\vert \mathbb{Pa}_i)\) would only involve \(x_i\) and <strong>the parents</strong> of it. When \(k\in\{j\vert x_i\in\mathbb{Pa}_j\}\) and \(k\ne i\), the term \(P(x_k\vert \mathbb{Pa}_k)\) are related to two parts. The first part \(x_k\) is <strong>the child</strong> of \(x_i\) as \(x_i\in\mathbb{Pa}_k\), while the second part \(\mathbb{Pa}_k\) is <strong>the co-parents</strong> of the child \(x_k\).</p> <div align="center"><img src="../../../../assets/images/Figure8.26.png" alt="Figure8.26 in PRML" width="250"/> </div> <center> <p style="font-size:80%;"> Figure 5. The Markov blanket of $$x_i$$ is denoted by colored nodes. (Figure 8.26 of PRML). </p> </center> <p>As shown in Figure 5, we say that a <em>Markov blanket</em> \(\mathcal{M}_i\) of a node \(x_i\) comprises the set of its <em>parents</em>, <em>child</em> and <em>co-parents</em>. Given the Markov blanket, the conditional distribution \(P(x_i\vert x_{\{j\ne i\}})\) can be rewritten as \(P(x_i\vert \mathcal{M}_i)\).</p> <h1 id="2-markov-network">2. Markov Network</h1> <p>The graphs we talked in the previous sections are directed. When it comes to undirected graphs, some concepts of directed graphs still play important roles while others do not. The graphical probabilistic models defined by <em>undirected graphs</em> is called <em>Markov networks</em>, also known as <em>Markov random fields</em>. Similar to Bayesian networks, the nodes in a Markov network represent random variables. However, as edges carry no arrows in undirected graphs, the function of edges changes a lot.</p> <h2 id="21-conditional-independence">2.1. Conditional Independence</h2> <p>The conditional independence of an undirected graph is given by the <em>absence</em> of edges. Specifically, for a graph with nodes representing random variable \(x_1,x_2,\dots, x_N\), we have:</p> <p><strong>Pairwise Markov Property</strong>: the <em>absence</em> of an edge between two nodes \(x_i\) and \(x_j\) means the corresponding random variables of the two nodes are conditionally independent given all the other random variables, which is</p> \[x_i\text{ and }x_j\text{ are not adjacent}\implies x_i\perp\!\!\!\perp x_j\vert X_{\setminus \{i,j\}},\] <p>where \(X_{\setminus\{i,j\}}\) denotes the set of all the variables with \(x_i\) and \(x_j\) removed.</p> <p><strong>Local Markov Property</strong>: a random variable \(x_i\) is conditionally independent of all other random variables given its neighbors, which is</p> \[x_i\perp\!\!\!\perp X_{\setminus \mathbb{Ne}_i}\vert \mathbb{Ne}_{i},\] <p>where \(\mathbb{Ne}_i\) is the set of neighbors of \(x_i\), <em>i.e.,</em> every node directly connected with \(x_i\) is in \(\mathbb{Ne}_i\). Recalling the definition of the <em>Markov blanket</em>, we can find that \(\mathbb{Ne}_i\) is the <em>Markov blanket</em> in the undirected graph.</p> <p>The property below is to Markov networks as <em>D-separation</em> is to Bayesian networks</p> <p><strong>Global Markov Property</strong>: for any three nonintersecting sets \(\mathcal{A}\), \(\mathcal{B}\) and \(\mathcal{C}\) of the nodes of the graph, we can determine whether</p> \[\mathcal{A}\perp\!\!\!\perp \mathcal{B}\vert\mathcal{C}\] <p>by the following steps:</p> <ul> <li>Find out all possible paths between any node in \(\mathcal{A}\) and any node in \(\mathcal{B}\);</li> <li>Check whether every path from \(\mathcal{A}\) to \(\mathcal{B}\) passes through at least one node in \(\mathcal{C}\);</li> <li>If so, the statement is true.</li> </ul> <p>Notice that compared with Bayesian networks, the way we check a statement of the conditional independence in Markov networks actually does not entail the concept ‘<em>block</em>’. Testing for conditional independence in undirected graphs is therefore simpler than in directed graphs.</p> <p>It can be shown that the three properties above are equivalent.</p> <h2 id="22-maximum-clique">2.2. Maximum Clique</h2> <p>As a Bayesian network can represent a joint distribution over finite random variables, there also exists a probability density function for each Markov network that is consistent with the three properties we mentioned above. Before moving on, we introduce a concept for a Markov network called a <em>clique</em>, which is defined as a subset of fully connected notes of the undirected graph. Obviously, there may be many different cliques for a Markov network. Among them, we particularly focus on the cliques each of which allows no other nodes to be added without it ceasing to be a clique, and we call such a clique a <em>maximal clique</em>.</p> <p>Denote the maximal cliques set of a Markov network with \(\{x_1,x_2,\dots,x_N\}\) by \(C_m=\{C\vert C\text{ is a maximal clique}\}\), and the nodes in maximal clique \(C\) by \(\text{x}_C=\{x_i\vert x_i\in C\}\). Then the joint distribution represented by the Markov network can be written as</p> \[P(x_1,x_2,\dots,x_N)=\frac{1}{Z}\prod_{C}^{C_m}\psi_C(\text{x}_C),\] <p>where \(\psi_C(\cdot)\) are positive functions called <em>potential functions</em>, and the quantity \(Z\) is called <em>partition function</em> that validates the distribution, <em>i.e.</em>,</p> \[Z=\sum_x\prod_C^{C_m}\psi_C(\text{x}_C).\] <p>Given a Markov network, the equivalence of the joint distribution and the conditional independence can be shown by <em>Hammesley-Clifford theorem</em>.</p> <p>We now consider the choice of potential functions. Given the existence of partition function, we have great flexibilities in choosing potential functions. However, it naturally raises the question of how to motivate a choice of potential function for a particular application. Since it requires the potential functions to be positive, a widely used function is <em>exponential function</em>:</p> \[\psi_C(\text{x}_C)=\exp\{-E(\text{x}_C)\},\] <p>where \(E(\text{x}_C)\) is called <em>energy function</em>. The joint distribution then is</p> \[\begin{aligned}P(x_1,x_2,\dots,x_N)&amp;=\frac{1}{Z}\prod_{C}^{C_m}\psi_C(\text{x}_C)\\&amp;=\frac{1}{Z}\exp\left\{-\sum_{C}^{C_m}E(\text{x}_C)\right\},\end{aligned}\] <p>which is known as <em>Boltzmann distribution</em> (or, <em>Gibbs distribution</em>). Moreover, we can see that the distribution is consistent with the definition of <a href="https://2ez4ai.github.io/2020/11/12/exponential_family-ml07/">exponential families</a>. The joint distribution of any Markov network in which every potential has the form of exponentials is in exponential families.</p> <h2 id="23-moralization">2.3. Moralization</h2> <p>We now consider the relation between the two graphical models. Particularly, we consider a problem of how to converting a Bayesian network to a Markov network. We start the discussion from the three examples of Bayesian networks mentioned in section 1.</p> <ul> <li> <p>Tail-to-tail: Given the tail-to-tail case shown in Figure 2, we have the joint distribution</p> \[P(a,b,c)=P(c)P(a\vert c)P(b\vert c).\] <p>A factorization can be easily obtained by identifying</p> \[\begin{aligned}P(a,b,c)&amp;=\psi(a,c)\psi(b,c),\\\psi(a,c)&amp;=P(c)P(a\vert c),\\\psi(b,c)&amp;=P(b\vert c),\end{aligned}\] <p>which is actually the joint distribution represented by the Markov network whose maximum cliques are \(\{a,c\}\) and \(\{b,c\}\) and potential functions are \(\psi(a,c)\) and \(\psi(b,c)\). Obviously, such a Markov network is the same as the Bayesian network in Figure 2 with removing its arrows.</p> </li> <li> <p>Head-to-tail: For the graph in Figure 3, we have</p> \[P(a,b,c)=P(a)P(c\vert a)P(b\vert c).\] <p>Similarly, by identifying</p> \[\begin{aligned}\psi(a,c)&amp;=P(a)P(c\vert a),\\\psi(b,c)&amp;=P(b\vert c).\end{aligned}\] <p>We have the corresponding Markov network with maximum cliques \(\{a,c\}\) and \(\{b,c\}\), which also can be obtained by removing the arrows of the graph.</p> </li> <li> <p>Head-to-head: The case shown in Figure 4 is a little tricky. Given the joint distribution</p> \[P(a,b,c)=P(a)P(b)P(c\vert a,b),\] <p>we can find that the term \(P(c\vert a,b)\) leads to a factor that depends on three nodes. Therefore the corresponding Markov network must have a maximum clique consists of \(\{a,b,c\}\). To this end, we need to not only remove the arrows of the graph but also add an edge between \(a\) and \(b\). Then we have the corresponding Markov network with potential function</p> \[\psi(a,b,c)=P(a)P(b)P(c\vert a,b).\] </li> </ul> <p>Given the above discussion, to convert a directed graph into an undirected graph, the general steps are</p> <ul> <li>Remove all the arrows in the directed graph;</li> <li>Add additional edges between all pairs of parents for each node;</li> <li>Initialize all the potential functions to 1;</li> <li>Multiply each conditional distribution factor into the potential function whose corresponding clique contains all the variables of the factor.</li> </ul> <p>The step <em>adding additional edges</em> is known as <em>moralization</em>. And the resulting undirected graph after <em>removing arrows</em> is called <em>moral graph</em>.</p> <h1 id="3-factor-graph">3. Factor Graph</h1> <p>Notice that in moralization, we may invite loops in the moral graph, which can be tricky in some cases. To avoid the issues incurred by the loops, we can leverage <em>factor graphs</em>. Given a joint distribution of a moral graph, we can construct a factor graph by</p> <ul> <li>Remove all the edges in the graph;</li> <li>Rewrite the joint distribution as a multiplication of multiple functions where the functions can depend on an arbitrary set of the nodes;</li> <li>Add new nodes for each function of the new expression;</li> <li>Add edges between each function and the nodes it depends on.</li> </ul> <figure> <div style="display:flex"> <figure> <img src="../../../../assets/images/Figure8.42a.png" width="250" alt="Figure8.42 (a) in PRML"/> <figcaption><center>(a)</center></figcaption> </figure> <figure> <img src="../../../../assets/images/Figure8.41a.png" width="250" alt="Figure8.41 (a) in PRML"/> <figcaption><center>(b)</center></figcaption> </figure> <figure> <img src="../../../../assets/images/Figure8.42c.png" width="250" alt="Figure8.42 (c) in PRML"/> <figcaption><center>(c)</center></figcaption> </figure> </div> </figure> <center> <p style="font-size:80%;"> Figure 6. (a) A directed graph. (b) The corresponding moral graph of the directed graph. (c) A factor graph of the graph where the factors are depicted by small solid squares. (Figure 8.41 and Figure 8.42 of PRML). </p> </center> <p>An example is shown in Figure 6. The directed graph represents the joint distribution</p> \[P(x_1,x_2,x_3)=P(x_1)P(x_2)P(x_3\vert x_1,x_2).\] <p>The moralization of the directed graph incurs a loop among \(x_1,x_2,x_3\) as shown in Figure 6 (b). Defining \(f_a(x_1)=P(x_1)\), \(f_b(x_2)=P(x_2)\) and \(f_c(x_1,x_2,x_3)=P(x_3\vert x_1,x_2)\), we have</p> \[P(x_1,x_2,x_3)=f_a(x_1)f_b(x_2)f_c(x_1,x_2,x_3).\] <p>The factor graph corresponding to such a factorization is shown in Figure 6 (c). Moreover, all factor graphs are <em>bipartite</em> as they consist of two distinct kinds of nodes. With factor graphs, we can conduct the related computation based on the factor nodes rather than variable nodes so that the loop can be avoided.</p> <h1 id="4-conclusion">4. Conclusion</h1> <p>In this post, we first introduced two kinds of probabilistic graphical models. One is <em>Bayesian networks</em> that is based on directed acyclic graph. The other is <em>Markov network</em> that is based on undirected graph. Both two models can be used to represent the joint distribution and reflect the conditional independences over a set of random variables. Then we discussed how to convert a Bayesian network into a Markov network. The loops in the <em>moral graph</em> incurred by the conversion can be avoided by transforming the graph into a <em>factor graph</em>.</p>]]></content><author><name></name></author><category term="Machine-Learning"/><summary type="html"><![CDATA[A long story. Had to deal with many other things during the writing. Hope the following posts can be finished regularly.]]></summary></entry><entry><title type="html">Machine Learning - 07 Exponential Family</title><link href="https://theallen1996.github.io/blog/2020/exponential_family-ml07/" rel="alternate" type="text/html" title="Machine Learning - 07 Exponential Family"/><published>2020-11-12T00:00:00+00:00</published><updated>2020-11-12T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2020/exponential_family-ml07</id><content type="html" xml:base="https://theallen1996.github.io/blog/2020/exponential_family-ml07/"><![CDATA[<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a> and the <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf">material</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-exponential-family" id="markdown-toc-1-exponential-family">1. Exponential Family</a></li> <li><a href="#2-sufficient-statistic" id="markdown-toc-2-sufficient-statistic">2. Sufficient Statistic</a></li> <li><a href="#3-log-partition-function" id="markdown-toc-3-log-partition-function">3. Log-partition Function</a></li> <li><a href="#4-maximum-entropy" id="markdown-toc-4-maximum-entropy">4. Maximum Entropy</a></li> <li><a href="#5-gaussian-distribution" id="markdown-toc-5-gaussian-distribution">5. Gaussian Distribution</a></li> <li><a href="#6-conclusion" id="markdown-toc-6-conclusion">6. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>An exponential family is a family of distributions which share some properties in common. The real message of this note is the simplicity and elegance of the exponential family. Once the ideas are mastered, it is often easier to work within the general exponential family framework than with specific instances.</p> <h1 id="1-exponential-family">1. Exponential Family</h1> <p>Given one real-vector parameter \(\mathbf{\theta}=[\theta_1,\theta_2,\dots,\theta_d]^T\), we define an <em>exponential family</em> of probability distributions as those distributions whose density have the following general form:</p> \[f_X(x\vert\eta)=h(x)\exp\left(\eta^T T(x)-A(\eta)\right).\] <p><strong>Canonical parameter</strong>: \(\eta\) is called <em>canonical</em>, or <em>natural parameter</em> (function), which can be viewed as a transformation of \(\mathcal{\theta}\). The set of values of \(\eta\) is always convex.</p> <p><strong>Sufficient statistic</strong>: Given a data set sampled from \(f_X(x\vert\eta)\), the sufficient statistic \(T(x)\) is a function of the data that holds all information the data set provides with regard to the unknown parameter \(\mathbf{\theta}\).</p> <p><strong>Log-partition function</strong>: \(A(\eta)\) is the <em>log-partition function</em> to normalize \(f_X(x\vert \eta)\) to be a probability distribution,</p> \[A(\eta)=\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right).\] <p>In the following sections, we will discuss them detailedly.</p> <h1 id="2-sufficient-statistic">2. Sufficient Statistic</h1> <p>Consider the problem of estimating the unknown parameters by <em>maximum likelihood estimation</em> (MLE) in exponential family cases. Specifically, for an <em>i.i.d.</em> data set \(\mathcal{D}=\{x_1,x_2,\dots,x_N\}\), we have the log likelihood</p> \[\mathcal{L}(\eta\vert\mathcal{D})=\log\left(\prod_{i=1}^Nh(x_i)\right)+\eta^T\left(\sum_{i=1}^NT(x_i)\right)-NA(\eta).\] <p>By <em>MLE</em>, we have the estimation \(\hat\eta\) when its gradient with respect to \(\eta\) is zero:</p> \[\mathcal{L}’(\eta\vert\mathcal{D})=\sum_{i=1}^NT(x_i)-NA’(\eta)=0.\] <p>Solving the equation, we have</p> \[A’(\hat\eta)=\frac{1}{N}\sum_{i=1}^NT(x_i),\] <p>which is the general formula of MLE for the parameters in the exponential family. Further, notice that our formula involves the data only via the sufficient statistic \(T(x_i)\). This gives the operational meaning to <em>sufficiency</em>—for the purpose of estimating parameters we retain only the sufficient statistic.</p> <h1 id="3-log-partition-function">3. Log-partition Function</h1> <p>As we mentioned in section 1, \(A(\eta)\) can be viewed as a normalization factor. In fact, \(A(\eta)\) is not a degree of freedom in the specification of an exponential family density; it is determined once \(T(x)\) and \(h(x)\) are determined. The relation between \(A(\eta)\) and \(T(x)\) can be further characterized by</p> \[\begin{aligned}A’(\eta)&amp;=\frac{\text{d}\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right)}{\text{d}\eta}\\&amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot T(x)\text{d}x}{\int_{X}h(x)\exp(\eta^T T(x))\text{d}x}\\&amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot T(x)\text{d}x}{\exp({A(\mathbf{\theta})})}\\&amp;=\int_{X}\underbrace{h(x)\exp(\eta^T T(x)-A(\eta))}_{f_X(x\vert \eta)}\cdot T(x)\text{d}x\\&amp;=\mathbb{E}_{f_X(x\vert\eta)}[T(x)].\end{aligned}\] <p>Further, we have</p> \[\begin{aligned}A’’(\eta)&amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x)-A’(\eta)) T(x)\text{d}x\\&amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x))^2\text{d}x-A’(\eta)\int_{X}f_X(x\vert \mathbf{\eta})\cdot T(x)\text{d}x\\&amp;=\mathbb{E}_{f_X(x\vert\eta)}[(T(x))^2]-\left(\mathbb{E}_{f_X(x\vert\eta)}[T(x)]\right)^2\\&amp;=var[T(x)],\end{aligned}\] <p>which also shows that \(A(\eta)\) is convex as \(var[T(x)]\ge 0\).</p> <h1 id="4-maximum-entropy">4. Maximum Entropy</h1> <p>The entropy of \(P\) with distribution \(p(x)\) supported on \(X\) is</p> \[H(P)=\mathbb{E}_{P}[-\log p(x)].\] <p>The <em>maximum entropy</em> principle is that: given some constraints (prior information) about the distribution \(P\), we consider all probability distributions satisfying said constraints such that the constraints are being utilized as <em>objective</em> as possible, <em>i.e.,</em> be as uncertain as possible.</p> <p>For example, consider the case where the very constraint is \(\sum_Xp(x)=1\), which formulates</p> \[\begin{aligned}\max&amp;\quad H(P)\\\text{s.t.}&amp;\quad \sum_{X}p(x)=1.\end{aligned}\] <p>By the definition we have</p> \[H(P)=-\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i).\] <p>Then the <em>Lagrangian</em> for the optimization problem is</p> \[L(P,\lambda)=\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i)+\lambda\left(1-\sum_{i=1}^{\vert X\vert}p(x_i)\right).\] <p>Setting the first derivation of the Lagrangian to be zero yields</p> \[\frac{\partial L}{\partial p(x_i)}=0\implies \hat{p}(x_i)=\exp(\lambda-1),\] <p>which gives that</p> \[\hat{p}(x_1)=\hat{p}(x_2)=\dots=\hat{p}(x_{\vert X\vert})=\frac{1}{\vert X\vert},\] <p><em>i.e.,</em> the distribution with maximum entropy is <em>uniform distribution</em>.</p> <p>We now consider a general case where \(p(x)\) is continuous with a general constraint \(\mathbb{E}_P[\Phi(x)]=\alpha\), where \(\Phi(x)=[\phi_1(x),\phi_2(x),\dots,\phi_d(x)]\in\mathbb{R}^d\) and \(\alpha=[\alpha_1,\alpha_2,\dots,\alpha_d]\in\mathbb{R}^d\), which formulates</p> \[\begin{aligned}\max&amp;\quad H(P)\\\text{s.t.}&amp;\quad \mathbb{E}_P[\Phi(x)]=\alpha\\\implies\min&amp;\quad \int_Xp(x)\log p(x)\text{d}x\\\text{s.t.}&amp;\quad \int_X p(x)\phi_i(x)\text{d}x=\alpha_i,\ i=1,2,\dots, d,\\&amp;\quad \int_X p(x)\text{d}x=1.\end{aligned}\] <p>Similarly, we obtain the Lagrangian as</p> \[L(P,\theta,\lambda)=\int_X p(x)\log p(x)\text{d}x+\sum_{i=1}^d\theta_i\left(\alpha_i-\int_X p(x)\phi_i(x)\text{d}x\right)+\lambda\left(\int_X p(x)\text{d}x-1\right).\] <p>By treating the density \(P=[p(x)]_{x\in X}\) as a finite vector such that \(\int_X p(x)\text{d}x\) is similar to \(\sum_X p(x)\), we have</p> \[\begin{aligned}\frac{\partial L}{\partial p(x)}&amp;=\frac{\partial }{\partial p(x)}\left(\sum_X p(x)\log p(x)-\sum_{i=1}^d\theta_i\sum_X p(x)\phi_i(x)+\lambda\sum_X p(x)\right)\\&amp;=1+\log p(x)-\sum_{i=1}^d\theta_i\phi_i(x)+\lambda\\&amp;=1+\log p(x)-\theta^T\Phi(x)+\lambda.\end{aligned}\] <p>Setting the derivation to be zero for all \(x\), we have</p> \[p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},\] <p>which is in the exponential family form with</p> \[\begin{aligned}\eta&amp;=\theta,\\T(x)&amp;=\Phi(x),\\A(\eta)&amp;=\lambda+1,\\h(x)&amp;=1.\end{aligned}\] <h1 id="5-gaussian-distribution">5. Gaussian Distribution</h1> <p>In this section, we consider an example, Gaussian distribution, which is of the exponential family and exemplifies the properties we mentioned above.</p> <p>We first rewritten the PDF of one-dimension Gaussian distribution to show it is in the exponential family .</p> <p><em>Proof:</em> Given unknown parameter \(\mathbf{\theta}=[\mu,\sigma^2]\), the Gaussian density can be written as follows,</p> \[\begin{aligned}f_X(x\vert \mathbf{\theta})&amp;=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}\\&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right\}\\&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;-\frac{1}{2}\sigma^2\end{bmatrix}\begin{bmatrix}x\\x^2\end{bmatrix}-\left(\frac{\mu^2}{2\sigma^2}+\log\sigma\right)\right\},\end{aligned}\] <p>which is in the exponential family form with</p> \[\begin{aligned}\eta&amp;=\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;-\frac{1}{2\sigma^2}\end{bmatrix}^T,\\T(x)&amp;=\begin{bmatrix}x&amp;x^2\end{bmatrix}^T,\\A(\eta)&amp;=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2),\\h(x)&amp;=\frac{1}{\sqrt{2\pi}}.\end{aligned}\] <p>\(\tag*{\)\blacksquare\(}\)</p> <p>Then we verify the relation between the sufficient statistic and MLE method.</p> <p><em>Proof:</em> Given a data set \(\mathcal{D}=\{x_1,x_2,\dots,x_N\}\), as we mentioned in section 2, we can derive the parameters via the sufficient statistic as follows,</p> \[\begin{cases}A’(\eta)=\frac{1}{N}\sum_{i=1}^NT(x)\implies\begin{cases}A’(\hat\eta_1)=\frac{1}{N}\sum_{i=1}^N x_i\\A’(\hat\eta_2)=\frac{1}{N}\sum_{i=1}^Nx_i^2\end{cases}\\A(\eta)=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2)\implies\begin{cases}A’(\hat\eta_1)=-\frac{\eta_1}{2\eta_2}=\hat\mu\\A’(\hat\eta_2)=\frac{\eta_1^2}{4\eta_2^2}-\frac{1}{2\eta_2}=\hat\sigma^2+\hat\mu^2\end{cases}\end{cases}\] <p>Solving the equations we have</p> \[\hat\mu=\frac{1}{N}\sum_{i=1}^Nx_i,\quad \hat\sigma=\frac{1}{N}\sum_{i=1}^Nx_i^2-\hat\mu^2,\] <p>which is consistent with the result in the <a href="https://2ez4ai.github.io/2020/09/28/intro-ml01/">post</a>. \(\tag*{\)\blacksquare\(}\)</p> <p>Now we show that</p> \[A’’(\hat\eta)=var[T(x)].\] <p><em>Proof</em>: Firstly, we have</p> \[\begin{cases}A’’(\hat\eta_1)=-\frac{1}{2\eta_2}=\sigma^2\\A’’(\hat\eta_2)=-\frac{\eta_1^2}{2\eta_2^3}+\frac{1}{2\eta_2^2}=4\sigma^2\mu^2+2\sigma^4\end{cases}\] <p>For \(T(x)=\begin{bmatrix}x&amp;x^2\end{bmatrix}^T\), we have</p> \[var[x]=\sigma^2, \text{ as }x\sim\mathcal{N}(\mu,\sigma^2),\] <p>and</p> \[var[x^2]=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2.\] <p>For \(\mathbb{E}[x^2]\), it follows that</p> \[\mathbb{E}[x^2]=var[x]+(\mathbb{E}[x])^2=\sigma^2+\mu^2.\] <p>For \(\mathbb{E}[x^4]\), to compute it we leverage <em>moment generating functions</em> which follows that</p> \[M_X(t)=e^{\mu t+\frac{1}{2}\sigma^2t^2},\quad \mathbb{E}[x^4]=M^{(4)}_X(0).\] <p>After a laborious computing, we have</p> \[\begin{aligned}var[x^2]&amp;=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2\\&amp;=3\sigma^4+6\sigma^2\mu^2+\mu^4-\sigma^4-2\sigma^2-\mu^4\\&amp;=4\sigma^2\mu^2+2\sigma^4.\end{aligned}\] <p>Therefore, we have</p> <p>\(A’’(\hat\eta)=\begin{bmatrix}var[x]\\var[x^2]\end{bmatrix}.\tag*{\)\blacksquare\(}\)</p> <p>Finally, we show that \(X\sim\mathcal{N}(\mu,\sigma^2)\) is the distribution that maximizes the entropy over all distributions \(P\) satisfying</p> \[\mathbb{E}_P\left[\left(\frac{X-\mu}{\sigma}\right)^2\right]=1.\] <p><em>Proof:</em> Consider the expression we formulated in section 4,</p> \[p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},\] <p>which maximizes the entropy while satisfying \(\mathbb{E}_P[\Phi(x)]=\alpha\). Now letting</p> \[\begin{aligned}\alpha&amp;=1,\\\Phi(x)&amp;=\frac{(x-\mu)^2}{\sigma^2},\\\theta&amp;=-\frac{1}{2},\\\exp\{-\lambda-1\}&amp;=\frac{1}{\sqrt{2\pi}\sigma}.\end{aligned}\] <p>Therefore we have</p> <p>\(p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}.\tag*{\)\blacksquare\(}\)</p> <h1 id="6-conclusion">6. Conclusion</h1> <p>In this post, we briefly introduced the basic form of the exponential family. Then we discussed its properties from three perspectives: sufficient statistic, log-partition function and maximum entropy. Moreover, with one-dimension Gaussian distribution, we exemplified the properties.</p>]]></content><author><name></name></author><category term="Machine-Learning"/><summary type="html"><![CDATA[Introduce concepts of exponential family.]]></summary></entry><entry><title type="html">Machine Learning - 06 Kernel Method</title><link href="https://theallen1996.github.io/blog/2020/kernel_method-ml06/" rel="alternate" type="text/html" title="Machine Learning - 06 Kernel Method"/><published>2020-11-05T00:00:00+00:00</published><updated>2020-11-05T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2020/kernel_method-ml06</id><content type="html" xml:base="https://theallen1996.github.io/blog/2020/kernel_method-ml06/"><![CDATA[<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-kernel-method" id="markdown-toc-1-kernel-method">1. Kernel method</a></li> <li><a href="#2-kernel-function" id="markdown-toc-2-kernel-function">2. Kernel function</a></li> <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>Kernel methods are a class of algorithms for pattern analysis. The name of kernel methods comes from the use of <em>kernel function</em>, which enable operations in a high-dimensional and implicit space. Specifically, by <a href="https://en.wikipedia.org/wiki/Cover%27s_theorem">Cover’s theorem</a>, given a set of training data that is not <em>linearly separable</em>, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some <em>non-linear transformation</em>. With the help of kernel function, the operation, <em>i.e.,</em> inner product, it involves after transforming can be often computationally cheaper than the explicit computation. Such an approach is called the <em>kernel trick</em>. In this post, we will focus on the application of kernel method to SVM.</p> <h1 id="1-kernel-method">1. Kernel method</h1> <p>Define the data set as \(\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}, X=\{x_1,x_2,\dots,x_N\}\) and \(Y=\{y_1,y_2,\dots,y_N\}\) where \(x_i\in\mathbb{R}^{d\times 1}\) and \(y_i\in\{-1,1\}\). We further assume that the data set is non-linearly separable. Kernel method supposes that there is a non-linear transformation \(\phi(x):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1},d&lt;p,\) such that \(\mathcal{D}_p=\{(\phi(x_1),y_1),(\phi(x_2),y_2),\dots,(\phi(x_N),y_N)\}\) are linearly separable. For such a linearly separable data set, recalling the problem we formulated in section 1.3 of <a href="https://2ez4ai.github.io/2020/10/28/support_vector_machine-ml05/">SVM</a>, we have the duality problem</p> \[\begin{aligned}\min_\lambda&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_j\phi^T(x_i)\phi(x_j)\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;\quad \sum_{i=1}^N\lambda_iy_i=0.\end{aligned}\] <p>However, after transforming, the inner product \(\phi(x_i)^T\phi(x_j)=\langle\phi(x_i),\phi(x_j)\rangle\) could be hard to obtain (consider the case that \(\phi(\cdot)\) has infinite dimensions), which requires the aid of <em>kernel function</em>.</p> <h1 id="2-kernel-function">2. Kernel function</h1> <p>A kernel function is defined as \(K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}\). Specifically, for non-linear transformation \(\phi(\cdot)\in\mathcal{H}\text{ (Hilbert space) }:\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}\) and any \(x_i,x_j\in\mathbb{R}^{d\times 1}\), we call \(K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle\) a kernel function. Such a kernel function is regarded <em>positive definite</em>, which satisfies</p> \[K(x_i,x_j)=K(x_j,x_i),\] <p>and for \(x_{1},x_{2},\dots,x_{N}\in\mathbb{R}^{d\times 1},\)</p> \[\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\text{ is a positive semi-definite (PSD) matrix},\] <p>where \(\mathcal{K}\) is called <em>Gram matrix</em> of \(K\) over set \(\{x_{1},x_{2},\dots,x_{N}\}\). When the explicit expression of \(\phi(\cdot)\) is hard to be determined, it quite often to show the positive definiteness of a kernel function via its corresponding Gram matrix.</p> <p><strong>(Properties)</strong> We now show two <em>properties</em> of kernel functions.</p> <ul> <li>Let \(K\) be kernel function such that \(K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}\), then we define its Gram matrix \(\mathcal{K}\in\mathbb{R}^{N\times N}\) over \(\{x_1,x_2,\dots,x_N\}\) where \(x_i\in\mathbb{R}^{d\times 1}\). Considering the mapping function \(\phi(\cdot):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}\), we have</li> </ul> \[K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle, \phi(\cdot)\in\mathcal{H}\implies \begin{cases}K(x_i,x_j)=K(x_j,x_i)\\ \mathcal{K}\text{ is a PSD matrix}\end{cases}.\] <p><em>Proof</em>:</p> <p>By the definition of \(K(x_i,x_j)\), we have</p> \[K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle,\quad K(x_j,x_i)=\langle \phi(x_j),\phi(x_i)\rangle.\] <p>By the symmetry of inner product, we have \(\langle \phi(x_i),\phi(x_j)\rangle=\langle \phi(x_j),\phi(x_i)\rangle\). It then follows that</p> \[K(x_i,x_j)=K(x_j,x_i).\] <p>Therefore the Gramian matrix \(\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\) is symmetric real matrix. Now we show that \(\forall\alpha\in\mathbb{R}^{R\times 1}, \alpha^T\mathcal{K}\alpha\ge 0.\) The notation is given by</p> \[\begin{aligned}\alpha^T\mathcal{K}\alpha=(\alpha_1,\alpha_2,\dots,\alpha_N)\begin{bmatrix}K_{11}&amp;K_{12}&amp;\dots&amp;K_{1N}\\K_{21}&amp;K_{22}&amp;\dots&amp;K_{2N}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\K_{N1}&amp;K_{N2}&amp;\dots&amp;K_{NN}\end{bmatrix}\begin{pmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_N\end{pmatrix}\end{aligned},\] <p>where \(K_{ij}=K(x_{ri},x_{rj})\). We then have</p> \[\begin{aligned}\alpha^T\mathcal{K}\alpha&amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_jK_{ij}\\&amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_j\phi^T(x_{ri})\phi(x_{rj})\\&amp;=\sum_{i=1}^R \alpha_i\phi^T(x_{ri})\sum_{j=1}^R\alpha_j\phi(x_{rj})\\&amp;=\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right)^T\left(\sum_{j=1}^R\alpha_j\phi(x_{rj})\right)\\&amp;=\left\langle\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right), \left(\sum_{j=1}^R \alpha_i\phi(x_{rj})\right)\right\rangle\\&amp;=\left\vert\left\vert\sum_{i=1}^R \alpha_i\phi(x_{ri})\right\vert\right\vert^2,\end{aligned}\] <p>therefore, \(\alpha^T\mathcal{K}\alpha\ge 0\) and \(\mathcal{K}\) is a PSD matrix.\(\tag*{\)\blacksquare\(}\)</p> <ul> <li>Let \(\mathcal{K}\in\mathbb{R}^{d\times d}\) be a symmetric PSD matrix, then for \(\{x_1,x_2,\dots,x_N\}\) where \(x_i\in\mathbb{R}^{d\times 1}\), we have kernel function \(K(x_i,x_j)=x_i^T\mathcal{K}x_j\).</li> </ul> <p><em>Proof</em>:</p> <p>Consider the <em>diagonalisation</em> of \(\mathcal{K}=Q^T\Lambda Q\) by an orthogonal matrix \(Q\), where \(\Lambda\) is a diagnoal matrix containing the non-negative eigenvalues of \(\mathcal{K}\). Let \(\sqrt{\Lambda}\) be the diagonal matrix with the square roots of the eigenvalues and set \(A=\sqrt{\Lambda}Q\). Then for \(\{x_1,x_2,\dots,x_N\}\) where \(x_i\in\mathbb{R}^{d\times 1}\), we have</p> \[x_i^T\mathcal{K}x_j=x_i^TQ^T\Lambda Qx_j=x_i^TA^TA x_j=\langle A x_i,Ax_j\rangle.\] <p>Therefore we have kernel function \(K(x_i,x_j)=x_i^T\mathcal{K}x_j=\langle Ax_i,Ax_j\rangle\) with linear transformation \(\phi(\cdot)=A\cdot. \tag*{\)\blacksquare\(}\)</p> <h1 id="3-conclusion">3. Conclusion</h1> <p>In this post, we introduced <em>kernel method</em> for classification problem. Given <em>Cover’s theorem</em>, we can project non-linear data into high-dimensional space and obtain linearly separable data. To simplify the computation incurred by the duality problem, we can leverage <em>kernel function</em> to avoid the computing labor.</p> <p>This is definitely not a good introduction to kernel methods. For more details of kernel method, I would recommend <a href="https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf">Kernel methods: an overview</a>.</p>]]></content><author><name></name></author><category term="Machine-Learning"/><summary type="html"><![CDATA[Introduce the basic concept of kernel methods and two properties of it with proof details.]]></summary></entry><entry><title type="html">Machine Learning - 05 Support Vector Machine</title><link href="https://theallen1996.github.io/blog/2020/support_vector_machine-ml05/" rel="alternate" type="text/html" title="Machine Learning - 05 Support Vector Machine"/><published>2020-10-28T00:00:00+00:00</published><updated>2020-10-28T00:00:00+00:00</updated><id>https://theallen1996.github.io/blog/2020/support_vector_machine-ml05</id><content type="html" xml:base="https://theallen1996.github.io/blog/2020/support_vector_machine-ml05/"><![CDATA[<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-hard-margin-svm" id="markdown-toc-1-hard-margin-svm">1. Hard-margin SVM</a> <ul> <li><a href="#11-problem-formulation" id="markdown-toc-11-problem-formulation">1.1. Problem Formulation</a></li> <li><a href="#12-lagrange-duality" id="markdown-toc-12-lagrange-duality">1.2. Lagrange Duality</a></li> <li><a href="#13-karushkuhntucker-conditions" id="markdown-toc-13-karushkuhntucker-conditions">1.3. Karush–Kuhn–Tucker Conditions</a></li> </ul> </li> <li><a href="#2-soft-margin-svm" id="markdown-toc-2-soft-margin-svm">2. Soft-margin SVM</a></li> <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>Support vector machine (SVM) is a supervised learning method for classification and regression analysis. It is one of the most robust prediction method. Here we mainly consider its applications in classification. Specifically, for the data of \(d\)-dimensional, we want to know whether we can separate classes with a \((d-1)\)-dimensional <em>hyperplane</em>. In particular, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class. According to whether the dataset is linearly separable or not, there are <em>hard-margin</em> SVM, <em>soft-margin</em> SVM and <em>kernel</em> SVM.</p> <h1 id="1-hard-margin-svm">1. Hard-margin SVM</h1> <p>Hard-margin SVM works only when data is completely linearly separable without any errors.</p> <div align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png" width="350"/> </div> <h2 id="11-problem-formulation">1.1. Problem Formulation</h2> <p>Suppose we have data set \(\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\) where \(x_i\in\mathbb{R}^{d\times 1}\) is the data feature and \(y_i\in\{-1,1\}\) is the corresponding class label. A case where \(d=2\) is shown in the figure above. The hyperplane that separates the data is defined as</p> \[w^Tx-b=0,\] <p>where \(w\in\mathbb{R}^{d\times 1}\) and \(b\in\mathbb{R}\) are parameters to be learned. Then like what we arrived in <a href="https://2ez4ai.github.io/2020/10/14/linear_classification-ml03/">perceptron</a>, a correct classifier should ensure that</p> \[y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N.\] <p>We further define the <em>margin</em> as the parallel lines that has the minimum distance from the data to the hyperplane. In <em>hard-margin</em> SVM, we need to find the <em>maximum-margin</em> hyperplane that maximizes the distance, which can be described by</p> \[\begin{aligned}\max_{w,b}\min_{i}&amp;\quad \frac{\vert w^Tx_i-b\vert}{\vert\vert w\vert\vert}\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N\end{aligned}.\] <p>The problem further is equivalent to</p> \[\begin{alignat*}{3}\max_{w,b}\min_{i}&amp;\quad \frac{y_i(w^Tx_i-b)}{\vert\vert w\vert\vert}\implies\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i&amp;\quad y_i(w^Tx_i-b)\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N\end{alignat*}.\] <p>For the constraint \(y_i(w^Tx_i-b)&gt;0, \forall i=1,2,\dots,N\), there exists a positive parameter \(r&gt;0\) such that</p> \[\min_i y_i(w^Tx_i-b)=r.\] <p>As there are many \(w,b\) available for the separation as long as they have the same directions. We add a new constraint that \(r=1\). Then it follows that</p> \[\min_i y_i(w^Tx_i-b)=y_i((w_{\text{old}}^T/r)x_i-b_{\text{old}}/r)=1.\] <p>The problem then is transformed into</p> \[\begin{alignat*}{3}&amp;&amp;\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i y_i(w^Tx_i-b)&amp;\implies\min_{w,b}\quad \frac{1}{2}w^Tw\\&amp;&amp;\text{s.t.}\quad y_i(w^Tx_i-b)&gt;0&amp;\implies\text{s.t.}\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{alignat*},\] <p>which is a linearly constrained <em>quadratic optimization</em> (QP) problem.</p> <h2 id="12-lagrange-duality">1.2. Lagrange Duality</h2> <p>The following content is about <a href="https://web.stanford.edu/~boyd/cvxbook/">convex optimization</a>. In section 1, we have the <em>primal problem</em></p> \[\begin{aligned}\min_{w,b}&amp;\quad \frac{1}{2}w^Tw\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{aligned}.\] <p>The <em>Lagrangian</em> for the problem is a function defined as</p> \[L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^N\lambda_i\left(1-y_i\left(w^Tx_i-b\right)\right),\] <p>where \(\lambda_i\ge 0\) is the <em>Lagrange multiplier</em> associated with the constraints. Consider the problem of \(\max_\lambda L(w,b,\lambda)\),</p> \[\max_{\lambda\ge 0} L(w,b,\lambda)=\begin{cases}\frac{1}{2}w^Tw+\infty&amp;\text{if }\exists i\in\{1,2,\dots,N\}\text{ s.t. }1-y_i(w^Tx_i-b)&gt;0,\\\frac{1}{2}w^Tw+0&amp;\text{otherwise.}\end{cases}\] <p>The problem makes sense (non infinity) only when the original constraint is satisfied. In that case, the primal problem is equivalent to</p> \[\begin{aligned}\min_{w,b}\max_\lambda&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}.\] <p>Then we define the <em>Lagrange dual function</em> for the primal problem as</p> \[g(\lambda)=\min_{w,b}L(w,b,\lambda).\] <blockquote> <p>Actually, the correct definition of the <em>Lagrange dual function</em> should be</p> \[g(\lambda)=\inf_{w,b}L(w,b,\lambda).\] <p>Here we assume the minimum exists and the infimum is the minimum for understanding.</p> </blockquote> <p>The <em>Lagrange dual problem</em> of the original problem is then defined as</p> \[\begin{aligned}\max_{\lambda}&amp;\quad g(\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}.\] <p>The dual problem is introduced for its convexity. Specifically, notice that the infimum (minimum in this case) of \(g(\lambda)\) is unconstrained as opposed to the original constrained minimization problem. Further, \(g(\lambda)\) is concave with respect to \(\lambda\) regardless of the original problem.</p> <table> <thead> <tr> <th style="text-align: center">Primal Problem</th> <th style="text-align: center">Lagrange Dual Problem</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(\begin{aligned}\min_{w,b}\max_\lambda&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}\)</td> <td style="text-align: center">\(\begin{aligned}\max_{\lambda}\min_{w,b}&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}\)</td> </tr> </tbody> </table> <p>A natural problem is whether the two problems are equivalent. Obviously, the equivalence is obtained if and only if</p> \[\min_{w,b}\max_{\lambda} L(w,b,\lambda)=\max_{\lambda}\min_{w,b} L(w,b,\lambda).\] <p>If the equation holds, we say the <em>strong duality</em> holds. It can also be shown that the <em>weak duality</em> always holds as</p> \[\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).\] <p><em>Proof:</em></p> <p>Obviously, we have</p> \[\max_\lambda L(w,b,\lambda)\ge L(w,b,\lambda)\ge\min_{w,b}L(w,b,\lambda).\] <p>Define \(F(w,b)=\max_\lambda L(w,b,\lambda)\) and \(G(\lambda)=\min_{w,b}L(w,b,\lambda)\). According the above inequality, it follows that</p> \[F(w,b)\ge G(\lambda)\implies\min_{w,b}F(w,b)\ge\max_\lambda G(\lambda).\] <p>Therefore we have</p> <p>\(\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).\tag*{\)\blacksquare\(}\)</p> <p>Solving the dual problem in fact is used to find nontrivial lower bounds for difficult original problems. In our case, the strong duality holds for the linearly constrained QP problem. Thus to solve the primal problem is to solve the dual problem.</p> <h2 id="13-karushkuhntucker-conditions">1.3. Karush–Kuhn–Tucker Conditions</h2> <p>For the primal problem and its dual problem, if the strong duality holds, then <em>Karush–Kuhn–Tucker (KKT) conditions</em> are satisfied as</p> <ul> <li> <p>(a). Primal Feasibility:</p> \[y_i(w^Tx_i-b)\ge1,i=1,2,\dots,N\] </li> <li> <p>(b). Dual Feasibility:</p> \[\lambda_i\ge 0,i=1,2,\dots,N\] </li> <li> <p>(c). Complementary Slackness:</p> \[\hat\lambda_i\left(1-y_i\left(\hat{w}^Tx_i-\hat b\right)\right)=0,i=1,2,\dots,N\] </li> <li> <p>(d). Zero gradient of Lagrangian with respect to \(w,b\):</p> \[\frac{\partial L}{\partial b}=0,\quad \frac{\partial L}{\partial w}=0.\] </li> </ul> <p>The conditions (a) and (b) are the original constraints. As for condition (c), recall that we define \(y_i(w^Tx_i-b)=1\) for the data that is exactly \(1/\vert\vert w\vert\vert\) away from the hyperplane \(w^Tx-b=0\), <em>i.e.,</em> on the margin of the hyperplane. For those which are not on the margin, to satisfy KKT conditions, it must follow that</p> \[\hat\lambda_k=0,k\in\{i\vert y_i(w^Tx_i-b)&gt;1,i=1,2,\dots,N\}.\] <p>The condition (d) is for the dual problem. Specifically, we consider the unconstrained problem \(\min_{w,b}L(w,b,\lambda)\) in the dual problem. For the differentiable function \(L(w,b,\lambda)\) , by <em>Fermat’s theorem</em>, the extremum exists when condition (d) is satisfied, \(i.e.,\)</p> \[\frac{\partial L}{\partial b}=\sum_{i=1}^N\lambda_iy_i=0,\\\frac{\partial L}{\partial w}=w-\sum_{i=1}^N\lambda_iy_ix_i=0.\] <p>Solving the equations we have</p> \[\sum_{i=1}^N\lambda_iy_i=0,\forall b,\quad \hat w=\sum_{i=1}^N\lambda_iy_ix_i.\] <p>Plugging them into \(L(w,b,\lambda)\), we can transform the problem into</p> \[\begin{aligned}\max_{\lambda\ge0}&amp;\quad \min_{w,b}L(w,b,\lambda)\\\implies\max_{\lambda\ge0}&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\left(\lambda_iy_ix_i^T\right)\sum_{i=1}^{N}\left(\lambda_iy_ix_i\right)+\sum_{i=1}^N\lambda_i-\sum_{i=1}^N\lambda_iy_i\left(\sum_{j=1}^N\lambda_jy_jx_j^T\right)x_i\\\text{s.t.}&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies\max_{\lambda\ge0}&amp;\quad -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)+\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies \min_\lambda&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\end{aligned}\] <p>The optimal \(\hat\lambda\) can be obtained by <em>sequential minimal optimization</em> (SMO) algorithm. Here we assume we already have the optimal value. Notice we have \(\sum_{i=1}^N\hat\lambda_iy_i=0\), which means there exists at least one \(\hat\lambda_k\ne0\) otherwise \(\hat w=0\). According our analysis, \(\hat\lambda_k\ne 0\) only when</p> \[y_k(w^Tx_k-b)-1=0.\] <p>Therefore we have the solution</p> \[\hat w=\sum_{i=1}^N\hat\lambda_iy_ix_i,\] \[\hat b=\sum_{i=1}^N\hat\lambda_iy_ix_i^Tx_k-y_k.\] <p>Accordingly, the hyperplane is the linear combination of the data on the margin with corresponding \(\hat\lambda_k&gt;0\). We call those data <em>support vectors</em> from where the name SVM comes.</p> <h1 id="2-soft-margin-svm">2. Soft-margin SVM</h1> <p>In practice, there are noise and outliers among the data, which makes the data nonlinearly separable. In that case, hard-margin fails to work. Now we introduce <em>soft-margin SVM</em> which extends SVM to the nonlinearly separable data. Recall that in hard-margin SVM, we have the constraint</p> \[y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\] <p>which confines the model to the linearly separable case. To extent the model to general cases, we introduce <em>loss function</em>, which can be defined as</p> <ul> <li> <p>The number of wrongly classifying:</p> \[\text{loss}=\sum_{i=1}^N I(y_i(w^Tx_i-b)&lt;1),\] <p>where \(I(\cdot)\) is the indicator function. However, such loss function is not differentiable.</p> </li> <li> <p>The sum of the distances between the hyperplane and the outliers:</p> \[\begin{aligned}\text{loss}_i&amp;=\begin{cases}0&amp;y_i(w^Tx_i-b)\ge 1\\1-y_i(w^Tx_i-b)&amp;y_i(w^Tx_i-b)&lt;1\text{ (wrongly classified)}\end{cases}\\&amp;=\max\{0, 1-y_i(w^Tx_i-b)\}.\\\text{loss}&amp;=\sum_{i=1}^N\text{loss}_i,\end{aligned}\] <p>which is called <strong>hinge loss</strong>.</p> </li> </ul> <p>However, the \(\max\) in the hinge loss is not differentiable neither. We now adapt the original constraint as</p> \[y_i(w^Tx_i-b)\ge 1-\xi_i,i=1,2,\dots,N\] <p>where \(\xi_i\ge0\) and \(\sum_{i=1}^N\xi_i\le\) constant are called <em>slack variables</em>. The slack variables is introduced to allow for some points to be on the wrong side of the margin. Specifically, for the points that are on the wrong side, it will break the original constraint \(y_i(w^Tx_i-b)\ge 1\) as</p> \[y_i(w^Tx_i-b)=\xi&lt; 1.\] <p>With slack variables, such classification is allowed as long as</p> \[\xi\ge 1-\xi_i\implies\xi_i\ge1-\xi.\] <p>Moreover, we do not want the \(\xi_i\) to be too large to distinguish points correctly. Thus we have the new formulation</p> \[\begin{aligned}\min_{w,b}&amp;\quad \frac{1}{2}w^Tw+C\sum_{i=1}^N\xi_i\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)\ge 1-\xi_i\\&amp;\quad \xi_i\ge 0\\&amp;\quad i=1,2,\dots,N\end{aligned},\] <p>where \(C\) is the <em>cost</em> parameter that determines to what extent we allow for outliers. To solve the problem one can refer to the hard-margin case as they are actually similar.</p> <h1 id="3-conclusion">3. Conclusion</h1> <p>In this post, we first introduced hard-margin SVM for linearly separable data. By introducing a loss function and slack variables, soft-margin SVM allows for noise and outliers so that it can handle non linear case. The two models can both be solved by <em>convex optimization</em> methods. For convex optimization, we briefly reviewed <em>Lagrange duality</em>, <em>Slater’s condition</em> and <em>KKT conditions</em>.</p>]]></content><author><name></name></author><category term="Machine-Learning"/><summary type="html"><![CDATA[Include support vector machine and Lagrange duality.]]></summary></entry></feed>