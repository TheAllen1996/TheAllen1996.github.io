<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Paper - Reinforcement Learning with Deep Energy-Based Policies | Jingye(Allen) Wang</title> <meta name="author" content="Jingye(Allen) Wang"> <meta name="description" content="A personal summary of paper Reinforcement Learning with Deep Energy-Based Policies."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%82&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://theallen1996.github.io/blog/2021/soft_q_learning/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jingye(Allen) </span>Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Gallery</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper - Reinforcement Learning with Deep Energy-Based Policies</h1> <p class="post-meta">January 11, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/reinforcement-learning"> <i class="fas fa-tag fa-sm"></i> Reinforcement-Learning</a>   </p> </header> <article class="post-content"> <p><em>This is a brief summary of paper <a href="https://arxiv.org/pdf/1702.08165.pdf" rel="external nofollow noopener" target="_blank">Reinforcement Learning with Deep Energy-Based Policies</a> for my personal interest.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-motivation" id="markdown-toc-1-motivation">1. Motivation</a></li> <li> <a href="#2-soft-definition" id="markdown-toc-2-soft-definition">2. Soft Definition</a> <ul> <li><a href="#21-objective-function" id="markdown-toc-21-objective-function">2.1. Objective Function</a></li> <li><a href="#22-soft-function" id="markdown-toc-22-soft-function">2.2. Soft Function</a></li> </ul> </li> <li> <a href="#3-theorem-analyses" id="markdown-toc-3-theorem-analyses">3. Theorem Analyses</a> <ul> <li><a href="#31-policy-improvement" id="markdown-toc-31-policy-improvement">3.1. Policy Improvement</a></li> <li><a href="#32-policy-iteration" id="markdown-toc-32-policy-iteration">3.2. Policy Iteration</a></li> <li><a href="#33-soft-bellman-equation" id="markdown-toc-33-soft-bellman-equation">3.3. Soft Bellman Equation</a></li> <li><a href="#34-soft-value-iteration" id="markdown-toc-34-soft-value-iteration">3.4. Soft Value Iteration</a></li> </ul> </li> <li><a href="#4-algorithm" id="markdown-toc-4-algorithm">4. Algorithm</a></li> <li><a href="#4-conclusion" id="markdown-toc-4-conclusion">4. Conclusion</a></li> <li><a href="#5-references" id="markdown-toc-5-references">5. References</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>After publishing the paper <a href="https://arxiv.org/pdf/1702.08165.pdf" rel="external nofollow noopener" target="_blank">soft Q learning</a> in Jul 2017, the author proposed the influential algorithm <a href="https://arxiv.org/pdf/1801.01290.pdf" rel="external nofollow noopener" target="_blank">SAC</a> in the next year. While SAC has received tremendous publicity, the discussion, if any, about this precedent work generally falls into the rut of <a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/" rel="external nofollow noopener" target="_blank">the excellent blog</a>. Therefore I decided to summarize some of thoughts and problems I had during the reading. Though this post is mainly for my personal interest, any advice will be appreciated.</p> <h1 id="1-motivation">1. Motivation</h1> <p>In reinforcement learning (RL) problem, given an agent and an environment, the objective of the agent is to learn a policy that maximizes the rewards the agent can gain. However, faced with an unknown environment, there is a tradeoff between exploitation and exploration. Before maximum entropy RL, the exploration is generally ensured by external mechanisms, such as \(\varepsilon-\)greedy in DQN and adding exploratory noise to the actions in DDPG. Un like those heuristic and inefficient methods, maximum entropy encourages the agent to conduct exploration by itself based on both reward and entropy. Specifically, in maximum entropy RL, the optimal policy is redefined as</p> \[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\mathbb{E}_{\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{1}\] <p>which adds a regularization term to the standard definition. Many paper adds a parameter \(\alpha\) to entropy, which we will ignore in the following discussion as it does not affect the related conclusion. Based on this redefinition, the motivation of this paper can be summarized as follows:</p> <ul> <li> <p>Generalize it to continuous cases:</p> <p>Before this work, entropy maximization was mainly utilized in discrete cases. Theoretical analyses were needed for applying into continuous cases.</p> </li> <li> <p>Take trajectory-wise entropy into consideration:</p> <p>The term \(\mathcal{H}(\pi(\cdot\vert s_t))\) only considers the entropy of the policy at state \(s_t\). Traditional methods tend to act greedily based on the entropy at the next state. In this work, the author considers the long term entropy reward instead that of the next state.</p> </li> <li> <p>Policy formulation:</p> <p>Even though we have an objective function given the definition, it still needs a probabilistic definition from which we can make sampling. Instead of using conditional Gaussian distribution like many other works, the author borrows the idea of Boltzmann distribution.</p> </li> </ul> <h1 id="2-soft-definition">2. Soft Definition</h1> <p>We now discuss the related definition in this paper.</p> <h2 id="21-objective-function">2.1. Objective Function</h2> <p>The optimal policy in this paper is defined as</p> \[\pi^\ast=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{2}\] <p>which differs from the one defined in \((1)\) as it aims to reach states where they may have high entropy in the future. Specifically, a detailed version is given by</p> \[\pi^\ast=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\left[r(s_l,a_l)+\mathcal{H}(\pi(\cdot\vert s_l))\right]\bigg\vert s_t,a_t\right],\tag{3}\] <p>where the first expectation \(\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\) is over all the pairs \((s_t, a_t)\) at any time step, and the second expectation \(\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\) is over all the trajectories originating from \((s_t,a_t)\).</p> <blockquote> <p>In the original paper, the detailed version is actually given by</p> \[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right]\bigg\vert s_t,a_t\right].\] <p>Problem: The subscripts (shown in red below) in the second expectation does confuse me a lot. Why it is over \(t\) rather than \(l\)?</p> \[r(s_{\color{red}t},a_{\color{red}t})+\mathcal{H}(\pi(\cdot\vert s_{\color{red}t}))?\] </blockquote> <h2 id="22-soft-function">2.2. Soft Function</h2> <p>The corresponding \(Q\)-function in this paper is defined as</p> \[Q_\text{soft}^\pi(s_t,a_t)\triangleq r(s_t,a_t)+\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right].\tag{4}\] <p>Notice that the entropy at state \(s_t\) is omitted in the definition.</p> <p>The corresponding value function is given by</p> \[V_\text{soft}^\pi(s_t)\triangleq \log \int_{\mathcal{A}}\exp\left(Q_\text{soft}^\pi(s_t,a)\right)da,\tag{5}\] <p>which is actually in the form of log sum exponential that approximates maximum.</p> <p>Given the two definitions, the soft policy is then given by</p> \[\pi(a_t\vert s_t)=\exp\left(Q^\pi_\text{soft}(s_t,a_t)-V^\pi_\text{soft}(s_t)\right).\tag{6}\] <p>As \(V^\pi_\text{soft}\) only depends on \(Q^\pi_\text{soft}\), the soft policy is actually the Boltzmann distribution based on the value of \(Q^\pi_\text{soft}\). The comparison is shown in Figure 1. It can be found that Boltzmann distribution assigns a reasonable likelihood for all actions (rather than just the optimal one).</p> <figure> <div style="max-width: 500px;"> <figure> <img src="http://bair.berkeley.edu/static/blog/softq/figure_3a_unimodal-policy.png"> <figcaption><center>(a)</center></figcaption> </figure> <figure> <img src="http://bair.berkeley.edu/static/blog/softq/figure_3b_multimodal_policy.png"> <figcaption><center>(b)</center></figcaption> </figure> </div> </figure> <center> <p style="font-size:100%;"> Figure 1. Policies based on the value of Q function. (a) Unimodal policy. (b) Multimodal policy. </p> </center> <p>With those definitions, we have proposed the solutions to the problems mentioned in the motivation: continuous function for continuous states and actions space; the trajectory-wise optimization defined by the objective function \((3)\); and Boltzmann distribution to represent the optimal policy. To ensure things work, we need theoretical analyses and feasible update rules.</p> <h1 id="3-theorem-analyses">3. Theorem Analyses</h1> <p>We now discuss the related theoretical guarantee. The first theorem shows us the optimality:</p> <p><strong>Theorem 1</strong>: The optimal policy for equation (3) is given by</p> \[\pi^\ast(a_t\vert s_t)=\exp\left(Q^\ast_\text{soft}(s_t,a_t)-V^\ast_\text{soft}(s_t)\right).\] <p>The proof sketch follows two steps: policy improvement and policy iteration.</p> <h2 id="31-policy-improvement">3.1. Policy Improvement</h2> <p>We first show that given any policy, we can improve it by ‘softmizing’ it. Specifically,</p> \[\forall\pi,\text{let }\tilde\pi(\cdot\vert s)\propto\exp\left(Q^\pi_\text{soft}(s,\cdot)\right),\text{then }Q^\pi_\text{soft}(s_t,a_t)\le Q^{\tilde\pi}_\text{soft}(s_t,a_t).\] <p>To show that, we rewrite the second part of \(Q\) function (defined in (4)) as</p> \[\begin{aligned}&amp;\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[[\gamma\mathcal{H}(\pi(\cdot\vert s_{l}))+\gamma r(s_{t+1},a_{t+1})\\&amp;+\sum_{l=t+2}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))]]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[\gamma\mathcal{H}(\pi(\cdot\vert s_{l}))+\gamma (r(s_{t+1},a_{t+1})\\&amp;+ \sum_{l=t+2}^\infty \gamma^{l-t-1}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))])]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[\gamma\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\gamma Q^\pi_\text{soft}(s_{t+1},a_{t+1})].\end{aligned}\] <p>As the entropy term is independent of \(a_{t+1}\), we then have the following equation</p> \[Q^\pi_\text{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}\left[Q^\pi_\text{soft}(s_{t+1},a_{t+1})]\right].\tag{7}\] <p>We then provide an inequality:</p> \[\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\le \mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right].\tag{8}\] <p><em>Proof of (8):</em></p> <p>We rewrite the left hand side of the inequality</p> \[\begin{aligned}&amp;\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\\=&amp;\mathbb{E}_{a_t\sim \pi}\left[{\color{red}{-\log\pi(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \pi}\left[-\log\pi(a_t\vert s_t){\color{red}{+\log\tilde{\pi}(a_t\vert s_t)-\log\tilde{\pi}(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;-\mathbb{E}_{a_t\sim \pi}\left[\log\pi(a_t\vert s_t)-\log\tilde{\pi}(a_t\vert s_t)\right]+\mathbb{E}_{a_t\sim \pi}\left[Q^\pi_\text{soft}(s_t,a_t)-\log\tilde{\pi}(a_t\vert s_t)\right]\\=&amp;-D_\text{KL}(\pi\vert\vert\tilde{\pi})+\mathbb{E}_{a_t\sim\pi}\left[Q^\pi_\text{soft}(s_t,a_t)-Q^\pi_\text{soft}(s_t,a_t)+\log\int_\mathcal{A}\exp(Q^\pi_\text{soft}(s_t,a'))da'\right]\\=&amp;-D_\text{KL}(\pi\vert\vert\tilde{\pi})+\log\int_\mathcal{A}\exp(Q^\pi_\text{soft}(s_t,a'))da’.\end{aligned}\] <p>For the right hand side of the inequality, we have</p> \[\begin{aligned}&amp;\mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[{\color{red}{-\log\tilde\pi(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[-\log\frac{\exp\left(Q^\pi_\text{soft}(s_t,a_t)\right)}{\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da'}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[-Q^\pi_\text{soft}(s_t,a_t)+\log\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da'+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\log\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da’.\end{aligned}\] <p>Since \(D_\text{KL}\ge 0\), we have</p> <p>\(\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\le \mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right].\tag*{\)\blacksquare\(}\)</p> <p>With (7) and (8), we now ready to show policy improvement. The idea is simple: we use inequality (8) to contract the right hand side of equality (7) to complete the proof.</p> <p><em>Proof of policy improvement:</em></p> \[\begin{aligned}Q_\text{soft}^\pi(s_t,a_t)=&amp; r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}[Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\right]\\\le&amp; r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[Q^\pi_\text{soft}(s_{t+1},a_{t+1})]\right]\\=&amp;r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))\\&amp;+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+2}\sim\mathcal{P}}[\mathcal{H}(\pi(\cdot\vert s_{t+2}))+\mathbb{E}_{a_{t+2}\sim\pi}[Q_\text{soft}^\pi(s_{t+2},a_{t+2})]]]\\\le &amp;r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))\\&amp;+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+2}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+2}))+\mathbb{E}_{a_{t+2}\sim{\color{red}{\tilde{\pi}}}}[Q_\text{soft}^\pi(s_{t+2},a_{t+2})]]]\\ \vdots \\ \le &amp; r(s_t,a_t)+\sum_{l=t+1}^\infty \mathbb{E}_{(s_l,a_l)\sim\rho_{\tilde\pi}}\left[r(s_l,a_l)+\mathcal{H}(\tilde\pi(\cdot\vert s_l))\right]\\=&amp; Q^{\tilde\pi}_\text{soft}(s_t,a_t).\end{aligned}\] <p>Therefore we complete the proof. \(\tag*{\)\blacksquare\(}\)</p> <h2 id="32-policy-iteration">3.2. Policy Iteration</h2> <p>With <em>policy improvement</em> theorem, we can improve any arbitrary policy. Therefore the policy can be naturally updated by</p> \[\pi_{i+1}(\cdot \vert s_t)\propto \exp\left(Q^{\pi_i}_\text{soft}(s_t,\cdot)\right).\] <p>Since any policy can be improved in this way, the optimal policy must satisfy this form, and the proof of <em>Theorem 1</em> is completed. \(\tag*{\)\blacksquare\(}\)</p> <h2 id="33-soft-bellman-equation">3.3. Soft Bellman Equation</h2> <p>Though we have that the optimal policy can be obtained by policy iteration, it would be exhausting to conduct the iteration exactly in that way (just think about the integral we omit with the help of \(\propto\))! Therefore, a more feasible way is to find the optimal \(Q\) function (which is why they call the algorithm <em>soft Q learning</em>, I guess) as</p> \[\pi^\ast(a_t\vert s_t)\propto\exp\left(Q^\ast_\text{soft}(s_t,a_t)\right).\] <p>We now show the soft Bellman optimality equation which connects the two optimal function.</p> <p><strong>Theorem 2.</strong> The soft \(Q\) function defined in (4) satisfies the soft Bellman equation</p> \[Q^\ast_\text{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\ast_\text{soft}(s_{t+1})\right].\] <p><em>Proof of Theorem 2</em>:</p> <p>The proof is pretty straightforward. Notice that</p> \[\begin{aligned}&amp;\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}[Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\ =&amp;\mathbb{E}_{a_{t+1}\sim\pi}[-\log \pi(a_{t+1}\vert s_{t+1})+Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\ =&amp;\mathbb{E}_{a_{t+1}\sim\pi}[-\log \exp(Q_\text{soft}^\pi(s_{t+1},a_{t+1})-V_\text{soft}^\pi(s_{t+1}))+Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\ =&amp;\mathbb{E}_{a_{t+1}\sim\pi}[V_\text{soft}^\pi(s_{t+1})]\\ =&amp;V_\text{soft}^\pi(s_{t+1}).\end{aligned}\] <p>Therefore the soft \(Q\) function defined in (4) is equivalent to</p> \[\begin{aligned}Q_\text{soft}^\pi(s_t,a_t)&amp;= r(s_t,a_t)+\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right]\\&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\pi_\text{soft}(s_{t+1})\right].\end{aligned}\] <p>\(\tag*{\)\blacksquare\(}\)</p> <p>Theorem 2 actually sheds light on how we update our \(Q\) function, which will be introduced in next section.</p> <h2 id="34-soft-value-iteration">3.4. Soft Value Iteration</h2> <p>So far we have shown the optimality of soft policy (<em>Theorem 1</em>) and soft \(Q\) function <em>(Theorem 2)</em>. However, we still need a rule to learn the function. Specifically, we mainly focus on the update rule of \(Q\) function as the policy and value function both are defined by \(Q\) function. To this end, the author provides the following theorem.</p> <p><strong>Theorem 3.</strong> The iteration</p> \[Q^\pi_\text{soft}(s_t,a_t)\gets r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\pi_\text{soft}(s_{t+1})\right],\] \[V_\text{soft}^\pi(s_t)\gets \log \int_{\mathcal{A}}\exp\left(Q_\text{soft}^\pi(s_t,a)\right)da,\] <p>converges to \(Q^\ast_\text{soft}\) and \(V^\ast_\text{soft}\), respectively.</p> <p>The proof is quite similar to the general case in RL. For the detailed proof one can refer to the paper directly. Notice that the update of \(Q\) function does not involve the policy, therefore it is an off-policy RL.</p> <h1 id="4-algorithm">4. Algorithm</h1> <p>Given the above analyses, there are two key issues in designing a truly practical algorithm:</p> <ul> <li>The intractable integral for computing the value of \(V^\pi_\text{soft}\);</li> <li>The intractable sampling from Boltzmann distribution.</li> </ul> <p>To deal with the integral issue, this paper leverages <em>importance sampling</em>, which has been widely used in many previous works. For the second issue, generally speaking, the author uses a neural network to approximate the Boltzmann distribution of the policy (rather than the policy itself, and that differs from actor-critic, claimed by the author), and the loss function is defined as</p> \[J_\pi(\phi;s_t)=D_\text{KL}\left(\pi^\phi(a_t\vert s_t)\bigg\vert\bigg\vert\exp\left(Q^\theta_\text{soft}(s_t,a_t)-V^\theta_\text{soft}(s_t)\right)\right).\] <p>The gradient of the loss function is given by <em>Stein Variational Gradient Descent</em> (SVGD). In my view, the use of SVGD is mainly for the analysis of the resemblance between the proposed algorithm, soft Q learning (SQL), and actor-critic, as the succeeding works seem to use no SVGD anymore.</p> <p>The author provides the implementation in <a href="https://github.com/haarnoja/softqlearning" rel="external nofollow noopener" target="_blank">github-softqlearning</a>. However, the latest version is faced with dependencies issue. Luckily, the older version (committed on Oct 30, 2017) works well. Other feasible implementation can be hardly found. For the performance, I tested it on Multigoal environment and the results are consistent with that of the original paper. Besides, I conducted experiments with varying values of \(\alpha\), which is shown in Figure 2.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/images/sql_alpha0_500-680.webp"></source> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/images/sql_alpha0_500-900.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/images/sql_alpha0_500-1600.webp"></source> <img src="/assets/images/sql_alpha0_500.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="$$\alpha=0$$" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/images/sql_alpha05_500-680.webp"></source> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/images/sql_alpha05_500-900.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/images/sql_alpha05_500-1600.webp"></source> <img src="/assets/images/sql_alpha05_500.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="$$\alpha=0.5$$" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 680px)" srcset="/assets/images/sql_alpha1_500-680.webp"></source> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/images/sql_alpha1_500-900.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1600px)" srcset="/assets/images/sql_alpha1_500-1600.webp"></source> <img src="/assets/images/sql_alpha1_500.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="$$\alpha=1$$" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. After 500 steps, the performance of SQL in Multigoal with differnt values of alpha=0, 0.5, 1, respectively. </div> <p>Generally speaking, when \(\alpha\ne 0\), SQL tends to have a high variance, which can also be viewed as the cost for exploration.</p> <h1 id="4-conclusion">4. Conclusion</h1> <p>It was proven that bringing in the entropy term does work for the end of better exploring. The author successfully extended the entropy RL framework to contiunous case in this paper. However, the high variance makes it challenging to be used for performing complicated tasks, and that may be one of the reasons why little (relatively) ink has been spilled on SQL. A wise choice could be to use SQL as an initializer rather than a trainer.</p> <h1 id="5-references">5. References</h1> <p><a href="https://arxiv.org/pdf/1702.08165.pdf" rel="external nofollow noopener" target="_blank">Reinforcement Learning with Deep Energy-Based Policies</a> - Tuomas Haarnoja et al.</p> <p><a href="https://arxiv.org/pdf/1801.01290.pdf" rel="external nofollow noopener" target="_blank">Soft Actor-Critic</a> - Tuomas Haarnoja et al.</p> <p><a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/" rel="external nofollow noopener" target="_blank">Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning</a> - BAIR</p> <p><a href="https://julien-vitay.net/deeprl/EntropyRL.html" rel="external nofollow noopener" target="_blank">Deep Reinforcement Learning</a> - Julien Vitay</p> <p><a href="https://www.slideshare.net/DongMinLee32/maximum-entropy-reinforcement-learning-stochastic-control" rel="external nofollow noopener" target="_blank">Maximum Entropy Reinforcement Learning (Stochastic Control)</a> - Dongmin Lee</p> <p><a href="https://arxiv.org/pdf/1608.04471.pdf" rel="external nofollow noopener" target="_blank">Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a> - Qiang Liu and Dilin Wang</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jingye(Allen) Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PD575TTWH8"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PD575TTWH8");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>