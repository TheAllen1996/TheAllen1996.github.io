<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Machine Learning - 04 Dimensionality Reduction | Jingye(Allen) Wang</title> <meta name="author" content="Jingye(Allen) Wang"> <meta name="description" content="Include PCA and its variants."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%82&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://theallen1996.github.io/blog/2020/dimensionality_reduction-ml04/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jingye(Allen) </span>Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Gallery</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning - 04 Dimensionality Reduction</h1> <p class="post-meta">October 26, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fas fa-tag fa-sm"></i> Machine-Learning</a>   </p> </header> <article class="post-content"> <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session" rel="external nofollow noopener" target="_blank">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/" rel="external nofollow noopener" target="_blank">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" rel="external nofollow noopener" target="_blank">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-curse-of-dimensionality" id="markdown-toc-0-curse-of-dimensionality">0. Curse of Dimensionality</a></li> <li><a href="#1-sample-mean-and-variance" id="markdown-toc-1-sample-mean-and-variance">1. Sample Mean and Variance</a></li> <li><a href="#2-principal-component-analysis" id="markdown-toc-2-principal-component-analysis">2. Principal Component Analysis</a></li> <li><a href="#3-principal-component-analysis---an-svd-perspective" id="markdown-toc-3-principal-component-analysis---an-svd-perspective">3. Principal Component Analysis - An SVD Perspective</a></li> <li><a href="#4-principal-coordinates-analysis" id="markdown-toc-4-principal-coordinates-analysis">4. Principal Coordinates Analysis</a></li> <li><a href="#5-probabilistic-principal-component-analysis" id="markdown-toc-5-probabilistic-principal-component-analysis">5. Probabilistic Principal Component Analysis</a></li> <li><a href="#6-conclusion" id="markdown-toc-6-conclusion">6. Conclusion</a></li> </ul> <h1 id="0-curse-of-dimensionality">0. Curse of Dimensionality</h1> <p><em>The following introduction is derived from <a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf" rel="external nofollow noopener" target="_blank">Pattern Recognition and Machine Learning</a>.</em></p> <p>High dimensionality often incurs not only calculation issue but also <em>overfitting</em>. The increasing in dimension can point to the methods becoming rapidly unwieldy and of limited practical utility.</p> <p>Our geometrical intuitions, formed through a life spent in a space of three-dimension, can fail badly when we consider spaces of higher dimensionality. As a simple example, consider a sphere of radius \(r = 1\) in a space of \(d\) dimensions, and ask what is the fraction of the volume of the sphere that lies between radius \(r=1-\varepsilon\) and \(r=1\). We can evaluate this fraction by noting that the volume of a sphere of radius \(r\) in \(d\) dimensions must scale as \(r^d\), and so we write</p> \[V_d(r)=K_dr^d,\] <p>where the constant \(K_d\) depends only on \(d\). Thus the required fraction is given by</p> \[\frac{V_d(1)-V_d(1-\varepsilon)}{V_d(1)}=\frac{K_dr^d-K_d(1-\varepsilon)^d}{K_dr^d}=1-(1-\varepsilon)^d.\] <p>Obviously, for large \(d\), this fraction tends to 1 even for small values of \(\varepsilon\). Thus, in spaces of high dimensionality, most of the volume of a sphere is concentrated in a thin shell near the surface!</p> <p>Luckily, for those data of high dimensionality, we can refer to <em>dimensionality reduction</em> algorithms.</p> <h1 id="1-sample-mean-and-variance">1. Sample Mean and Variance</h1> <p>Before we move on, we give the following definitions. Suppose we have data \(X=(x_1,x_2,\dots,x_N)^T\in\mathbb{R}^{N\times d}\), where \(x_i\in\mathbb{R}^{d\times 1}\) is a sample with \(d\) features. Specifically,</p> \[X=(x_1,x_2,\dots,x_N)^T=\begin{pmatrix}x_{1}^T\\x_{2}^T\\\vdots\\x_{N}^T\end{pmatrix}=\begin{pmatrix}x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1d}\\x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2d}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\x_{N1}&amp;x_{N2}&amp;\cdots&amp;x_{Nd}\end{pmatrix}_{N\times d}.\] <p>Then we have the <strong>sample mean</strong></p> \[\bar X=\frac{1}{N}\sum_{i=1}^Nx_i,\] <p>and the <strong>sample covariance</strong></p> \[S=\frac{1}{N}\sum_{i=1}^N\left(x_i-\bar X\right)\left(x_i-\bar X\right)^T.\] <p>However, the <em>sum</em> operation is inconvenient in calculating. Thus we further transform them into</p> \[\bar X=\frac{1}{N}(x_1,x_2,\dots,x_N)\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix}=\frac{1}{N}X^T\mathbf{1}_{N\times 1},\] <p>and</p> \[\begin{aligned}S&amp;=\frac{1}{N}\underbrace{(x_1-\bar X,x_2-\bar X,\dots,x_N-\bar X)}_{X^T-\bar X(1,1,\dots,1)}\begin{pmatrix}(x_1-\bar X)^T\\(x_2-\bar X)^T\\\vdots\\(x_N-\bar X)^T\end{pmatrix}\\&amp;=\frac{1}{N}\left(X^T-\bar X\mathbf{1}_{1\times N}\right)\left(X^T-\bar X\mathbf{1}_{1\times N}\right)^T\\&amp;=\frac{1}{N}\left(X^T-\frac{1}{N}X^T\mathbf{1}_{N\times 1}\mathbf{1}_{1\times N}\right)\left(X^T-\frac{1}{N}X^T\mathbf{1}_{N\times 1}\mathbf{1}_{1\times N}\right)^T\\&amp;=\frac{1}{N}X^T\underbrace{\left(\mathbf{I}_{N}-\frac{1}{N}\mathbf{1}_{N\times N}\right)}_{H}\left(\mathbf{I}_{N}-\frac{1}{N}\mathbf{1}_{N\times N}\right)^TX\\&amp;=\frac{1}{N}X^THH^TX\\&amp;=\frac{1}{N}X^THX,\end{aligned}\] <p>where \(H\) is <em>centering matrix</em>, and it can be shown that \(H^T=H,H^n=H\).</p> <h1 id="2-principal-component-analysis">2. Principal Component Analysis</h1> <p>Principal component analysis (PCA) is defined as an <a href="https://en.wikipedia.org/wiki/Orthogonal_transformation" rel="external nofollow noopener" target="_blank">orthogonal</a> <a href="https://en.wikipedia.org/wiki/Linear_transformation" rel="external nofollow noopener" target="_blank">linear transformation</a> that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.</p> <p>Recall that in <a href="https://2ez4ai.github.io/2020/10/14/linear_classification-ml03/" rel="external nofollow noopener" target="_blank">the previous post</a>, we introduce <em>linear discriminant analysis</em> (LDA), which requires a projection that maximizes the distance between different classes. In PCA, instead of classes, we need to find a projection that maximizes the distance (variance) of all the data so that we can reduce the dimensionality (as a projection involves one dimensionality reduction) while losing the least information (as the greatest variance implies most information). Now we give the mathematically formulation.</p> <p>The notation in section 1 will be used in this section as well. Since PCA is sensitive to the variance of the data, it is critical to normalize the variables over all dimensions, which yields</p> \[z_i=x_i-\bar X.\] <p>In many case, the values of \(z_i\in\mathbb{R}^{d\times 1}\) should be scaled in \([0,1]\) but here we just consider the simple case. We then denote the transformation vector as \(W=(w_1,w_2,\dots,w_d)\in\mathbb{R}^{d\times d}\) which is the <em>orthogonal basis</em> of the new coordinate system. Then the new data \(\tilde z_i\), <em>i.e.</em>, the projection of \(z_i\) on the new coordinate system is</p> \[\tilde z_i=z_i^TW=(z_i^Tw_1,z_i^Tw_2,\dots,z_i^Tw_d),\] <p>where \(z_i^Tw_j\in\mathbb{R}\) is the value of \(j\)-th dimension of the new data. Now suppose we want reduce the dimension from \(d\) to \(k\), <em>i.e.,</em> we want to preserve the first \(k\) dimensions of \(\tilde z_i\) while maximizing the variance, which gives us the objective function</p> \[\mathcal{J}(w)=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j-\overline{z^Tw_j}\right)^2,\] <p>where \(\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j-\overline{z^Tw_j}\right)^2\) is the variance of the new data in \(j\)-th dimension. \(\overline{z^Tw_j}\) is the mean of the new data in \(j\)-th dimension, which is</p> \[\begin{aligned}\overline{z^Tw_j}&amp;=\frac{1}{N}\sum_{i=1}^Nz_i^Tw_j\\&amp;=\frac{1}{N}\sum_{i=1}^N(x_i-\bar X)^Tw_j\\&amp;=\frac{1}{N}\left(\sum_{i=1}^Nx_i^Tw_j-N\bar X^Tw_j\right)\\&amp;=\frac{1}{N}\left(\sum_{i=1}^Nx_i^Tw_j-\sum_{i=1}^Nx_i^Tw_j\right)\\&amp;=0\end{aligned}.\] <p>Therefore the objective function follows that</p> \[\begin{aligned}\mathcal{J}(w)&amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j\right)^2\\&amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left((x_i-\bar X)^Tw_j\right)^2\\&amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}w_j^T(x_i-\bar X)(x_i-\bar X)^Tw_j\\&amp;=\sum_{j=1}^kw_j^TSw_j\end{aligned},\] <p>where each \(w_j^TSw_j\) can be maximized independently since \(w_j\) are orthogonal. Combined with the constraint that \(w_j\) is a unit vector, the problem is then equivalent to</p> \[\hat w_j=\arg\max_{w_j} w_j^TSw_j\quad \text{s.t. }w_j^Tw_j=1.\] <p>The problem can be solved by <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="external nofollow noopener" target="_blank">the method of Lagrange multipliers</a> as follows,</p> \[\begin{aligned}\mathcal{L}(w_j,\lambda_j)&amp;=w_j^TSw_j+\lambda_j(1-w_j^Tw_j),\\\frac{\partial\mathcal{L}(w_j,\lambda_j)}{\partial w_j}&amp;=2Sw_j-2\lambda_j w_j.\end{aligned}\] <p>Setting the second equation to be zero, we have the standard eigenvalue problem</p> \[S\hat{w}_j=\lambda_j\hat{w}_j\implies SW=\text{diag}(\lambda)W ,\] <p><em>i.e.,</em> \(\hat w_j\) is actually the eigenvalue of \(S\). Plugging the above equation to the objective function, we have</p> \[\begin{aligned}\max\mathcal{J(\hat{w})}&amp;=\max\sum_{j=1}^k\hat{w}_j^TS\hat{w}_j\\&amp;=\max\sum_{j=1}^k\hat{w}_j^T\lambda_j\hat{w}_j\\&amp;=\max\sum_{j=1}^k\lambda_j\end{aligned}.\] <p>Therefore, to reduce the data from \(d\) to \(k\) dimension, one should select the \(k\) eigenvectors of \(S\) corresponding to the \(k\) largest eigenvalues to construct the \(W\in\mathbb{R}^{d\times k}\), which is</p> \[W=(w_{(1)},w_{(2)},\dots,w_{(k)}),\] <p>where \(w_{(i)}\) is the eigenvector corresponding to the \(i\)-th largest eigenvalue. Then the data in the new coordinate system is</p> \[X^\text{new}=(x_1-\bar X,x_2-\bar X,\dots, x_N-\bar X)^TW.\] <h1 id="3-principal-component-analysis---an-svd-perspective">3. Principal Component Analysis - An SVD Perspective</h1> <p>Now we consider the <em>singular vector decomposition</em> (SVD) of the centralized data,</p> \[HX=U\Sigma V^T,\] <p>where \(U=(u_1,u_2,\dots,u_N)\in\mathbb{R}^{N\times N},\Sigma\in\mathbb{R}^{N\times d},V=(v_1,v_2,\dots,v_N)\in\mathbb{R}^{d\times d}\), and they have the following properties,</p> \[\begin{aligned}UU^T&amp;=U^TU=\mathbf{I}_{N},\\VV^T&amp;=V^TV=\mathbf{I}_{d},\\\Sigma_{ij}=0,\quad i&amp;=0,1,…,N,j=0,1,…,d,i\ne j.\end{aligned}\] <p>We further represent \(\Sigma\) as \(\lambda(\sigma_1,\sigma_2,\dots,\sigma_d)\). Then according to the analysis in section 1, the covariance of the data is</p> \[\begin{aligned}S&amp;=\frac{1}{N}X^THX\\&amp;=\frac{1}{N}X^TH^THX\\&amp;=\frac{1}{N}(HX)^THX\\&amp;=\frac{1}{N}V\Sigma^TU^TU\Sigma V^T\\&amp;=V\text{diag}\left(\frac{\sigma_1^2}{N},\frac{\sigma_2^2}{N},\dots,\frac{\sigma_d^2}{N}\right)V^T\end{aligned}.\] <p>By multiplying \(V\) on both sides, it follows that</p> \[SV=V\text{diag}\left(\frac{\sigma_1^2}{N},\frac{\sigma_2^2}{N},\dots,\frac{\sigma_d^2}{N}\right)\implies Sv_i=\frac{\sigma_i^2}{N}v_i,\] <p>which is consistent with the conclusion of PCA. Therefore, instead of decomposing the covariance matrix \(S\), we can conduct an SVD on the centralized data, which gives the transformation matrix that allows us to obtain the new data</p> \[Z=HX\cdot V.\] <p>By selecting \(k\) vectors of \(V\) according to the single values, we can reduce the original data matrix from \(d\) to \(k\) dimensions. Such decomposition may take advantage when \(N\ll d\). Intuitively, \(HX\in\mathbb{R}^{N\times d}\) and \(S\in\mathbb{R}^{d\times d}\). When \(N\ll d\), decomposing \(HX\) should be more efficient than decomposing \(S\).</p> <h1 id="4-principal-coordinates-analysis">4. Principal Coordinates Analysis</h1> <p><em>Principal coordinates analysis</em> (PCoA) is a well known technique in many fields. It actually can be derived from PCA. Specifically, we consider a matrix</p> \[T=HXX^TH^T.\] <p>By SVD, we have</p> \[T=U\Sigma V^TV\Sigma^TU^T=U\Sigma\Sigma^T U^T,\] <p>which is similar to the decomposition of \(S\) in section 3. Further, by multiplying \(U\Sigma\) on the both sides, we have</p> \[TU\Sigma=U\Sigma\Sigma^TU^TU\Sigma=U\Sigma\text{diag}\left(\sigma_1^2,\sigma_2^2,\dots,\sigma_d^2\right)\implies T(U\Sigma)_i=\sigma^2_i(U\Sigma)_i.\] <p>Therefore, \(U\Sigma\) is actually composed of \(d\) eigenvalues of \(T\). Recall that in section 3, we have the new data as</p> \[Z=HX\cdot V=U\Sigma V^T\cdot V=U\Sigma,\] <p>which implies that by the eigenvalue decomposition of \(T\), we can get the new data directly. Such a dimensionality reduction technique with a different perspective is PCoA. Notice \(T\in\mathbb{R}^{N\times N}\), thus the complexity of PCoA is \(O(N^2)\).</p> <h1 id="5-probabilistic-principal-component-analysis">5. Probabilistic Principal Component Analysis</h1> <p>Just like the notations we used in previous sections, we define the new data we want to transform \(X\) into is \(Z=(z_1,z_2,\dots,z_N)\) where \(z_i\in\mathbb{R}^{k\times 1}\). However, in <em>probabilistic principal component analysis</em> (PPCA), we further introduce randomness as</p> \[z_i\sim\mathcal{N}(\mathbf{0}_{k\times 1}, \mathbf{I}_{k\times k}),\] \[x_i=Wz_i+\mu+\varepsilon,\] \[\varepsilon\sim\mathcal{N}(\mathbf{0}_{d\times 1},\sigma^2\mathbf{I}_{d\times d}),\] <p>where \(W\in\mathbb{R}^{d\times k}, \mu\in\mathbb{R}^{d\times 1}\) and \(\sigma^2\) are the parameters to be learned (\(\mu\) can be viewed as the bias term of many machine learning model). Such randomization can generalize the model to the unseen data. One can also refer such a model to <em>linear Gaussian model</em>. In particular, there are two phases. The first is learning:</p> \[\hat W,\hat \mu, \hat \sigma^2=\arg\max_{W,\mu,\sigma^2}P(X\vert Z;W,\mu,\sigma^2).\] <p>The second is inference (dimensionality reduction):</p> \[Z=\arg\max_{\tilde Z} P(\tilde Z\vert X).\] <p>The learning part can be dealt with <em>expectation–maximization algorithm</em>. The following is the details of the inference procedure. According to the definition, we have</p> \[x_i\vert z_i\sim\mathcal{N}(Wz_i+\mu,\sigma^2\mathbf{I}_{d\times d}),\] <p>where \(z_i\) is a sample rather than a random variable. Also, we have</p> \[\mathbb{E}[x_i]=\mathbb{E}[Wz_i+\mu+\varepsilon]=\mu,\] \[var[x_i]=var[Wz_i+\mu+\varepsilon]=var[Wz_i]+var[\varepsilon]=WW^T+\sigma^2\mathbf{1}_{d\times d},\] <p>and</p> \[x_i\sim\mathcal{N}(\mu, WW^T+\sigma^2\mathbf{1}_{d\times d}).\] <p>Then we consider the covariance \(Cov(x_i,z_i)\),</p> \[\begin{aligned}Cov(x_i,z_i)&amp;=\mathbb{E}[(x_i-\mu)(z_i-\mathbf{0}_{k\times 1})^T]\\&amp;=\mathbb{E}[(x_i-\mu)z_i^T]\\&amp;=\mathbb{E}[(Wz_i+\varepsilon)z_i^T]\\&amp;=\mathbb{E}[Wz_iz_i^T]+\mathbb{E}[\varepsilon z_i^T]\\&amp;=Wvar[z_i]+\mathbb{E}[\varepsilon]\mathbb{E}[z_i^T]\\&amp;=W\end{aligned}.\] <p>Hence the union distribution of \((x_i,z_i)^T\) is</p> \[\begin{pmatrix}x_i\\z_i\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu\\\mathbf{0}_{k\times1} \end{pmatrix},\begin{pmatrix}WW^T+\sigma^2\mathbf{1}_{d\times d}&amp;W\\W^T&amp;\mathbf{1}_{k\times k}\end{pmatrix}\right).\] <p>By the formula derived in <a href="https://2ez4ai.github.io/2020/09/28/intro-ml01/" rel="external nofollow noopener" target="_blank">session 01</a>, it can be shown that</p> \[z_i\vert x_i\sim\mathcal{N}(W^T(WW^T+\sigma^2\mathbf{1}_{d\times d})^{-1}(x_i-\mu),\mathbf{1}_{k\times k}-W^T(WW^T+\sigma^2\mathbf{1}_{d\times d})^{-1}W).\] <h1 id="6-conclusion">6. Conclusion</h1> <p>In this post, we introduced the naive <em>principal component analysis</em> (PCA) model. Then we conducted <em>singular vector decomposition</em> on the centralized data, which gives us the same conclusion as that of PCA. Such conclusion can also be derived from <em>Principal coordinates analysis</em> (PCoA) model. Further, by introducing parameters, <em>probabilistic principal component analysis</em> (PPCA) can generalize PCA to handle unseen data.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jingye(Allen) Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PD575TTWH8"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PD575TTWH8");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>