<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Machine Learning - 05 Support Vector Machine | Jingye(Allen) Wang</title> <meta name="author" content="Jingye(Allen) Wang"> <meta name="description" content="Include support vector machine and Lagrange duality."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%82&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://theallen1996.github.io/blog/2020/support_vector_machine-ml05/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jingye(Allen) </span>Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Gallery</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning - 05 Support Vector Machine</h1> <p class="post-meta">October 28, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fas fa-tag fa-sm"></i> Machine-Learning</a>   </p> </header> <article class="post-content"> <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session" rel="external nofollow noopener" target="_blank">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/" rel="external nofollow noopener" target="_blank">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" rel="external nofollow noopener" target="_blank">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li> <a href="#1-hard-margin-svm" id="markdown-toc-1-hard-margin-svm">1. Hard-margin SVM</a> <ul> <li><a href="#11-problem-formulation" id="markdown-toc-11-problem-formulation">1.1. Problem Formulation</a></li> <li><a href="#12-lagrange-duality" id="markdown-toc-12-lagrange-duality">1.2. Lagrange Duality</a></li> <li><a href="#13-karushkuhntucker-conditions" id="markdown-toc-13-karushkuhntucker-conditions">1.3. Karush–Kuhn–Tucker Conditions</a></li> </ul> </li> <li><a href="#2-soft-margin-svm" id="markdown-toc-2-soft-margin-svm">2. Soft-margin SVM</a></li> <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>Support vector machine (SVM) is a supervised learning method for classification and regression analysis. It is one of the most robust prediction method. Here we mainly consider its applications in classification. Specifically, for the data of \(d\)-dimensional, we want to know whether we can separate classes with a \((d-1)\)-dimensional <em>hyperplane</em>. In particular, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class. According to whether the dataset is linearly separable or not, there are <em>hard-margin</em> SVM, <em>soft-margin</em> SVM and <em>kernel</em> SVM.</p> <h1 id="1-hard-margin-svm">1. Hard-margin SVM</h1> <p>Hard-margin SVM works only when data is completely linearly separable without any errors.</p> <div align="center"> <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png" width="350"> </div> <h2 id="11-problem-formulation">1.1. Problem Formulation</h2> <p>Suppose we have data set \(\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\) where \(x_i\in\mathbb{R}^{d\times 1}\) is the data feature and \(y_i\in\{-1,1\}\) is the corresponding class label. A case where \(d=2\) is shown in the figure above. The hyperplane that separates the data is defined as</p> \[w^Tx-b=0,\] <p>where \(w\in\mathbb{R}^{d\times 1}\) and \(b\in\mathbb{R}\) are parameters to be learned. Then like what we arrived in <a href="https://2ez4ai.github.io/2020/10/14/linear_classification-ml03/" rel="external nofollow noopener" target="_blank">perceptron</a>, a correct classifier should ensure that</p> \[y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N.\] <p>We further define the <em>margin</em> as the parallel lines that has the minimum distance from the data to the hyperplane. In <em>hard-margin</em> SVM, we need to find the <em>maximum-margin</em> hyperplane that maximizes the distance, which can be described by</p> \[\begin{aligned}\max_{w,b}\min_{i}&amp;\quad \frac{\vert w^Tx_i-b\vert}{\vert\vert w\vert\vert}\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N\end{aligned}.\] <p>The problem further is equivalent to</p> \[\begin{alignat*}{3}\max_{w,b}\min_{i}&amp;\quad \frac{y_i(w^Tx_i-b)}{\vert\vert w\vert\vert}\implies\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i&amp;\quad y_i(w^Tx_i-b)\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N\end{alignat*}.\] <p>For the constraint \(y_i(w^Tx_i-b)&gt;0, \forall i=1,2,\dots,N\), there exists a positive parameter \(r&gt;0\) such that</p> \[\min_i y_i(w^Tx_i-b)=r.\] <p>As there are many \(w,b\) available for the separation as long as they have the same directions. We add a new constraint that \(r=1\). Then it follows that</p> \[\min_i y_i(w^Tx_i-b)=y_i((w_{\text{old}}^T/r)x_i-b_{\text{old}}/r)=1.\] <p>The problem then is transformed into</p> \[\begin{alignat*}{3}&amp;&amp;\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i y_i(w^Tx_i-b)&amp;\implies\min_{w,b}\quad \frac{1}{2}w^Tw\\&amp;&amp;\text{s.t.}\quad y_i(w^Tx_i-b)&gt;0&amp;\implies\text{s.t.}\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{alignat*},\] <p>which is a linearly constrained <em>quadratic optimization</em> (QP) problem.</p> <h2 id="12-lagrange-duality">1.2. Lagrange Duality</h2> <p>The following content is about <a href="https://web.stanford.edu/~boyd/cvxbook/" rel="external nofollow noopener" target="_blank">convex optimization</a>. In section 1, we have the <em>primal problem</em></p> \[\begin{aligned}\min_{w,b}&amp;\quad \frac{1}{2}w^Tw\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{aligned}.\] <p>The <em>Lagrangian</em> for the problem is a function defined as</p> \[L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^N\lambda_i\left(1-y_i\left(w^Tx_i-b\right)\right),\] <p>where \(\lambda_i\ge 0\) is the <em>Lagrange multiplier</em> associated with the constraints. Consider the problem of \(\max_\lambda L(w,b,\lambda)\),</p> \[\max_{\lambda\ge 0} L(w,b,\lambda)=\begin{cases}\frac{1}{2}w^Tw+\infty&amp;\text{if }\exists i\in\{1,2,\dots,N\}\text{ s.t. }1-y_i(w^Tx_i-b)&gt;0,\\\frac{1}{2}w^Tw+0&amp;\text{otherwise.}\end{cases}\] <p>The problem makes sense (non infinity) only when the original constraint is satisfied. In that case, the primal problem is equivalent to</p> \[\begin{aligned}\min_{w,b}\max_\lambda&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}.\] <p>Then we define the <em>Lagrange dual function</em> for the primal problem as</p> \[g(\lambda)=\min_{w,b}L(w,b,\lambda).\] <blockquote> <p>Actually, the correct definition of the <em>Lagrange dual function</em> should be</p> \[g(\lambda)=\inf_{w,b}L(w,b,\lambda).\] <p>Here we assume the minimum exists and the infimum is the minimum for understanding.</p> </blockquote> <p>The <em>Lagrange dual problem</em> of the original problem is then defined as</p> \[\begin{aligned}\max_{\lambda}&amp;\quad g(\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}.\] <p>The dual problem is introduced for its convexity. Specifically, notice that the infimum (minimum in this case) of \(g(\lambda)\) is unconstrained as opposed to the original constrained minimization problem. Further, \(g(\lambda)\) is concave with respect to \(\lambda\) regardless of the original problem.</p> <table> <thead> <tr> <th style="text-align: center">Primal Problem</th> <th style="text-align: center">Lagrange Dual Problem</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(\begin{aligned}\min_{w,b}\max_\lambda&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}\)</td> <td style="text-align: center">\(\begin{aligned}\max_{\lambda}\min_{w,b}&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}\)</td> </tr> </tbody> </table> <p>A natural problem is whether the two problems are equivalent. Obviously, the equivalence is obtained if and only if</p> \[\min_{w,b}\max_{\lambda} L(w,b,\lambda)=\max_{\lambda}\min_{w,b} L(w,b,\lambda).\] <p>If the equation holds, we say the <em>strong duality</em> holds. It can also be shown that the <em>weak duality</em> always holds as</p> \[\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).\] <p><em>Proof:</em></p> <p>Obviously, we have</p> \[\max_\lambda L(w,b,\lambda)\ge L(w,b,\lambda)\ge\min_{w,b}L(w,b,\lambda).\] <p>Define \(F(w,b)=\max_\lambda L(w,b,\lambda)\) and \(G(\lambda)=\min_{w,b}L(w,b,\lambda)\). According the above inequality, it follows that</p> \[F(w,b)\ge G(\lambda)\implies\min_{w,b}F(w,b)\ge\max_\lambda G(\lambda).\] <p>Therefore we have</p> <p>\(\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).\tag*{\)\blacksquare\(}\)</p> <p>Solving the dual problem in fact is used to find nontrivial lower bounds for difficult original problems. In our case, the strong duality holds for the linearly constrained QP problem. Thus to solve the primal problem is to solve the dual problem.</p> <h2 id="13-karushkuhntucker-conditions">1.3. Karush–Kuhn–Tucker Conditions</h2> <p>For the primal problem and its dual problem, if the strong duality holds, then <em>Karush–Kuhn–Tucker (KKT) conditions</em> are satisfied as</p> <ul> <li> <p>(a). Primal Feasibility:</p> \[y_i(w^Tx_i-b)\ge1,i=1,2,\dots,N\] </li> <li> <p>(b). Dual Feasibility:</p> \[\lambda_i\ge 0,i=1,2,\dots,N\] </li> <li> <p>(c). Complementary Slackness:</p> \[\hat\lambda_i\left(1-y_i\left(\hat{w}^Tx_i-\hat b\right)\right)=0,i=1,2,\dots,N\] </li> <li> <p>(d). Zero gradient of Lagrangian with respect to \(w,b\):</p> \[\frac{\partial L}{\partial b}=0,\quad \frac{\partial L}{\partial w}=0.\] </li> </ul> <p>The conditions (a) and (b) are the original constraints. As for condition (c), recall that we define \(y_i(w^Tx_i-b)=1\) for the data that is exactly \(1/\vert\vert w\vert\vert\) away from the hyperplane \(w^Tx-b=0\), <em>i.e.,</em> on the margin of the hyperplane. For those which are not on the margin, to satisfy KKT conditions, it must follow that</p> \[\hat\lambda_k=0,k\in\{i\vert y_i(w^Tx_i-b)&gt;1,i=1,2,\dots,N\}.\] <p>The condition (d) is for the dual problem. Specifically, we consider the unconstrained problem \(\min_{w,b}L(w,b,\lambda)\) in the dual problem. For the differentiable function \(L(w,b,\lambda)\) , by <em>Fermat’s theorem</em>, the extremum exists when condition (d) is satisfied, \(i.e.,\)</p> \[\frac{\partial L}{\partial b}=\sum_{i=1}^N\lambda_iy_i=0,\\\frac{\partial L}{\partial w}=w-\sum_{i=1}^N\lambda_iy_ix_i=0.\] <p>Solving the equations we have</p> \[\sum_{i=1}^N\lambda_iy_i=0,\forall b,\quad \hat w=\sum_{i=1}^N\lambda_iy_ix_i.\] <p>Plugging them into \(L(w,b,\lambda)\), we can transform the problem into</p> \[\begin{aligned}\max_{\lambda\ge0}&amp;\quad \min_{w,b}L(w,b,\lambda)\\\implies\max_{\lambda\ge0}&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\left(\lambda_iy_ix_i^T\right)\sum_{i=1}^{N}\left(\lambda_iy_ix_i\right)+\sum_{i=1}^N\lambda_i-\sum_{i=1}^N\lambda_iy_i\left(\sum_{j=1}^N\lambda_jy_jx_j^T\right)x_i\\\text{s.t.}&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies\max_{\lambda\ge0}&amp;\quad -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)+\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies \min_\lambda&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\end{aligned}\] <p>The optimal \(\hat\lambda\) can be obtained by <em>sequential minimal optimization</em> (SMO) algorithm. Here we assume we already have the optimal value. Notice we have \(\sum_{i=1}^N\hat\lambda_iy_i=0\), which means there exists at least one \(\hat\lambda_k\ne0\) otherwise \(\hat w=0\). According our analysis, \(\hat\lambda_k\ne 0\) only when</p> \[y_k(w^Tx_k-b)-1=0.\] <p>Therefore we have the solution</p> \[\hat w=\sum_{i=1}^N\hat\lambda_iy_ix_i,\] \[\hat b=\sum_{i=1}^N\hat\lambda_iy_ix_i^Tx_k-y_k.\] <p>Accordingly, the hyperplane is the linear combination of the data on the margin with corresponding \(\hat\lambda_k&gt;0\). We call those data <em>support vectors</em> from where the name SVM comes.</p> <h1 id="2-soft-margin-svm">2. Soft-margin SVM</h1> <p>In practice, there are noise and outliers among the data, which makes the data nonlinearly separable. In that case, hard-margin fails to work. Now we introduce <em>soft-margin SVM</em> which extends SVM to the nonlinearly separable data. Recall that in hard-margin SVM, we have the constraint</p> \[y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\] <p>which confines the model to the linearly separable case. To extent the model to general cases, we introduce <em>loss function</em>, which can be defined as</p> <ul> <li> <p>The number of wrongly classifying:</p> \[\text{loss}=\sum_{i=1}^N I(y_i(w^Tx_i-b)&lt;1),\] <p>where \(I(\cdot)\) is the indicator function. However, such loss function is not differentiable.</p> </li> <li> <p>The sum of the distances between the hyperplane and the outliers:</p> \[\begin{aligned}\text{loss}_i&amp;=\begin{cases}0&amp;y_i(w^Tx_i-b)\ge 1\\1-y_i(w^Tx_i-b)&amp;y_i(w^Tx_i-b)&lt;1\text{ (wrongly classified)}\end{cases}\\&amp;=\max\{0, 1-y_i(w^Tx_i-b)\}.\\\text{loss}&amp;=\sum_{i=1}^N\text{loss}_i,\end{aligned}\] <p>which is called <strong>hinge loss</strong>.</p> </li> </ul> <p>However, the \(\max\) in the hinge loss is not differentiable neither. We now adapt the original constraint as</p> \[y_i(w^Tx_i-b)\ge 1-\xi_i,i=1,2,\dots,N\] <p>where \(\xi_i\ge0\) and \(\sum_{i=1}^N\xi_i\le\) constant are called <em>slack variables</em>. The slack variables is introduced to allow for some points to be on the wrong side of the margin. Specifically, for the points that are on the wrong side, it will break the original constraint \(y_i(w^Tx_i-b)\ge 1\) as</p> \[y_i(w^Tx_i-b)=\xi&lt; 1.\] <p>With slack variables, such classification is allowed as long as</p> \[\xi\ge 1-\xi_i\implies\xi_i\ge1-\xi.\] <p>Moreover, we do not want the \(\xi_i\) to be too large to distinguish points correctly. Thus we have the new formulation</p> \[\begin{aligned}\min_{w,b}&amp;\quad \frac{1}{2}w^Tw+C\sum_{i=1}^N\xi_i\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)\ge 1-\xi_i\\&amp;\quad \xi_i\ge 0\\&amp;\quad i=1,2,\dots,N\end{aligned},\] <p>where \(C\) is the <em>cost</em> parameter that determines to what extent we allow for outliers. To solve the problem one can refer to the hard-margin case as they are actually similar.</p> <h1 id="3-conclusion">3. Conclusion</h1> <p>In this post, we first introduced hard-margin SVM for linearly separable data. By introducing a loss function and slack variables, soft-margin SVM allows for noise and outliers so that it can handle non linear case. The two models can both be solved by <em>convex optimization</em> methods. For convex optimization, we briefly reviewed <em>Lagrange duality</em>, <em>Slater’s condition</em> and <em>KKT conditions</em>.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jingye(Allen) Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PD575TTWH8"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PD575TTWH8");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>