<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Machine Learning - 09 Exact Inference of Graphical Models | Jingye(Allen) Wang</title> <meta name="author" content="Jingye(Allen) Wang"> <meta name="description" content="A very abstract post as it involves graphs and trees while providing no figures and examples."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%82&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://theallen1996.github.io/blog/2020/inference-ml09/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jingye(Allen) </span>Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Gallery</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning - 09 Exact Inference of Graphical Models</h1> <p class="post-meta">December 5, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fas fa-tag fa-sm"></i> Machine-Learning</a>   </p> </header> <article class="post-content"> <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session" rel="external nofollow noopener" target="_blank">session</a>, <a href="https://ermongroup.github.io/cs228-notes/" rel="external nofollow noopener" target="_blank">CS228-notes</a> and <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" rel="external nofollow noopener" target="_blank">PRML</a>. For the fundamental of probability, one can refer to <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view" rel="external nofollow noopener" target="_blank">Introduction to Probability</a>. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-variable-elimination" id="markdown-toc-1-variable-elimination">1. Variable Elimination</a></li> <li> <a href="#2-belief-propagation" id="markdown-toc-2-belief-propagation">2. Belief Propagation</a> <ul> <li><a href="#21-message" id="markdown-toc-21-message">2.1. Message</a></li> <li><a href="#22-sum-product" id="markdown-toc-22-sum-product">2.2. Sum-Product</a></li> <li><a href="#23-max-product" id="markdown-toc-23-max-product">2.3 Max-Product</a></li> </ul> </li> <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>In the last <a href="https://2ez4ai.github.io/2020/11/29/probabilistic_graphical_models-ml08/" rel="external nofollow noopener" target="_blank">post</a>, we introduce graphical models which is capable of representing random variables and the conditional independences among them. We now consider the problem of inference in graphical models. Particularly, we wish to compute posterior distributions of one or more nodes conditioned on some other known (observed) nodes, and the techniques we shall talk in this post are for <em>exact inference</em>.</p> <h1 id="1-variable-elimination">1. Variable Elimination</h1> <p>We first consider the marginal inference in a <em>chain</em> Bayesian network which has the following joint distribution,</p> \[P(x_1,x_2,\dots,x_n)=P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1}).\] <p>Suppose we want to infer the marginal distribution \(P(x_n)\). A naive way to do that is marginalizing \(x_1,x_2,\dots,x_{n-1}\):</p> \[P(x_n)=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1,x_2,\dots,x_{n-1}).\] <p>However, as there are \(n-1\) variables each with \(k\) states, the computation needs to sum the probability over \(k^{n-1}\) values and would scale exponentially with the length of the chain. To simplify the computation, we can leverage <em>variable elimination</em>.</p> <p>The key of <em>variable elimination</em> is in <em>rearranging the order</em> of the summations and the multiplications. Specifically, the marginal distribution follows that</p> \[\begin{aligned}P(x_n)&amp;=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1})\\&amp;=\sum_{x_{n-1}}P(x_n\vert x_{n-1})\cdot \sum_{x_{n-2}}P(x_{n-1}\vert x_{n-2})\cdot\dots\cdot \sum_{x_{1}}P(x_{2}\vert x_{1})P(x_1).\end{aligned}\] <p>Such a rearrangement works because multiplication is distributive over addition,</p> \[ab+ac=a(b+c),\] <p>where the number of arithmetic operations are reduced from three (the left-hand side) to two (the right-hand side). Before we move on, we generalize the new expression to a Markov network since every Bayesian network can be transformed into a Markov network. For the chain Bayesian network we considered, we can remove all the arrows of the graph to obtain a Markov network. The obtained Markov network would have maximum cliques \(\{x_1,x_2\}\), …, \(\{x_{n-2},x_{n-1}\}\) and \(\{x_{n-1},x_{n}\}\) and the corresponding potentials are</p> \[\begin{aligned}\psi_{1,2}(x_1,x_2)&amp;=P(x_2\vert x_1)P(x_1),\\&amp;\vdots\\\psi_{x_{n-2},x_{n-1}}(x_{n-2},x_{n-1})&amp;=P(x_{n-2}\vert x_{n-1}),\\\psi_{n-1,n}(x_{n-1},x_n)&amp;=P(x_{n-1}\vert x_n).\end{aligned}\] <p>The rearrangement for the Markov network then follows that</p> \[P(x_n)=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1}, x_{n})\cdot \sum_{x_{n-2}}\psi_{n-2,n-1}(x_{n-2}, x_{n-1})\cdot\dots\cdot \sum_{x_{1}}\psi_{1,2}(x_1,x_2).\] <p>Now the computation composes of \(n-1\) summations. More importantly, unlike the previous one sums over \(k^{n-1}\) values, this expression allows each term only need to sum over \(k\times k\) values. Specifically, the sum</p> \[\sum_{x_i}\psi_{x_i,x_{i+1}}(x_{i},x_{i+1})\] <p>only involves two variables and thus the summation is over \(k\times k\) values. Then the overall computation is of \(O(nk^2)\) complexity, which is much better than the naive \(O(k^n)\) method.</p> <p>However, the variable elimination (VE) method requires an ordering over the variables. In fact, the running time of VE on different orderings would vary greatly, while to find the best ordering is still an NP-hard problem. Moreover, VE method for \(P(x_n)\) can be hard to be generalized to other marginal distribution as it does not store the intermediate results.</p> <h1 id="2-belief-propagation">2. Belief Propagation</h1> <p>For convenience, we consider undirected graphs with tree structure, where the optimal variable elimination ordering for node \(x_i\) is the post-order iteration of the subtree rooted at \(x_i\). The relationship between any two directly connected nodes is decided by which node the tree is rooted at and how far the two nodes are away from the root: the close one is the parent of the farther one.</p> <h2 id="21-message">2.1. Message</h2> <p>For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal \(P(x_i)\), we need to eliminate all nodes that are in the subtree of \(x_i\). For node \(x_j\), the elimination involves computing \(\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\) where \(x_k\) is the parent of \(x_j\) in the tree. The term \(m_{j,k}\) can be thought of a <em>message</em> that \(x_j\) sends to \(x_k\) about the subtree rooted at \(x_j\). Similarly, the computing result can be viewed as</p> \[m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\] <p>that contains the information for \(x_l\), the parent of \(x_k\), about the subtree rooted at \(x_k\). By doing so, at the end of VE, \(x_i\) would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.</p> <p>Suppose that after computing \(P(x_i)\), we are interested in computing \(P(x_k)\) as well. If we use VE algorithm again, we can find that the computation also involves the messages \(m_{j,k}\) as node \(x_k\) is still the parent of node \(x_j\). Moreover, such a message is exactly the same as the one used in computing \(P(x_i)\) since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.</p> <h2 id="22-sum-product">2.2. Sum-Product</h2> <p>Belief propagation can be viewed as a combination of VE and <em>caching</em>. For each edge between \(x_i\) and \(x_j\), the messages passing on it are \(m_{i,j}\) and \(m_{j,i}\), which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.</p> <p>Belief propagation:</p> <ul> <li>Set a node, for example, node \(x_i\), as the root;</li> <li>For each \(x_j\) in \(N(x_i)\), <em>i.e.,</em> the neighborhood of \(x_i\), collect the messages sent to \(x_i\):</li> </ul> \[m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};\] <ul> <li>For each \(x_j\) in \(N(x_i)\), collect the messages sent from \(x_i\):</li> </ul> \[m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.\] <p>By doing so, we can obtain all the messages with \(2\vert E\vert\) steps, where \(E\) is the set of edges. Then for any marginal we have</p> \[P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.\] <h2 id="23-max-product">2.3 Max-Product</h2> <p>We now consider a problem of finding the set of values that have the largest probability so that</p> \[\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).\] <p>Notice that</p> \[\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).\] <p>By <em>sum-product</em>, we have</p> \[\begin{aligned}\max_{\text{x}} P(\text{x})&amp;=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&amp;=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\\&amp;\quad\ \dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned}\] <p>Such a method for maximizing <em>max-product</em> is known as <em>max-product</em> algorithm.</p> <h1 id="3-conclusion">3. Conclusion</h1> <p>In this post, we briefly introduced two algorithms for <em>exact inference</em> in graphical models. Given a proper order of nodes, <em>variable elimination</em> algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, <em>belief propagation</em> stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random variables with the largest probability, which is known as <em>max-product</em>.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jingye(Allen) Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PD575TTWH8"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PD575TTWH8");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>