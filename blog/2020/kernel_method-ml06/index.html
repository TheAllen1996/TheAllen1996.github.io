<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Machine Learning - 06 Kernel Method | Jingye(Allen) Wang</title> <meta name="author" content="Jingye(Allen) Wang"> <meta name="description" content="Introduce the basic concept of kernel methods and two properties of it with proof details."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%82&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://theallen1996.github.io/blog/2020/kernel_method-ml06/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jingye(Allen) </span>Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Gallery</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning - 06 Kernel Method</h1> <p class="post-meta">November 5, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fas fa-tag fa-sm"></i> Machine-Learning</a>   </p> </header> <article class="post-content"> <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session" rel="external nofollow noopener" target="_blank">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/" rel="external nofollow noopener" target="_blank">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" rel="external nofollow noopener" target="_blank">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-kernel-method" id="markdown-toc-1-kernel-method">1. Kernel method</a></li> <li><a href="#2-kernel-function" id="markdown-toc-2-kernel-function">2. Kernel function</a></li> <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>Kernel methods are a class of algorithms for pattern analysis. The name of kernel methods comes from the use of <em>kernel function</em>, which enable operations in a high-dimensional and implicit space. Specifically, by <a href="https://en.wikipedia.org/wiki/Cover%27s_theorem" rel="external nofollow noopener" target="_blank">Cover’s theorem</a>, given a set of training data that is not <em>linearly separable</em>, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some <em>non-linear transformation</em>. With the help of kernel function, the operation, <em>i.e.,</em> inner product, it involves after transforming can be often computationally cheaper than the explicit computation. Such an approach is called the <em>kernel trick</em>. In this post, we will focus on the application of kernel method to SVM.</p> <h1 id="1-kernel-method">1. Kernel method</h1> <p>Define the data set as \(\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}, X=\{x_1,x_2,\dots,x_N\}\) and \(Y=\{y_1,y_2,\dots,y_N\}\) where \(x_i\in\mathbb{R}^{d\times 1}\) and \(y_i\in\{-1,1\}\). We further assume that the data set is non-linearly separable. Kernel method supposes that there is a non-linear transformation \(\phi(x):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1},d&lt;p,\) such that \(\mathcal{D}_p=\{(\phi(x_1),y_1),(\phi(x_2),y_2),\dots,(\phi(x_N),y_N)\}\) are linearly separable. For such a linearly separable data set, recalling the problem we formulated in section 1.3 of <a href="https://2ez4ai.github.io/2020/10/28/support_vector_machine-ml05/" rel="external nofollow noopener" target="_blank">SVM</a>, we have the duality problem</p> \[\begin{aligned}\min_\lambda&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_j\phi^T(x_i)\phi(x_j)\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;\quad \sum_{i=1}^N\lambda_iy_i=0.\end{aligned}\] <p>However, after transforming, the inner product \(\phi(x_i)^T\phi(x_j)=\langle\phi(x_i),\phi(x_j)\rangle\) could be hard to obtain (consider the case that \(\phi(\cdot)\) has infinite dimensions), which requires the aid of <em>kernel function</em>.</p> <h1 id="2-kernel-function">2. Kernel function</h1> <p>A kernel function is defined as \(K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}\). Specifically, for non-linear transformation \(\phi(\cdot)\in\mathcal{H}\text{ (Hilbert space) }:\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}\) and any \(x_i,x_j\in\mathbb{R}^{d\times 1}\), we call \(K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle\) a kernel function. Such a kernel function is regarded <em>positive definite</em>, which satisfies</p> \[K(x_i,x_j)=K(x_j,x_i),\] <p>and for \(x_{1},x_{2},\dots,x_{N}\in\mathbb{R}^{d\times 1},\)</p> \[\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\text{ is a positive semi-definite (PSD) matrix},\] <p>where \(\mathcal{K}\) is called <em>Gram matrix</em> of \(K\) over set \(\{x_{1},x_{2},\dots,x_{N}\}\). When the explicit expression of \(\phi(\cdot)\) is hard to be determined, it quite often to show the positive definiteness of a kernel function via its corresponding Gram matrix.</p> <p><strong>(Properties)</strong> We now show two <em>properties</em> of kernel functions.</p> <ul> <li>Let \(K\) be kernel function such that \(K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}\), then we define its Gram matrix \(\mathcal{K}\in\mathbb{R}^{N\times N}\) over \(\{x_1,x_2,\dots,x_N\}\) where \(x_i\in\mathbb{R}^{d\times 1}\). Considering the mapping function \(\phi(\cdot):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}\), we have</li> </ul> \[K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle, \phi(\cdot)\in\mathcal{H}\implies \begin{cases}K(x_i,x_j)=K(x_j,x_i)\\ \mathcal{K}\text{ is a PSD matrix}\end{cases}.\] <p><em>Proof</em>:</p> <p>By the definition of \(K(x_i,x_j)\), we have</p> \[K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle,\quad K(x_j,x_i)=\langle \phi(x_j),\phi(x_i)\rangle.\] <p>By the symmetry of inner product, we have \(\langle \phi(x_i),\phi(x_j)\rangle=\langle \phi(x_j),\phi(x_i)\rangle\). It then follows that</p> \[K(x_i,x_j)=K(x_j,x_i).\] <p>Therefore the Gramian matrix \(\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\) is symmetric real matrix. Now we show that \(\forall\alpha\in\mathbb{R}^{R\times 1}, \alpha^T\mathcal{K}\alpha\ge 0.\) The notation is given by</p> \[\begin{aligned}\alpha^T\mathcal{K}\alpha=(\alpha_1,\alpha_2,\dots,\alpha_N)\begin{bmatrix}K_{11}&amp;K_{12}&amp;\dots&amp;K_{1N}\\K_{21}&amp;K_{22}&amp;\dots&amp;K_{2N}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\K_{N1}&amp;K_{N2}&amp;\dots&amp;K_{NN}\end{bmatrix}\begin{pmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_N\end{pmatrix}\end{aligned},\] <p>where \(K_{ij}=K(x_{ri},x_{rj})\). We then have</p> \[\begin{aligned}\alpha^T\mathcal{K}\alpha&amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_jK_{ij}\\&amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_j\phi^T(x_{ri})\phi(x_{rj})\\&amp;=\sum_{i=1}^R \alpha_i\phi^T(x_{ri})\sum_{j=1}^R\alpha_j\phi(x_{rj})\\&amp;=\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right)^T\left(\sum_{j=1}^R\alpha_j\phi(x_{rj})\right)\\&amp;=\left\langle\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right), \left(\sum_{j=1}^R \alpha_i\phi(x_{rj})\right)\right\rangle\\&amp;=\left\vert\left\vert\sum_{i=1}^R \alpha_i\phi(x_{ri})\right\vert\right\vert^2,\end{aligned}\] <p>therefore, \(\alpha^T\mathcal{K}\alpha\ge 0\) and \(\mathcal{K}\) is a PSD matrix.\(\tag*{\)\blacksquare\(}\)</p> <ul> <li>Let \(\mathcal{K}\in\mathbb{R}^{d\times d}\) be a symmetric PSD matrix, then for \(\{x_1,x_2,\dots,x_N\}\) where \(x_i\in\mathbb{R}^{d\times 1}\), we have kernel function \(K(x_i,x_j)=x_i^T\mathcal{K}x_j\).</li> </ul> <p><em>Proof</em>:</p> <p>Consider the <em>diagonalisation</em> of \(\mathcal{K}=Q^T\Lambda Q\) by an orthogonal matrix \(Q\), where \(\Lambda\) is a diagnoal matrix containing the non-negative eigenvalues of \(\mathcal{K}\). Let \(\sqrt{\Lambda}\) be the diagonal matrix with the square roots of the eigenvalues and set \(A=\sqrt{\Lambda}Q\). Then for \(\{x_1,x_2,\dots,x_N\}\) where \(x_i\in\mathbb{R}^{d\times 1}\), we have</p> \[x_i^T\mathcal{K}x_j=x_i^TQ^T\Lambda Qx_j=x_i^TA^TA x_j=\langle A x_i,Ax_j\rangle.\] <p>Therefore we have kernel function \(K(x_i,x_j)=x_i^T\mathcal{K}x_j=\langle Ax_i,Ax_j\rangle\) with linear transformation \(\phi(\cdot)=A\cdot. \tag*{\)\blacksquare\(}\)</p> <h1 id="3-conclusion">3. Conclusion</h1> <p>In this post, we introduced <em>kernel method</em> for classification problem. Given <em>Cover’s theorem</em>, we can project non-linear data into high-dimensional space and obtain linearly separable data. To simplify the computation incurred by the duality problem, we can leverage <em>kernel function</em> to avoid the computing labor.</p> <p>This is definitely not a good introduction to kernel methods. For more details of kernel method, I would recommend <a href="https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf" rel="external nofollow noopener" target="_blank">Kernel methods: an overview</a>.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jingye(Allen) Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PD575TTWH8"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PD575TTWH8");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>