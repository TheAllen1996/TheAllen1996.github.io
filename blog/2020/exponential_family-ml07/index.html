<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Machine Learning - 07 Exponential Family | Jingye(Allen) Wang</title> <meta name="author" content="Jingye(Allen) Wang"> <meta name="description" content="Introduce concepts of exponential family."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%82&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://theallen1996.github.io/blog/2020/exponential_family-ml07/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jingye(Allen) </span>Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Gallery</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Machine Learning - 07 Exponential Family</h1> <p class="post-meta">November 12, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fas fa-tag fa-sm"></i> Machine-Learning</a>   </p> </header> <article class="post-content"> <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session" rel="external nofollow noopener" target="_blank">session</a> and the <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf" rel="external nofollow noopener" target="_blank">material</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/" rel="external nofollow noopener" target="_blank">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" rel="external nofollow noopener" target="_blank">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p> <ul id="markdown-toc"> <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li> <li><a href="#1-exponential-family" id="markdown-toc-1-exponential-family">1. Exponential Family</a></li> <li><a href="#2-sufficient-statistic" id="markdown-toc-2-sufficient-statistic">2. Sufficient Statistic</a></li> <li><a href="#3-log-partition-function" id="markdown-toc-3-log-partition-function">3. Log-partition Function</a></li> <li><a href="#4-maximum-entropy" id="markdown-toc-4-maximum-entropy">4. Maximum Entropy</a></li> <li><a href="#5-gaussian-distribution" id="markdown-toc-5-gaussian-distribution">5. Gaussian Distribution</a></li> <li><a href="#6-conclusion" id="markdown-toc-6-conclusion">6. Conclusion</a></li> </ul> <h1 id="0-introduction">0. Introduction</h1> <p>An exponential family is a family of distributions which share some properties in common. The real message of this note is the simplicity and elegance of the exponential family. Once the ideas are mastered, it is often easier to work within the general exponential family framework than with specific instances.</p> <h1 id="1-exponential-family">1. Exponential Family</h1> <p>Given one real-vector parameter \(\mathbf{\theta}=[\theta_1,\theta_2,\dots,\theta_d]^T\), we define an <em>exponential family</em> of probability distributions as those distributions whose density have the following general form:</p> \[f_X(x\vert\eta)=h(x)\exp\left(\eta^T T(x)-A(\eta)\right).\] <p><strong>Canonical parameter</strong>: \(\eta\) is called <em>canonical</em>, or <em>natural parameter</em> (function), which can be viewed as a transformation of \(\mathcal{\theta}\). The set of values of \(\eta\) is always convex.</p> <p><strong>Sufficient statistic</strong>: Given a data set sampled from \(f_X(x\vert\eta)\), the sufficient statistic \(T(x)\) is a function of the data that holds all information the data set provides with regard to the unknown parameter \(\mathbf{\theta}\).</p> <p><strong>Log-partition function</strong>: \(A(\eta)\) is the <em>log-partition function</em> to normalize \(f_X(x\vert \eta)\) to be a probability distribution,</p> \[A(\eta)=\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right).\] <p>In the following sections, we will discuss them detailedly.</p> <h1 id="2-sufficient-statistic">2. Sufficient Statistic</h1> <p>Consider the problem of estimating the unknown parameters by <em>maximum likelihood estimation</em> (MLE) in exponential family cases. Specifically, for an <em>i.i.d.</em> data set \(\mathcal{D}=\{x_1,x_2,\dots,x_N\}\), we have the log likelihood</p> \[\mathcal{L}(\eta\vert\mathcal{D})=\log\left(\prod_{i=1}^Nh(x_i)\right)+\eta^T\left(\sum_{i=1}^NT(x_i)\right)-NA(\eta).\] <p>By <em>MLE</em>, we have the estimation \(\hat\eta\) when its gradient with respect to \(\eta\) is zero:</p> \[\mathcal{L}’(\eta\vert\mathcal{D})=\sum_{i=1}^NT(x_i)-NA’(\eta)=0.\] <p>Solving the equation, we have</p> \[A’(\hat\eta)=\frac{1}{N}\sum_{i=1}^NT(x_i),\] <p>which is the general formula of MLE for the parameters in the exponential family. Further, notice that our formula involves the data only via the sufficient statistic \(T(x_i)\). This gives the operational meaning to <em>sufficiency</em>—for the purpose of estimating parameters we retain only the sufficient statistic.</p> <h1 id="3-log-partition-function">3. Log-partition Function</h1> <p>As we mentioned in section 1, \(A(\eta)\) can be viewed as a normalization factor. In fact, \(A(\eta)\) is not a degree of freedom in the specification of an exponential family density; it is determined once \(T(x)\) and \(h(x)\) are determined. The relation between \(A(\eta)\) and \(T(x)\) can be further characterized by</p> \[\begin{aligned}A’(\eta)&amp;=\frac{\text{d}\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right)}{\text{d}\eta}\\&amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot T(x)\text{d}x}{\int_{X}h(x)\exp(\eta^T T(x))\text{d}x}\\&amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot T(x)\text{d}x}{\exp({A(\mathbf{\theta})})}\\&amp;=\int_{X}\underbrace{h(x)\exp(\eta^T T(x)-A(\eta))}_{f_X(x\vert \eta)}\cdot T(x)\text{d}x\\&amp;=\mathbb{E}_{f_X(x\vert\eta)}[T(x)].\end{aligned}\] <p>Further, we have</p> \[\begin{aligned}A’’(\eta)&amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x)-A’(\eta)) T(x)\text{d}x\\&amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x))^2\text{d}x-A’(\eta)\int_{X}f_X(x\vert \mathbf{\eta})\cdot T(x)\text{d}x\\&amp;=\mathbb{E}_{f_X(x\vert\eta)}[(T(x))^2]-\left(\mathbb{E}_{f_X(x\vert\eta)}[T(x)]\right)^2\\&amp;=var[T(x)],\end{aligned}\] <p>which also shows that \(A(\eta)\) is convex as \(var[T(x)]\ge 0\).</p> <h1 id="4-maximum-entropy">4. Maximum Entropy</h1> <p>The entropy of \(P\) with distribution \(p(x)\) supported on \(X\) is</p> \[H(P)=\mathbb{E}_{P}[-\log p(x)].\] <p>The <em>maximum entropy</em> principle is that: given some constraints (prior information) about the distribution \(P\), we consider all probability distributions satisfying said constraints such that the constraints are being utilized as <em>objective</em> as possible, <em>i.e.,</em> be as uncertain as possible.</p> <p>For example, consider the case where the very constraint is \(\sum_Xp(x)=1\), which formulates</p> \[\begin{aligned}\max&amp;\quad H(P)\\\text{s.t.}&amp;\quad \sum_{X}p(x)=1.\end{aligned}\] <p>By the definition we have</p> \[H(P)=-\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i).\] <p>Then the <em>Lagrangian</em> for the optimization problem is</p> \[L(P,\lambda)=\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i)+\lambda\left(1-\sum_{i=1}^{\vert X\vert}p(x_i)\right).\] <p>Setting the first derivation of the Lagrangian to be zero yields</p> \[\frac{\partial L}{\partial p(x_i)}=0\implies \hat{p}(x_i)=\exp(\lambda-1),\] <p>which gives that</p> \[\hat{p}(x_1)=\hat{p}(x_2)=\dots=\hat{p}(x_{\vert X\vert})=\frac{1}{\vert X\vert},\] <p><em>i.e.,</em> the distribution with maximum entropy is <em>uniform distribution</em>.</p> <p>We now consider a general case where \(p(x)\) is continuous with a general constraint \(\mathbb{E}_P[\Phi(x)]=\alpha\), where \(\Phi(x)=[\phi_1(x),\phi_2(x),\dots,\phi_d(x)]\in\mathbb{R}^d\) and \(\alpha=[\alpha_1,\alpha_2,\dots,\alpha_d]\in\mathbb{R}^d\), which formulates</p> \[\begin{aligned}\max&amp;\quad H(P)\\\text{s.t.}&amp;\quad \mathbb{E}_P[\Phi(x)]=\alpha\\\implies\min&amp;\quad \int_Xp(x)\log p(x)\text{d}x\\\text{s.t.}&amp;\quad \int_X p(x)\phi_i(x)\text{d}x=\alpha_i,\ i=1,2,\dots, d,\\&amp;\quad \int_X p(x)\text{d}x=1.\end{aligned}\] <p>Similarly, we obtain the Lagrangian as</p> \[L(P,\theta,\lambda)=\int_X p(x)\log p(x)\text{d}x+\sum_{i=1}^d\theta_i\left(\alpha_i-\int_X p(x)\phi_i(x)\text{d}x\right)+\lambda\left(\int_X p(x)\text{d}x-1\right).\] <p>By treating the density \(P=[p(x)]_{x\in X}\) as a finite vector such that \(\int_X p(x)\text{d}x\) is similar to \(\sum_X p(x)\), we have</p> \[\begin{aligned}\frac{\partial L}{\partial p(x)}&amp;=\frac{\partial }{\partial p(x)}\left(\sum_X p(x)\log p(x)-\sum_{i=1}^d\theta_i\sum_X p(x)\phi_i(x)+\lambda\sum_X p(x)\right)\\&amp;=1+\log p(x)-\sum_{i=1}^d\theta_i\phi_i(x)+\lambda\\&amp;=1+\log p(x)-\theta^T\Phi(x)+\lambda.\end{aligned}\] <p>Setting the derivation to be zero for all \(x\), we have</p> \[p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},\] <p>which is in the exponential family form with</p> \[\begin{aligned}\eta&amp;=\theta,\\T(x)&amp;=\Phi(x),\\A(\eta)&amp;=\lambda+1,\\h(x)&amp;=1.\end{aligned}\] <h1 id="5-gaussian-distribution">5. Gaussian Distribution</h1> <p>In this section, we consider an example, Gaussian distribution, which is of the exponential family and exemplifies the properties we mentioned above.</p> <p>We first rewritten the PDF of one-dimension Gaussian distribution to show it is in the exponential family .</p> <p><em>Proof:</em> Given unknown parameter \(\mathbf{\theta}=[\mu,\sigma^2]\), the Gaussian density can be written as follows,</p> \[\begin{aligned}f_X(x\vert \mathbf{\theta})&amp;=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}\\&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right\}\\&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;-\frac{1}{2}\sigma^2\end{bmatrix}\begin{bmatrix}x\\x^2\end{bmatrix}-\left(\frac{\mu^2}{2\sigma^2}+\log\sigma\right)\right\},\end{aligned}\] <p>which is in the exponential family form with</p> \[\begin{aligned}\eta&amp;=\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;-\frac{1}{2\sigma^2}\end{bmatrix}^T,\\T(x)&amp;=\begin{bmatrix}x&amp;x^2\end{bmatrix}^T,\\A(\eta)&amp;=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2),\\h(x)&amp;=\frac{1}{\sqrt{2\pi}}.\end{aligned}\] <p>\(\tag*{\)\blacksquare\(}\)</p> <p>Then we verify the relation between the sufficient statistic and MLE method.</p> <p><em>Proof:</em> Given a data set \(\mathcal{D}=\{x_1,x_2,\dots,x_N\}\), as we mentioned in section 2, we can derive the parameters via the sufficient statistic as follows,</p> \[\begin{cases}A’(\eta)=\frac{1}{N}\sum_{i=1}^NT(x)\implies\begin{cases}A’(\hat\eta_1)=\frac{1}{N}\sum_{i=1}^N x_i\\A’(\hat\eta_2)=\frac{1}{N}\sum_{i=1}^Nx_i^2\end{cases}\\A(\eta)=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2)\implies\begin{cases}A’(\hat\eta_1)=-\frac{\eta_1}{2\eta_2}=\hat\mu\\A’(\hat\eta_2)=\frac{\eta_1^2}{4\eta_2^2}-\frac{1}{2\eta_2}=\hat\sigma^2+\hat\mu^2\end{cases}\end{cases}\] <p>Solving the equations we have</p> \[\hat\mu=\frac{1}{N}\sum_{i=1}^Nx_i,\quad \hat\sigma=\frac{1}{N}\sum_{i=1}^Nx_i^2-\hat\mu^2,\] <p>which is consistent with the result in the <a href="https://2ez4ai.github.io/2020/09/28/intro-ml01/" rel="external nofollow noopener" target="_blank">post</a>. \(\tag*{\)\blacksquare\(}\)</p> <p>Now we show that</p> \[A’’(\hat\eta)=var[T(x)].\] <p><em>Proof</em>: Firstly, we have</p> \[\begin{cases}A’’(\hat\eta_1)=-\frac{1}{2\eta_2}=\sigma^2\\A’’(\hat\eta_2)=-\frac{\eta_1^2}{2\eta_2^3}+\frac{1}{2\eta_2^2}=4\sigma^2\mu^2+2\sigma^4\end{cases}\] <p>For \(T(x)=\begin{bmatrix}x&amp;x^2\end{bmatrix}^T\), we have</p> \[var[x]=\sigma^2, \text{ as }x\sim\mathcal{N}(\mu,\sigma^2),\] <p>and</p> \[var[x^2]=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2.\] <p>For \(\mathbb{E}[x^2]\), it follows that</p> \[\mathbb{E}[x^2]=var[x]+(\mathbb{E}[x])^2=\sigma^2+\mu^2.\] <p>For \(\mathbb{E}[x^4]\), to compute it we leverage <em>moment generating functions</em> which follows that</p> \[M_X(t)=e^{\mu t+\frac{1}{2}\sigma^2t^2},\quad \mathbb{E}[x^4]=M^{(4)}_X(0).\] <p>After a laborious computing, we have</p> \[\begin{aligned}var[x^2]&amp;=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2\\&amp;=3\sigma^4+6\sigma^2\mu^2+\mu^4-\sigma^4-2\sigma^2-\mu^4\\&amp;=4\sigma^2\mu^2+2\sigma^4.\end{aligned}\] <p>Therefore, we have</p> <p>\(A’’(\hat\eta)=\begin{bmatrix}var[x]\\var[x^2]\end{bmatrix}.\tag*{\)\blacksquare\(}\)</p> <p>Finally, we show that \(X\sim\mathcal{N}(\mu,\sigma^2)\) is the distribution that maximizes the entropy over all distributions \(P\) satisfying</p> \[\mathbb{E}_P\left[\left(\frac{X-\mu}{\sigma}\right)^2\right]=1.\] <p><em>Proof:</em> Consider the expression we formulated in section 4,</p> \[p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},\] <p>which maximizes the entropy while satisfying \(\mathbb{E}_P[\Phi(x)]=\alpha\). Now letting</p> \[\begin{aligned}\alpha&amp;=1,\\\Phi(x)&amp;=\frac{(x-\mu)^2}{\sigma^2},\\\theta&amp;=-\frac{1}{2},\\\exp\{-\lambda-1\}&amp;=\frac{1}{\sqrt{2\pi}\sigma}.\end{aligned}\] <p>Therefore we have</p> <p>\(p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}.\tag*{\)\blacksquare\(}\)</p> <h1 id="6-conclusion">6. Conclusion</h1> <p>In this post, we briefly introduced the basic form of the exponential family. Then we discussed its properties from three perspectives: sufficient statistic, log-partition function and maximum entropy. Moreover, with one-dimension Gaussian distribution, we exemplified the properties.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jingye(Allen) Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PD575TTWH8"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PD575TTWH8");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>